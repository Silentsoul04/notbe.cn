<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Hawk： 20分钟无编程抓取大众点评17万数据 « NotBeCN</title>
  <meta name="description" content="                  1. 主角出场：Hawk介绍     Hawk是沙漠之鹰开发的一款数据抓取和清洗工具，目前已经在Github开源。详细介绍可参考：http://www.cnblogs.com/buptzym/p/5454190.html 强烈建议先读这篇文章，该文介绍了详细原理和抓取链家二手房...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/03/28/weixin_33849215_90133432.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Hawk： 20分钟无编程抓取大众点评17万数据</h1>
    <p class="post-meta">Mar 28, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <div class="blogpost-body cnblogs-markdown"> 
    <h2>1. 主角出场：Hawk介绍</h2> 
    <p>Hawk是沙漠之鹰开发的一款数据抓取和清洗工具，目前已经在Github开源。详细介绍可参考：<a class="uri" href="http://www.cnblogs.com/buptzym/p/5454190.html" rel="nofollow">http://www.cnblogs.com/buptzym/p/5454190.html</a><br> 强烈建议先读这篇文章，该文介绍了详细原理和抓取链家二手房的攻略，以此为基础，才能较好的理解整个操作。<br> GitHub地址：<a class="uri" href="https://github.com/ferventdesert/Hawk" rel="nofollow">https://github.com/ferventdesert/Hawk</a><br> 本文将讲解通过本软件，获取大众点评的所有美食数据，可选择任一城市，也可以很方便地修改成获取其他生活门类信息的爬虫。<br> 在这个过程中你可以学会：</p> 
    <ul>
     <li>自动登录</li> 
     <li>自动翻页</li> 
     <li>组合多种情况，以破解翻页限制</li> 
     <li>自动获取页面的表格数据</li> 
    </ul>
    <p>本文将省略原理，<strong>一步步地介绍如何在20分钟内完成爬虫的设计</strong>，基本不需要编程，还能自动并行抓取。<br> 看完这篇文章，你应该就能举一反三地抓取绝大多数网站的数据了。Hawk是一整套工具，它的能力取决于你的设计和思路。希望你会喜欢它。<br> 详细过程视频可参考：<a class="uri" href="http://v.qq.com/page/z/g/h/z01891n1rgh.html%EF%BC%8C%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E6%98%AF%EF%BC%8C%E7%94%B1%E4%BA%8E%E8%BD%AF%E4%BB%B6%E4%B8%8D%E6%96%AD%E5%8D%87%E7%BA%A7%EF%BC%8C%E5%9B%A0%E6%AD%A4%E7%BB%86%E8%8A%82%E5%92%8C%E8%A7%86%E9%A2%91%E5%8F%AF%E8%83%BD%E6%9C%89%E6%89%80%E5%87%BA%E5%85%A5" rel="nofollow">http://v.qq.com/page/z/g/h/z01891n1rgh.html，值得注意的是，由于软件不断升级，因此细节和视频可能有所出入</a>。<br> 准备好了么？Let's do it!</p> 
    <h2>2. 菜场买菜：编译和安装</h2> 
    <p>编译可使用VS2015(推荐)，否则可直接从网盘下载可执行程序：<br><a class="uri" href="http://pan.baidu.com/s/1c8zBiQ" rel="nofollow">http://pan.baidu.com/s/1c8zBiQ</a> 密码：4iy0<br> 之后双击Hawk.exe，即可运行。<br> 依赖环境要求.NET Framework 4.5, win7和以上版本。没有其他依赖项。</p> 
    <h2>3. 做饭先生火：自动设置cookie：</h2> 
    <p>我们先打开大众点评的美食列表页面：<br><a class="uri" href="http://www.dianping.com/search/category/2/10/g311" rel="nofollow">http://www.dianping.com/search/category/2/10/g311</a><br><img alt="image_1airusse2977232s23148o1pi9.png-224.2kB" src="http://static.zybuluo.com/buptzym/c9mq7m6kp5fi3icl59jsxheo/image_1airusse2977232s23148o1pi9.png"><br> 这是北京的"北京菜"列表，但你会注意到，只能抓取前50页数据（如箭头所示），是一种防爬虫策略，我们之后来破解它。<br> 我们双击打开一个网页采集器：<br><img alt="image_1airv085o1pdf5u1egvuerjgnm.png-31.3kB" src="http://static.zybuluo.com/buptzym/asybhai8k3hrs24pesdyi5od/image_1airv085o1pdf5u1egvuerjgnm.png"><br> 之后在最上方的地址栏里填写地址：<br><img alt="image_1airv1slp1qnb1abtginug2kc913.png-20.2kB" src="http://static.zybuluo.com/buptzym/u0l98zjacgaz86umxgxyk7nm/image_1airv1slp1qnb1abtginug2kc913.png"><br> 但会发现远程服务器拒绝了请求，原因是大众点评认为Hawk是爬虫而不是浏览器。<br> 没有关系，我们让Hawk来监控浏览器的行为，在右侧的<strong>自动嗅探</strong>窗口中，填写url过滤和内容筛选，之后点击<strong>开始</strong>。浏览器会自动打开该网页，程序后台自动记录了所有的行为，之后点击<strong>关闭</strong>按钮（切记点击关闭）。<br><img alt="image_1airvo473v1h1rj61f3c1fvt9nq37.png-17.8kB" src="http://static.zybuluo.com/buptzym/7adqt2zzhq1zkj8htb67yqel/image_1airvo473v1h1rj61f3c1fvt9nq37.png"><br> (此处大概介绍原理：Hawk在点击开始之后，会自动成为代理，所有的浏览器请求都会经过Hawk，在输入特定的URL筛选前缀和关键字，则Hawk会自动拦截符合要求的Request，并将其详细信息记录下来，并最终模拟它)。</p> 
    <p>之后，我们点击右方的“<strong>高级设置</strong>”里，能够看到Hawk已经把这次访问的cookie和headers自动保存下来：<br><img alt="image_1airves3k7bs44uo0112o01l381t.png-336.8kB" src="http://static.zybuluo.com/buptzym/cy393cj7pte66uyc5s0ntg40/image_1airves3k7bs44uo0112o01l381t.png"><br> 我们再次点击<strong>刷新网页</strong>，可以看到已经能成功获取网页内容：<br><img alt="image_1airvksaq19ac1ml84d92661rff2a.png-259.1kB" src="http://static.zybuluo.com/buptzym/vzp2wrgcqu84ywxzlmcr2xzt/image_1airvksaq19ac1ml84d92661rff2a.png"><br> 完成这一步之后，我们就能够像普通网页那样免登陆抓取信息了。这也适合需要登录的各类网站。</p> 
    <h2>4.洗菜切菜：获取门店列表</h2> 
    <p>我们通过自动和手动两种方式来获取门店列表，你可以两种都试试。</p> 
    <h3>4.1 全自动获取</h3> 
    <p>直接点击<strong>手气不错</strong>即可，不需要其他操作：<br><img alt="image_1aiss9bm91n5fvcto85i7b1q8nm.png-141.7kB" src="http://static.zybuluo.com/buptzym/21o36ebagi5s6949d8qdkz22/image_1aiss9bm91n5fvcto85i7b1q8nm.png"></p> 
    <h3>4.2 纯手工获取</h3> 
    <p>我们先手工输入筛选条件吧：<br><img alt="image_1airusse2977232s23148o1pi9.png-224.2kB" src="http://static.zybuluo.com/buptzym/c9mq7m6kp5fi3icl59jsxheo/image_1airusse2977232s23148o1pi9.png"><br> 输入上面的关键字3774，命名为<strong>点评</strong>：<br><img alt="image_1airvuu8ef54a78mmd7anvit4h.png-37.3kB" src="http://static.zybuluo.com/buptzym/oeo163r4iu5v0c2o8ek21ddo/image_1airvuu8ef54a78mmd7anvit4h.png"><br> 接着，填入89,你会发现是下面这样：<br><img alt="image_1ais0256617201p1d1ilv1na6rur4u.png-31.8kB" src="http://static.zybuluo.com/buptzym/vy4zzvxfcpd9nvea2z5hj1lg/image_1ais0256617201p1d1ilv1na6rur4u.png"><br> 注意XPath表达式和点评的表达式不大一样，这是因为89太普通，在网页中出现多次，再次点击<strong>搜索XPath</strong>，即可找到正确的位置。<br> 类似地，将所有你认为需要的属性添加进去，加上合适的命名，大概是这个样子：<br><img alt="image_1ais08cp4dr31nmqr78u7u1lc85b.png-25.3kB" src="http://static.zybuluo.com/buptzym/ednv2uc8wf9taxj4yp7a14i0/image_1ais08cp4dr31nmqr78u7u1lc85b.png"><br> 当然，我们还需要门店的ID，但在页面上并不提供，那在浏览器上点击那个“四季民福烤鸭店”（沙漠君厌恶吃鸭子），你会看到它的链接为：<br><img alt="image_1ais0br53kf7175t7du1pm210fo65.png-13.3kB" src="http://static.zybuluo.com/buptzym/theesmj4b7mowdwfq55qm46h/image_1ais0br53kf7175t7du1pm210fo65.png"><br> 我们将18002657添加并搜索，发现不论怎么点都搜索不到，此时勾选<strong>提取标签</strong>，系统会在标签中搜索：<br><img alt="image_1aissgnhl1ga3bm3m610g1uml13.png-18.4kB" src="http://static.zybuluo.com/buptzym/8u6elnu3hbj70gwpcsfgrrm8/image_1aissgnhl1ga3bm3m610g1uml13.png"></p> 
    <p>之后获取了全部属性后，点击<strong>提取测试</strong>，系统会自动优化XPath，列表父节点会显示在下方。</p> 
    <p><em>笔者建议自动加手动配合的方式，自动抓取大部分数据，再用手动修改调整，手气不错虽然智能，但并不是什么时候都管用</em>。</p> 
    <p>将本模块命名为<strong>门店列表</strong>，供之后备用：<br><img alt="image_1aissqg3s1vp01pjg1rnibqjdgj1g.png-5.5kB" src="http://static.zybuluo.com/buptzym/h3gtaax8e550z03ellxngiyn/image_1aissqg3s1vp01pjg1rnibqjdgj1g.png"></p> 
    <h2>5.餐前甜点：获取50页数据</h2> 
    <p>我们先用50页数据试试手，在刚才那个浏览器页面的最下方，点击翻页，可以发现是如下的结构：</p> 
    <pre><code class="hljs dts"><span class="hljs-symbol">http:</span><span class="hljs-comment">//www.dianping.com/search/category/2/10/g311p2</span>
<span class="hljs-symbol">http:</span><span class="hljs-comment">//www.dianping.com/search/category/2/10/g311p3</span>
<span class="hljs-symbol">http:</span><span class="hljs-comment">//www.dianping.com/search/category/2/10/g311p4</span>
...</code></pre> 
    <p>好，新建<strong>数据清洗</strong>，随便给它起个名字，从左面拖入<strong>生成区间数</strong>，双击配置列名为page，最大值填50，再拖入<strong>合并多列</strong>到page列，配置如下：<br><img alt="image_1aiu7pia11jtv1fs1crf1eu41fg39.png-40.4kB" src="http://static.zybuluo.com/buptzym/ft6vhudfo2yz4g766baakokh/image_1aiu7pia11jtv1fs1crf1eu41fg39.png"><br> 其中Format设置为：<br><code>http://www.dianping.com/search/category/2/10/g311p{0}</code><br> 这是C#的一种字符串替换语法，{0}会被依次替换为1,2,3...<br> 最后，拖<strong>从爬虫转换</strong>到url列，奇迹出现了吧？<br> 为了保存结果，我们拖<strong>写入数据表</strong>到<strong>任意一列</strong>,这里选择了<strong>名称列</strong>，配置如下：<br><img alt="image_1aiu81b8s1jic1eu3vnp1m5517i1m.png-51.8kB" src="http://static.zybuluo.com/buptzym/vsj3fs1p3hefbfu6egk6jyg8/image_1aiu81b8s1jic1eu3vnp1m5517i1m.png"><br> 之后，在右侧选择并行或串行模式（随你），点击<strong>执行</strong>即可。<br><img alt="image_1aiu84m21n836rr12jgqgt1b4b13.png-183.8kB" src="http://static.zybuluo.com/buptzym/uzqkq6976i38tceixeuphfex/image_1aiu84m21n836rr12jgqgt1b4b13.png"><br> 数据采集完成了！</p> 
    <p><em>如果看到这一步累了，可以不看下面的内容，但如果想获取全部内容，步骤就复杂多了，如果你下决心学习，我们接着往下看</em></p> 
    <h2>5. 准备葱蒜：获取城市的美食门类</h2> 
    <p>解决问题的办法是分而治之，获取每个区县（如北京的海淀区）下的某一种美食门类（东北菜），自然就没50页那么多了。所以，要获取美食门类，再获取所有的区域。</p> 
    <p>先找到所有美食门类的位置：<br><code>http://www.dianping.com/shopall/2/0</code><br> 为了获取此页面上的信息，我们再新建一个<strong>网页采集器</strong>，命名为<strong>通用采集器</strong>，它的目标是获取整个HTML页面，因此<br><strong>读取模式</strong>改成One,将刚才<strong>门店列表</strong>采集器里的高级设置-&gt;Parameters的内容拷贝到本采集器对应的窗口中。<br> (其实也可以做嗅探，但这个更快一些)。<br> 之后，我们来获取这个页面上的所有美食门类，新建<strong>数据清洗</strong>，命名为<strong>门类</strong>，然后从左侧拖<strong>从文本生成</strong>到右侧任意一列，命名如下：<br><img alt="image_1aistmi1i1fibku21ni1loc66s2n.png-21.9kB" src="http://static.zybuluo.com/buptzym/iaur01tzo0kljfh069ycmjfy/image_1aistmi1i1fibku21ni1loc66s2n.png"><br> 再拖入<strong>从爬虫转换</strong>，配置如下：<br><img alt="image_1aistobto1jstlqfduiv7aihq34.png-23.9kB" src="http://static.zybuluo.com/buptzym/uak0916xglyhh8x0ieww9xob/image_1aistobto1jstlqfduiv7aihq34.png"><br> 即可调用刚才的<strong>通用采集器</strong>。另外，左侧的工具栏支持搜索，直接关键字即可快速定位，结果如下：<br><img alt="image_1aistqm3s333b775vr1fsvif3h.png-25.2kB" src="http://static.zybuluo.com/buptzym/3o4tzlp3t146se3qeetuq95w/image_1aistqm3s333b775vr1fsvif3h.png"></p> 
    <p><img alt="image_1aistrkufs37jsm1o9c1opsgv53u.png-56.7kB" src="http://static.zybuluo.com/buptzym/lsb9lsmd7bdaymfmnz26kh42/image_1aistrkufs37jsm1o9c1opsgv53u.png"><br> 为了获取下图的<strong>北京菜</strong>所在的位置，虽然可以用Hawk，但为了方便可以使用Chrome，搜狗和360浏览器的F12开发者工具功能，找到对应的元素，点击右键，拷贝XPath:<br><img alt="image_1aisuaq7ib2k8blb5h3r51p5q4b.png-23kB" src="http://static.zybuluo.com/buptzym/o8crnkobek9okgljh5p9wwam/image_1aisuaq7ib2k8blb5h3r51p5q4b.png"><br> 内容为：<code>//*[@id="top"]/div[6]/div/div[1]/dl[1]/dd/ul/li[1]</code>，<br> 因为要获取所有的子li，在刚才的数据清洗中，向Content列拖入<strong>XPath筛选器</strong>，配置如下：<br><img alt="image_1aisui6polg49f134f1rvo8s94o.png-28.3kB" src="http://static.zybuluo.com/buptzym/doo7bjnu1euea1n9r2cdqrhx/image_1aisui6polg49f134f1rvo8s94o.png"><br> 由于要获取所有的li子节点，所以去掉了最后的<a href="http://static.zybuluo.com/buptzym/c9mq7m6kp5fi3icl59jsxheo/image_1airusse2977232s23148o1pi9.png" rel="nofollow">1</a>，可以适当复习XPath语法。<br> 奇迹出现了：<br><img alt="image_1aisujo637p1iptteav57ct55.png-55.6kB" src="http://static.zybuluo.com/buptzym/2rb3uo8kiaigy9sga7qv9xnv/image_1aisujo637p1iptteav57ct55.png"><br> 接下来步骤很简单，我不截图了：</p> 
    <ul>
     <li>拖入<strong>HTML字符转义</strong>到Text列，可以清除该列的乱码</li> 
     <li>再拖入<strong>字符串分割</strong>到Text，勾选<strong>空格分割</strong>，可对该数据用空格分割，并获取默认的第一个子串</li> 
     <li>拖入<strong>删除该列</strong>到OHTML,该列没有用</li> 
     <li> <p>再拖入<strong>正则转换器</strong>到HTML，配置如下：<br><img alt="image_1aiu937op1opn1u6a19tr1fppd8h19.png-42.7kB" src="http://static.zybuluo.com/buptzym/vvq250kd7mntb2p5hw8kaho5/image_1aiu937op1opn1u6a19tr1fppd8h19.png"><br> g\d+代表匹配那个门类的ID，比如刚才的g311</p> </li> 
     <li>拖入<strong>删除该列</strong>到HTML</li> 
     <li> <p>直接在Text列的上方修改名称为<strong>门类</strong></p> </li> 
    </ul>
    <p>最终结果如下：<br><img alt="image_1aiu979h42tg9pg1uembp81fip1m.png-31.4kB" src="http://static.zybuluo.com/buptzym/cuuho5a8rfnwzaehnjyschdb/image_1aiu979h42tg9pg1uembp81fip1m.png"></p> 
    <h2>6.获取北京的区域</h2> 
    <p>这一步和上一步非常类似，因此我很简明地介绍一下。<br> 区域在这个页面：<br><code>http://www.dianping.com/search/category/2/10/g311p2</code><br><img alt="image_1aiu9bfftljg1lmi63h18d7r7823.png-48.1kB" src="http://static.zybuluo.com/buptzym/jst5l3etdj1z3ud4kau5sq3i/image_1aiu9bfftljg1lmi63h18d7r7823.png"><br> 这些节点的XPath是：//*[@id="region-nav"]/a<br> 你可以按照刚才类似的步骤进行，也是创建新的<strong>数据清洗</strong>，把这个子模块命名为<strong>区域</strong>，最终结果如下：<br><img alt="image_1aiu9omeh1k1bf9pp1g10v51b3330.png-46.7kB" src="http://static.zybuluo.com/buptzym/xxgm3mgx17mnj43kw3ckubnt/image_1aiu9omeh1k1bf9pp1g10v51b3330.png"></p> 
    <p>如果自己做不下来，也没有关系，加载Github上大众点评的<code>教程.xml</code>，可以直接用这个现成的模块，也可以单步调试之，看看它是怎么写的。</p> 
    <h2>7.正菜开始：主流程</h2> 
    <p>下面是最难也是最复杂的部分。我们的思路是，组合所有的门类和区域，构成m*n的一组序对，如<code>海淀区-北京菜</code>，<code>朝阳区-火锅</code>等等，获取对应序对的页数，再将所有结果拼接起来。<br> 准备好了么？我们继续。</p> 
    <p>新建<strong>数据清洗</strong>，命名为<strong>主流程</strong>，我们要调用刚才定义的模块，拖入<strong>子流-生成</strong>到任意一列，配置如下：<br><img alt="image_1aiua4trfd641198v121hpt1cma3t.png-26.1kB" src="http://static.zybuluo.com/buptzym/klrphq7fpp37iv4dtg3ryjco/image_1aiua4trfd641198v121hpt1cma3t.png"><br> 记得要勾选<strong>启用</strong>，这些模块默认是不启用的。<br> 再拖入<strong>子流-生成</strong>到任意一列，配置如下：<br><img alt="image_1aiua70q350rgop1tljiud1ovk4a.png-54.4kB" src="http://static.zybuluo.com/buptzym/32a8im1rrdei6xbecdgbny7a/image_1aiua70q350rgop1tljiud1ovk4a.png"><br> 注意生成模式改为Cross。<br> 具体不同模式的工作方式，可参考这篇文章：<a class="uri" href="http://www.cnblogs.com/buptzym/p/5501003.html" rel="nofollow">http://www.cnblogs.com/buptzym/p/5501003.html</a><br> 之后，就是这个样子：<br><img alt="image_1aiua9pco128h1qsanbe1g3u12i54n.png-97.6kB" src="http://static.zybuluo.com/buptzym/ixcz6pbctuga7q17lafgyoav/image_1aiua9pco128h1qsanbe1g3u12i54n.png"><br> 我们将两列组合起来，可看到Url为如下的形式：<br><a class="uri" href="http://www.dianping.com/search/category/2/10/**g311r14**" rel="nofollow">http://www.dianping.com/search/category/2/10/**g311r14**</a><br> 拖<strong>合并多列</strong>到type，配置如下：<br><img alt="image_1aiuaeshc14pib744751r5vfp554.png-27.6kB" src="http://static.zybuluo.com/buptzym/gg4gxxy0szga3cfb6jootbbz/image_1aiuaeshc14pib744751r5vfp554.png"><br> {0}{1}相当于组合了多个元素，拖入的当前列为第0元素，其他项用空格分割，分别代表第1,2...个元素。<br> 为了获取每个门类的页数，需要在页面上找一下：<br><img alt="image_1aiuajl6nhdrdgh1t3sikl3a95h.png-120.9kB" src="http://static.zybuluo.com/buptzym/hv45rid5loaoh7fulv899210/image_1aiuajl6nhdrdgh1t3sikl3a95h.png"><br> 它的XPath是<code>/html[1]/body[1]/div[6]/div[1]/span[7]</code></p> 
    <ul>
     <li>拖入<strong>从爬虫转换</strong>到url列，配置爬虫选择为<strong>通用采集器</strong>，就能获取对应的HTML</li> 
     <li>拖入<strong>XPath筛选器</strong>到HTML所在的Content列，XPath表达式如上<code>/html[1]/body[1]/div[6]/div[1]/span[7]</code>。只获取一个数据，新列名为count</li> 
     <li>拖入<strong>删除该列</strong>到Content列。<br><img alt="image_1aiub6nl319v01bo714su63p1gkq5u.png-65.5kB" src="http://static.zybuluo.com/buptzym/mastmg94is32lrna5c0sbuzt/image_1aiub6nl319v01bo714su63p1gkq5u.png"></li> 
     <li>拖入<strong>提取数字</strong>到count列</li> 
     <li> <p>拖入<strong>Python转换器</strong>到count列，这是本文唯一要写的代码：<br> 配置如下：<br><img alt="image_1aiubbcsg1qeg1l0712u216gu1me46b.png-28.8kB" src="http://static.zybuluo.com/buptzym/grll4ghrd4lu7rlqcs1nrlpm/image_1aiubbcsg1qeg1l0712u216gu1me46b.png"><br> 代码在下面：</p> <pre><code class="hljs fortran">v=<span class="hljs-built_in">int</span>(<span class="hljs-keyword">value</span>)
<span class="hljs-number">50</span> <span class="hljs-keyword">if</span> v&gt;<span class="hljs-number">750</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">int</span>(v/<span class="hljs-number">15</span>)+<span class="hljs-number">1</span></code></pre> <p>Python代码很好理解吧，大概是说超过750个就按50页处理，页数等于数量除以每页15个，取整后+1,截图中的代码是错误的，感谢热心网友提醒<br><img alt="image_1aiubggfv15qt1ot719b5d3l1de69.png-64.2kB" src="http://static.zybuluo.com/buptzym/xljzmvnxfag8yvjvfkr74fo5/image_1aiubggfv15qt1ot719b5d3l1de69.png"><br> 你会发现即使这样，每个门类还是超过了50页，这个问题我们之后再讨论。<br> 为了方便并行，拖入<strong>流实例化</strong>到任意一列，配置如下：<br><img alt="image_1aiublrdv1nio6a1l5k1vj5riom.png-69kB" src="http://static.zybuluo.com/buptzym/zr357ycxi3jebcgf0nv2b0yv/image_1aiublrdv1nio6a1l5k1vj5riom.png">。<br> 执行器会将每一个门类区县对分配一个独立的线程，注意方括号[url]的写法，系统会把url列的内容赋值到这里，如果你只写url，那所有的线程名称都叫url了。你可以不添加流实例化，看看系统最后是怎么工作的。</p> </li> 
    </ul>
    <p>接下来，我们要把page列展开，生成[0-page]的区间数，一页一页去抓取。拖<strong>生成区间数</strong>到page列，配置如下：<br><img alt="image_1aiubrrau2rpdb61m7rcno1si113.png-38.8kB" src="http://static.zybuluo.com/buptzym/5m5w6fk3kg0lsrpie549jxxf/image_1aiubrrau2rpdb61m7rcno1si113.png"><br> 注意Cross和[page]，我就不多解释了。<br> 把刚才的url和现在的p列合并，就构成了每一页的真实url.<br> 拖入<strong>合并多列</strong>到url，配置如下：<br><img alt="image_1aiuc0p8pv4p18ks1rts1nfh1ivf1t.png-19.4kB" src="http://static.zybuluo.com/buptzym/7ghtgh8h1cgjrtv1j8fg0b45/image_1aiuc0p8pv4p18ks1rts1nfh1ivf1t.png"><br> 仔细理解一下配置的意思，尤其是{0}p{1}，我觉得读者到了这一步，已经对整个系统有点感觉了。<br> 雄关漫道真如铁，我们马上到达目的地了。<br> 现在url列已经是这个样子了（点击<strong>查看样例</strong>即可）<br><img alt="image_1aiuc41shkp2cju10rgrji1h942q.png-26.7kB" src="http://static.zybuluo.com/buptzym/pskqcz337ersa52va02nn3n6/image_1aiuc41shkp2cju10rgrji1h942q.png"><br> 将<strong>从爬虫转换</strong>到url，配置爬虫来源为<strong>门店列表</strong>！然后等待奇迹出现<br> （卖个关子，我就不截图了）<br> 然后拖入<strong>写入数据表</strong>到任意一列，为表名起个名字，点击执行去跑就可以了。<br> 如果你到这一步就满意了，那么文章可以不用往下看了。</p> 
    <h2>8.注重细节</h2> 
    <p>一道大菜要非常注意细节，爬虫也一样。</p> 
    <h3>8.1 保留原始表的信息</h3> 
    <p>你看到数据表里没有这家店的区县，也没有所在的页数，感觉从爬虫转换丢失了原始表的一部分信息，事实上它在1转多的时候，原始表默认都会丢掉。<br> 因此在下图的位置，点击编辑集合，选择最后的那个从爬虫转换，配置如下：<br><img alt="image_1aiucj94dqir1rlt19n51n41ue63k.png-6.9kB" src="http://static.zybuluo.com/buptzym/srbjv3cyo5mxgzh6us3kyw2y/image_1aiucj94dqir1rlt19n51n41ue63k.png"><br> 它会将p和区域两列，添加到新表中。</p> 
    <h3>8.2 我想写入数据库</h3> 
    <p>目前Hawk没有强的自动建表功能，因此建议使用MongoDB,如果你已经安装了，在<strong>模块管理</strong>的数据源哪里，点击右键，可新建MongoDB连接器。<br> 可以在主流程的最后位置，在拖入<strong>写入数据库</strong>，即可。</p> 
    <h3>8.3 还是没有获取所有数据</h3> 
    <p>即使是刚才这样的复杂操作，依然不能获取所有的美食，因为火锅太火，朝阳海淀的火锅都超过了50页，解决方法是再细分商区，比如朝阳的三元桥，国贸，望京...这样就能完整解决了。但本文限于篇幅就不讨论了。</p> 
    <h3>8.4 如何将数据表导出到文件？</h3> 
    <p>在右下角的数据管理，在要导出的表上点右键，建议输出为xml,json和txt文件，excel文件在数据量较大（5万以上）会有性能问题。</p> 
    <h3>8.5 这种图形化操作有什么优势？</h3> 
    <p>效率！所见即所得！你可以试着用任意一种代码去写，烦死你</p> 
    <h3>8.6 如何保存所有操作？</h3> 
    <p><img alt="image_1aiuctvts8qq1l7nn291lvk1ol841.png-33.7kB" src="http://static.zybuluo.com/buptzym/6g248woqyqk6h2ubondf2nco/image_1aiuctvts8qq1l7nn291lvk1ol841.png"><br> 会将所有刚才的操作保存在工程文件中。</p> 
    <h3>8.6 我的服务器在Linux上，怎么办</h3> 
    <p>Hawk是WPF,C#开发的，因此只能在Windows上运行，不过它生成的xml可以被Python解释，参考github上的etlpy.</p> 
    <h3>8.7 Hawk是你一个人写的吗？用了多久</h3> 
    <p>目前来看是这样的。业余时间四年</p> 
    <h3>8.8我想获取各个城市的，不限于美食的数据</h3> 
    <p>这个就更复杂了，可以借助脚本实现，这是下一篇的话题。</p> 
    <h2>9.总结</h2> 
    <p>为了方便大家学习使用，刚才的整个操作已经上传到了Github。地址为https://github.com/ferventdesert/Hawk-Projects<br> 大众点评-教程.xml</p> 
    <p>本软件是我在.NET领域最后的绝唱，估计以后不会再继续开发C#相关的东西了。<br> 有任何问题，欢迎留言。</p> 
   </div> 
   <div> 
    <div> 
     <p style="border-top:#e0e0e0 1px dashed;border-right:#e0e0e0 1px dashed;border-bottom:#e0e0e0 1px dashed;border-left:#e0e0e0 1px dashed;background:#e5f1f4 url(&quot;https://images.cnblogs.com/cnblogs_com/lloydsheng/239039/o_copyright.gif&quot;) no-repeat 1% 50%;font-family:'微软雅黑';font-size:11px;"><br> 作者：<a href="http://www.cnblogs.com/buptzym/" rel="nofollow">热情的沙漠</a> <br> 出处：<a href="http://www.cnblogs.com/buptzym/" rel="nofollow">http://www.cnblogs.com/buptzym/</a> <br> 本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利。 <br></p> 
    </div> 
   </div> 
   <div class="clear"></div> 
   <div> 
    <div>
     分类: 
     <a href="http://www.cnblogs.com/buptzym/category/809619.html" rel="nofollow">开源项目</a> 
    </div> 
    <div>
     <br>
    </div> 
    <div>
     本文转自FerventDesert博客园博客，原文链接：http://www.cnblogs.com/buptzym/p/5500979.html，如需转载请自行联系原作者
     <br>
    </div> 
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
