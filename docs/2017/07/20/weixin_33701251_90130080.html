<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>使用腾讯云 GPU 学习深度学习系列之二：Tensorflow 简明原理【转】 « NotBeCN</title>
  <meta name="description" content="             转自：https://www.qcloud.com/community/article/598765?fromSource=gwzcw.117333.117333.117333    这是《使用腾讯云 GPU 学习深度学习》系列文章的第二篇，主要介绍了 Tensorflow 的原理，以及...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/07/20/weixin_33701251_90130080.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">使用腾讯云 GPU 学习深度学习系列之二：Tensorflow 简明原理【转】</h1>
    <p class="post-meta">Jul 20, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p>转自：<a href="https://www.qcloud.com/community/article/598765?fromSource=gwzcw.117333.117333.117333" rel="nofollow">https://www.qcloud.com/community/article/598765?fromSource=gwzcw.117333.117333.117333</a></p> 
   <p>这是《使用腾讯云 GPU 学习深度学习》系列文章的第二篇，主要介绍了 Tensorflow 的原理，以及如何用最简单的Python代码进行功能实现。本系列文章主要介绍如何使用&nbsp;<a href="https://www.qcloud.com/product/gpu" rel="nofollow">腾讯云GPU服务器</a>&nbsp;进行深度学习运算，前面主要介绍原理部分，后期则以实践为主。</p> 
   <p>往期内容：</p> 
   <p><a href="https://www.qcloud.com/community/article/790616" rel="nofollow">使用腾讯云 GPU 学习深度学习系列之一：传统机器学习的回顾</a></p> 
   <h2>1. 神经网络原理</h2> 
   <p>神经网络模型，是上一章节提到的典型的监督学习问题，即我们有一组输入以及对应的目标输出，求最优模型。通过最优模型，当我们有新的输入时，可以得到一个近似真实的预测输出。</p> 
   <p>我们先看一下如何实现这样一个简单的神经网络：</p> 
   <ul>
    <li> <p>输入&nbsp;<code>x = [1,2,3]</code>,</p> </li> 
    <li> <p>目标输出&nbsp;<code>y = [-0.85, 0.72]</code></p> </li> 
    <li> <p>中间使用一个包含四个单元的隐藏层。</p> </li> 
    <li> <p>结构如图：<img src="https://yqfile.alicdn.com/img_aed747c3d00dcbc82a31a46239cafa56.png" alt="png"></p> </li> 
   </ul>
   <p>求所需参数&nbsp;<code>w1_0</code>&nbsp;<code>w2_0</code>&nbsp;<code>b1_0</code>&nbsp;<code>b2_0</code>， 使得给定输入 x 下得到的输出 ，和目标输出&nbsp;<img src="https://yqfile.alicdn.com/img_529729fc5e41ec05ca8b7c5b9e9ddb87.gif" alt="png">&nbsp;之间的平均均方误差 （Mean Square Errors, MSE) 最小化 。</p> 
   <p>我们首先需要思考，有几个参数？由于是两层神经网络，结构如下图（图片来源<a href="http://stackoverflow.com/questions/22054877/backpropagation-training-stuck" rel="nofollow">http://stackoverflow.com/questions/22054877/backpropagation-training-stuck</a>）, 其中输入层为 3，中间层为 4，输出层是 2：</p> 
   <p><img src="https://yqfile.alicdn.com/img_5747d28f192f623afb22a35961367162.jpg" alt="png"></p> 
   <p>因此，其中总共包含 (3x4+4) + (4*2+2) = 26 个参数需要训练。我们可以如图初始化参数。参数可以随机初始化，也可以随便指定：</p> 
   <pre><code class="lang-python"><span class="hljs-meta"># python <span class="hljs-keyword">import numpy as np w1_0 = np.<span class="hljs-built_in">array([[ <span class="hljs-number">0.1, <span class="hljs-number">0.2, <span class="hljs-number">0.3, <span class="hljs-number">0.4], [ <span class="hljs-number">0.5, <span class="hljs-number">0.6, <span class="hljs-number">0.7, <span class="hljs-number">0.8], [ <span class="hljs-number">0.9, <span class="hljs-number">1.0, <span class="hljs-number">1.1, <span class="hljs-number">1.2]]) w2_0 = np.<span class="hljs-built_in">array([[ <span class="hljs-number">1.3, <span class="hljs-number">1.4], [ <span class="hljs-number">1.5, <span class="hljs-number">1.6], [ <span class="hljs-number">1.7, <span class="hljs-number">1.8], [ <span class="hljs-number">1.9, <span class="hljs-number">2.0]]) b1_0 = np.<span class="hljs-built_in">array( [<span class="hljs-number">-2.0, <span class="hljs-number">-6.0, <span class="hljs-number">-1.0, <span class="hljs-number">-7.0]) b2_0 = np.<span class="hljs-built_in">array( [<span class="hljs-number">-2.5, <span class="hljs-number">-5.0]) </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre> 
   <p>我们进行一次正向传播：</p> 
   <pre><code class="lang-python"><span class="hljs-comment"># python <span class="hljs-attr">x = [<span class="hljs-number">1,<span class="hljs-number">2,<span class="hljs-number">3] <span class="hljs-attr">y = [-<span class="hljs-number">0.85, <span class="hljs-number">0.72] <span class="hljs-attr">o1 = np.dot(x, w<span class="hljs-number">1_0 ) + b<span class="hljs-number">1_0 <span class="hljs-attr">os1 = np.power(<span class="hljs-number">1+np.exp(o1*-<span class="hljs-number">1), -<span class="hljs-number">1) <span class="hljs-attr">o2 = np.dot(os1, w<span class="hljs-number">2_0) + b<span class="hljs-number">2_0 <span class="hljs-attr">os2 = np.tanh(o2) </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre> 
   <p>再进行一次反向传播：</p> 
   <pre><code class="lang-python"><span class="hljs-comment"># python alpha = 0.1 grad_os2 = (y - os2) * (1-np.power(os2, 2)) grad_os1 = np.dot(w2_0, grad_os2.T).T * (1-os1)*os1 grad_w2 = ... grad_b2 = ... ... ... w2_0 = w2_0 + alpha * grad_w2 b2_0 = b2_0 + alpha * grad_b2 ... ... </span></code></pre> 
   <p>如此反复多次，直到最终误差收敛。进行反向传播时，需要将所有参数的求导结果都写上去，然后根据求导结果更新参数。我这里就没有写全，因为一层一层推导实在是太过麻烦。更重要的是，当我们需要训练新的神经网络结构时，这些都需要重新推导一次，费时费力。</p> 
   <p>然而仔细想一想，这个推导的过程也并非无规律可循。即上一级的神经网络梯度输出，会被用作下一级计算梯度的输入，同时下一级计算梯度的输出，会被作为上一级神经网络的输入。于是我们就思考能否将这一过程抽象化，做成一个可以自动求导的框架？OK，以 Tensorflow 为代表的一系列深度学习框架，正是根据这一思路诞生的。</p> 
   <h2>2.深度学习框架</h2> 
   <p>近几年最火的深度学习框架是什么？毫无疑问，Tensorflow 高票当选。</p> 
   <p><img src="https://yqfile.alicdn.com/img_16a486ad33423b5a375a28a0136b72e8.jpg" alt="png"></p> 
   <p>但实际上，这些深度学习框架都具有一些普遍特征。Gokula Krishnan Santhanam认为，<a href="http://www.leiphone.com/news/201701/DZeAwe2qgx8JhbU8.html" rel="nofollow">大部分深度学习框架都包含以下五个核心组件</a>：</p> 
   <ol>
    <li> <p>张量（Tensor）</p> </li> 
    <li> <p>基于张量的各种操作</p> </li> 
    <li> <p>计算图（Computation Graph）</p> </li> 
    <li> <p>自动微分（Automatic Differentiation）工具</p> </li> 
    <li> <p>BLAS、cuBLAS、cuDNN等拓展包</p> </li> 
   </ol>
   <p>其中，张量 Tensor 可以理解为任意维度的数组——比如一维数组被称作向量（Vector），二维的被称作矩阵（Matrix），这些都属于张量。有了张量，就有对应的基本操作，如取某行某列的值，张量乘以常数等。运用拓展包其实就相当于使用底层计算软件加速运算。</p> 
   <p>我们今天重点介绍的，就是计算图模型，以及自动微分两部分。首先介绍以 Torch 框架为例，谈谈如何实现自动求导，然后再用最简单的方法，实现这两部分。</p> 
   <h3>2.1. 深度学习框架如何实现自动求导</h3> 
   <p>诸如 Tensorflow 这样的深度学习框架的入门，网上有大量的 几行代码、几分钟入门这样的资料，可以快速实现手写数字识别等简单任务。但如果想深入了解 Tensorflow 的背后原理，可能就不是这么容易的事情了。这里我们简单的谈一谈这一部分。</p> 
   <p>我们知道，当我们拿到数据、训练神经网络时，网络中的所有参数都是&nbsp;变量。训练模型的过程，就是如何得到一组最佳变量，使预测最准确的过程。这个过程实际上就是，输入数据经过&nbsp;正向传播，变成预测，然后预测与实际情况的误差&nbsp;反向传播&nbsp;误差回来，更新变量。如此反复多次，得到最优的参数。这里就会遇到一个问题，神经网络这么多层，如何保证正向、反向传播都可以正确运行？</p> 
   <p>值得思考的是，这两种传播方式，都具有&nbsp;管道传播&nbsp;的特征。正向传播一层一层算就可以了，上一层网络的结果作为下一层的输入。而反向传播过程可以利用&nbsp;链式求导法则，从后往前，不断将误差分摊到每一个参数的头上。</p> 
   <p>图片来源：<a href="http://colah.github.io/posts/2015-08-Backprop/" rel="nofollow">Colah博客</a></p> 
   <p><img src="https://yqfile.alicdn.com/img_986aacfebb87f4e9573fa2fe87f439d1.png" alt="png"><br><img src="https://yqfile.alicdn.com/img_92cb5ae353459f3dbdcb9e3d0644df5a.png" alt="png"></p> 
   <p>进过抽象化后，我们发现，深度学习框架中的&nbsp;每一个模块都需要两个函数，一个连接正向，一个连接反向。这里的正向和反向，如同武侠小说中的&nbsp;任督二脉。而训练模型的过程，数据通过正向传播生成预测结果，进而将误差反向传回更新参数，就如同让真气通过任督二脉在体内游走，随着训练误差逐渐缩小收敛，深度神经网络也将打通任督二脉。</p> 
   <p>接下来，我们将首先审视一下 Torch 框架的源码如何实现这两部分内容，其次我们通过 Python 直接编写一个最简单的深度学习框架。</p> 
   <p><img src="https://yqfile.alicdn.com/img_24db7abaf74169ffb3774fccc8cd544e.jpg" alt="png"></p> 
   <p>举 Torch 的 nn 项目的例子是因为Torch 的代码文件结构比较简单，Tensorflow 的规律和Torch比较近似，但文件结构相对更加复杂，有兴趣的可以仔细读读<a href="http://www.cnblogs.com/yao62995/p/5773018.html" rel="nofollow">相关文章</a>。</p> 
   <p>Torch&nbsp;<a href="https://github.com/torch/nn/blob/master/" rel="nofollow">nn 模块Github 源码</a>&nbsp;这个目录下的几乎所有 .lua 文件，都有这两个函数：</p> 
   <pre><code class="lang-lua"># <span class="hljs-selector-tag">lua <span class="hljs-selector-tag">function <span class="hljs-selector-tag">xxx<span class="hljs-selector-pseudo">:updateOutput(input) <span class="hljs-selector-tag">input<span class="hljs-selector-class">.THNN<span class="hljs-selector-class">.xxx_updateOutput( <span class="hljs-selector-tag">input<span class="hljs-selector-pseudo">:cdata(), <span class="hljs-selector-tag">self<span class="hljs-selector-class">.output<span class="hljs-selector-pseudo">:cdata() ) <span class="hljs-selector-tag">return <span class="hljs-selector-tag">self<span class="hljs-selector-class">.output <span class="hljs-selector-tag">end <span class="hljs-selector-tag">function <span class="hljs-selector-tag">xxx<span class="hljs-selector-pseudo">:updateGradInput(input, <span class="hljs-selector-tag">gradOutput) <span class="hljs-selector-tag">input<span class="hljs-selector-class">.THNN<span class="hljs-selector-class">.xxx_updateGradInput( <span class="hljs-selector-tag">input<span class="hljs-selector-pseudo">:cdata(), <span class="hljs-selector-tag">gradOutput<span class="hljs-selector-pseudo">:cdata(), <span class="hljs-selector-tag">self<span class="hljs-selector-class">.gradInput<span class="hljs-selector-pseudo">:cdata(), <span class="hljs-selector-tag">self<span class="hljs-selector-class">.output<span class="hljs-selector-pseudo">:cdata() ) <span class="hljs-selector-tag">return <span class="hljs-selector-tag">self<span class="hljs-selector-class">.gradInput <span class="hljs-selector-tag">end </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre> 
   <p>这里其实是相当于留了两个方法的定义，没有写具体功能。具体功能的代码，在&nbsp;<a href="https://github.com/torch/nn/tree/master/lib/THNN/generic" rel="nofollow">./lib/THNN/generic 目录</a>中用 C 实现实现，具体以 Sigmoid 函数举例。</p> 
   <p>我们知道 Sigmoid 函数的形式是：</p> 
   <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/107e59e2de5db1dd92b6ef52ecfadfe0d39c29ea" alt="png"></p> 
   <p>代码实现起来是这样：</p> 
   <pre><code class="lang-lua"><span class="hljs-meta"># lua <span class="hljs-function"><span class="hljs-keyword">void <span class="hljs-title">THNN_<span class="hljs-params">(Sigmoid_updateOutput)<span class="hljs-params">( THNNState *state, THTensor *input, THTensor *output) { THTensor_(resizeAs)(output, input); TH_TENSOR_APPLY2(real, output, real, input, *output_data = <span class="hljs-number">1./(<span class="hljs-number">1.+ <span class="hljs-built_in">exp(- *input_data)); ); } </span></span></span></span></span></span></span></span></span></code></pre> 
   <p>Sigmoid 函数求导变成：</p> 
   <p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/63836efc6aeb3deec8ec9946398bf3996a614d91" alt="png"></p> 
   <p>所以这里在实现的时候就是：</p> 
   <pre><code class="lang-c"><span class="hljs-comment">// c <span class="hljs-function"><span class="hljs-keyword">void <span class="hljs-title">THNN_<span class="hljs-params">(Sigmoid_updateGradInput)<span class="hljs-params">( THNNState *state, THTensor *input, THTensor *gradOutput, THTensor *gradInput, THTensor *output) { THNN_CHECK_NELEMENT(input, gradOutput); THTensor_(resizeAs)(gradInput, output); TH_TENSOR_APPLY3(real, gradInput, real, gradOutput, real, output, real z = * output_data; *gradInput_data = *gradOutput_data * (<span class="hljs-number">1. - z) * z; ); } </span></span></span></span></span></span></span></code></pre> 
   <p>大家应该注意到了一点，&nbsp;<code>updateOutput</code>&nbsp;函数, output_data 在等号左边， input_data 在等号右边。 而&nbsp;<code>updateGradInput</code>&nbsp;函数， gradInput_data 在等号左边， gradOutput_data 在等号右边。 这里，<code>output = f(input)</code>&nbsp;对应的是&nbsp;正向传播&nbsp;<code>input = f(output)</code>&nbsp;对应的是&nbsp;反向传播。</p> 
   <h3>1.2 用 Python 直接编写一个最简单的深度学习框架</h3> 
   <p>这部分内容属于“造轮子”，并且借用了优达学城的一个小型项目&nbsp;<a href="https://github.com/BillZito/miniflow" rel="nofollow">MiniFlow</a>。</p> 
   <h4>数据结构部分</h4> 
   <p>首先，我们实现一个父类&nbsp;<code>Node</code>，然后基于这个父类，依次实现 Input Linear Sigmoid 等模块。这里运用了简单的 Python&nbsp;<a href="http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431865288798deef438d865e4c2985acff7e9fad15e3000" rel="nofollow">Class 继承</a>。这些模块中，需要将&nbsp;<code>forward</code>&nbsp;和&nbsp;<code>backward</code>&nbsp;两个方法针对每个模块分别重写。</p> 
   <p><img src="https://yqfile.alicdn.com/img_9c59fdb322b251d792d23ea8c929593a.png" alt="png"></p> 
   <p>代码如下：</p> 
   <pre><code class="lang-python"><span class="hljs-comment"># python <span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">Node<span class="hljs-params">(object): <span class="hljs-string">""" Base class for nodes in the network. Arguments: `inbound_nodes`: A list of nodes with edges into this node. """ <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self, inbound_nodes=[]): <span class="hljs-string">""" Node's constructor (runs when the object is instantiated). Sets properties that all nodes need. """ <span class="hljs-comment"># A list of nodes with edges into this node. self.inbound_nodes = inbound_nodes <span class="hljs-comment"># The eventual value of this node. Set by running <span class="hljs-comment"># the forward() method. self.value = <span class="hljs-keyword">None <span class="hljs-comment"># A list of nodes that this node outputs to. self.outbound_nodes = [] <span class="hljs-comment"># New property! Keys are the inputs to this node and <span class="hljs-comment"># their values are the partials of this node with <span class="hljs-comment"># respect to that input. self.gradients = {} <span class="hljs-comment"># Sets this node as an outbound node for all of <span class="hljs-comment"># this node's inputs. <span class="hljs-keyword">for node <span class="hljs-keyword">in inbound_nodes: node.outbound_nodes.append(self) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward<span class="hljs-params">(self): <span class="hljs-string">""" Every node that uses this class as a base class will need to define its own `forward` method. """ <span class="hljs-keyword">raise NotImplementedError <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">backward<span class="hljs-params">(self): <span class="hljs-string">""" Every node that uses this class as a base class will need to define its own `backward` method. """ <span class="hljs-keyword">raise NotImplementedError <span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">Input<span class="hljs-params">(Node): <span class="hljs-string">""" A generic input into the network. """ <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self): Node.__init__(self) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward<span class="hljs-params">(self): <span class="hljs-keyword">pass <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">backward<span class="hljs-params">(self): self.gradients = {self: <span class="hljs-number">0} <span class="hljs-keyword">for n <span class="hljs-keyword">in self.outbound_nodes: self.gradients[self] += n.gradients[self] <span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">Linear<span class="hljs-params">(Node): <span class="hljs-string">""" Represents a node that performs a linear transform. """ <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self, X, W, b): Node.__init__(self, [X, W, b]) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward<span class="hljs-params">(self): <span class="hljs-string">""" Performs the math behind a linear transform. """ X = self.inbound_nodes[<span class="hljs-number">0].value W = self.inbound_nodes[<span class="hljs-number">1].value b = self.inbound_nodes[<span class="hljs-number">2].value self.value = np.dot(X, W) + b <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">backward<span class="hljs-params">(self): <span class="hljs-string">""" Calculates the gradient based on the output values. """ self.gradients = {n: np.zeros_like(n.value) <span class="hljs-keyword">for n <span class="hljs-keyword">in self.inbound_nodes} <span class="hljs-keyword">for n <span class="hljs-keyword">in self.outbound_nodes: grad_cost = n.gradients[self] self.gradients[self.inbound_nodes[<span class="hljs-number">0]] += np.dot(grad_cost, self.inbound_nodes[<span class="hljs-number">1].value.T) self.gradients[self.inbound_nodes[<span class="hljs-number">1]] += np.dot(self.inbound_nodes[<span class="hljs-number">0].value.T, grad_cost) self.gradients[self.inbound_nodes[<span class="hljs-number">2]] += np.sum(grad_cost, axis=<span class="hljs-number">0, keepdims=<span class="hljs-keyword">False) <span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">Sigmoid<span class="hljs-params">(Node): <span class="hljs-string">""" Represents a node that performs the sigmoid activation function. """ <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self, node): Node.__init__(self, [node]) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">_sigmoid<span class="hljs-params">(self, x): <span class="hljs-string">""" This method is separate from `forward` because it will be used with `backward` as well. `x`: A numpy array-like object. """ <span class="hljs-keyword">return <span class="hljs-number">1. / (<span class="hljs-number">1. + np.exp(-x)) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward<span class="hljs-params">(self): <span class="hljs-string">""" Perform the sigmoid function and set the value. """ input_value = self.inbound_nodes[<span class="hljs-number">0].value self.value = self._sigmoid(input_value) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">backward<span class="hljs-params">(self): <span class="hljs-string">""" Calculates the gradient using the derivative of the sigmoid function. """ self.gradients = {n: np.zeros_like(n.value) <span class="hljs-keyword">for n <span class="hljs-keyword">in self.inbound_nodes} <span class="hljs-keyword">for n <span class="hljs-keyword">in self.outbound_nodes: grad_cost = n.gradients[self] sigmoid = self.value self.gradients[self.inbound_nodes[<span class="hljs-number">0]] += sigmoid * (<span class="hljs-number">1 - sigmoid) * grad_cost <span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">Tanh<span class="hljs-params">(Node): <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self, node): <span class="hljs-string">""" The tanh cost function. Should be used as the last node for a network. """ Node.__init__(self, [node]) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward<span class="hljs-params">(self): <span class="hljs-string">""" Calculates the tanh. """ input_value = self.inbound_nodes[<span class="hljs-number">0].value self.value = np.tanh(input_value) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">backward<span class="hljs-params">(self): <span class="hljs-string">""" Calculates the gradient of the cost. """ self.gradients = {n: np.zeros_like(n.value) <span class="hljs-keyword">for n <span class="hljs-keyword">in self.inbound_nodes} <span class="hljs-keyword">for n <span class="hljs-keyword">in self.outbound_nodes: grad_cost = n.gradients[self] tanh = self.value self.gradients[self.inbound_nodes[<span class="hljs-number">0]] += (<span class="hljs-number">1 + tanh) * (<span class="hljs-number">1 - tanh) * grad_cost.T <span class="hljs-class"><span class="hljs-keyword">class <span class="hljs-title">MSE<span class="hljs-params">(Node): <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">__init__<span class="hljs-params">(self, y, a): <span class="hljs-string">""" The mean squared error cost function. Should be used as the last node for a network. """ Node.__init__(self, [y, a]) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward<span class="hljs-params">(self): <span class="hljs-string">""" Calculates the mean squared error. """ y = self.inbound_nodes[<span class="hljs-number">0].value.reshape(<span class="hljs-number">-1, <span class="hljs-number">1) a = self.inbound_nodes[<span class="hljs-number">1].value.reshape(<span class="hljs-number">-1, <span class="hljs-number">1) self.m = self.inbound_nodes[<span class="hljs-number">0].value.shape[<span class="hljs-number">0] self.diff = y - a self.value = np.mean(self.diff**<span class="hljs-number">2) <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">backward<span class="hljs-params">(self): <span class="hljs-string">""" Calculates the gradient of the cost. """ self.gradients[self.inbound_nodes[<span class="hljs-number">0]] = (<span class="hljs-number">2 / self.m) * self.diff self.gradients[self.inbound_nodes[<span class="hljs-number">1]] = (<span class="hljs-number">-2 / self.m) * self.diff </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre> 
   <h4>调度算法与优化部分</h4> 
   <p>优化部分则会在以后的系列中单独详细说明。这里主要将简单讲一下图计算的算法调度。就是实际上Tensorflow的各个模块会生成一个有向无环图，如下图（来源<a href="http://www.geeksforgeeks.org/topological-sorting-indegree-based-solution/" rel="nofollow">http://www.geeksforgeeks.org/topological-sorting-indegree-based-solution/</a>）:</p> 
   <p><img src="http://www.geeksforgeeks.org/wp-content/uploads/graph.png" alt="png"></p> 
   <p>在计算过程中，几个模块存在着相互依赖关系，比如要计算模块1，就必须完成模块3和模块4，而要完成模块3，就需要在之前顺次完成模块5、2；因此这里可以使用 Kahn 算法作为调度算法（下面的&nbsp;<code>topological_sort</code>&nbsp;函数），从计算图中，推导出类似 5-&gt;2-&gt;3-&gt;4-&gt;1 的计算顺序。</p> 
   <pre><code class="lang-python"><span class="hljs-comment"># python <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">topological_sort<span class="hljs-params">(feed_dict): <span class="hljs-string">""" Sort the nodes in topological order using Kahn's Algorithm. `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node. Returns a list of sorted nodes. """ input_nodes = [n <span class="hljs-keyword">for n <span class="hljs-keyword">in feed_dict.keys()] G = {} nodes = [n <span class="hljs-keyword">for n <span class="hljs-keyword">in input_nodes] <span class="hljs-keyword">while len(nodes) &gt; <span class="hljs-number">0: n = nodes.pop(<span class="hljs-number">0) <span class="hljs-keyword">if n <span class="hljs-keyword">not <span class="hljs-keyword">in G: G[n] = {<span class="hljs-string">'in': set(), <span class="hljs-string">'out': set()} <span class="hljs-keyword">for m <span class="hljs-keyword">in n.outbound_nodes: <span class="hljs-keyword">if m <span class="hljs-keyword">not <span class="hljs-keyword">in G: G[m] = {<span class="hljs-string">'in': set(), <span class="hljs-string">'out': set()} G[n][<span class="hljs-string">'out'].add(m) G[m][<span class="hljs-string">'in'].add(n) nodes.append(m) L = [] S = set(input_nodes) <span class="hljs-keyword">while len(S) &gt; <span class="hljs-number">0: n = S.pop() <span class="hljs-keyword">if isinstance(n, Input): n.value = feed_dict[n] L.append(n) <span class="hljs-keyword">for m <span class="hljs-keyword">in n.outbound_nodes: G[n][<span class="hljs-string">'out'].remove(m) G[m][<span class="hljs-string">'in'].remove(n) <span class="hljs-keyword">if len(G[m][<span class="hljs-string">'in']) == <span class="hljs-number">0: S.add(m) <span class="hljs-keyword">return L <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">forward_and_backward<span class="hljs-params">(graph): <span class="hljs-string">""" Performs a forward pass and a backward pass through a list of sorted Nodes. Arguments: `graph`: The result of calling `topological_sort`. """ <span class="hljs-keyword">for n <span class="hljs-keyword">in graph: n.forward() <span class="hljs-keyword">for n <span class="hljs-keyword">in graph[::<span class="hljs-number">-1]: n.backward() <span class="hljs-function"><span class="hljs-keyword">def <span class="hljs-title">sgd_update<span class="hljs-params">(trainables, learning_rate=<span class="hljs-number">1e-2): <span class="hljs-string">""" Updates the value of each trainable with SGD. Arguments: `trainables`: A list of `Input` Nodes representing weights/biases. `learning_rate`: The learning rate. """ <span class="hljs-keyword">for t <span class="hljs-keyword">in trainables: t.value = t.value - learning_rate * t.gradients[t] </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre> 
   <h4>使用模型</h4> 
   <pre><code class="lang-python"><span class="hljs-comment"># python import numpy as np from sklearn.utils import resample np.random.seed(0) w1_0 = np.array([[ 0.1, 0.2, 0.3, 0.4], [ 0.5, 0.6, 0.7, 0.8], [ 0.9, 1.0, 1.1, 1.2]]) w2_0 = np.array([[ 1.3, 1.4], [ 1.5, 1.6], [ 1.7, 1.8], [ 1.9, 2.0]]) b1_0 = np.array( [-2.0, -6.0, -1.0, -7.0]) b2_0 = np.array( [-2.5, -5.0]) X_ = np.array([[1.0, 2.0, 3.0]]) y_ = np.array([[-0.85, 0.75]]) n_features = X_.shape[1] W1_ = w1_0 b1_ = b1_0 W2_ = w2_0 b2_ = b2_0 X, y = Input(), Input() W1, b1 = Input(), Input() W2, b2 = Input(), Input() l1 = Linear(X, W1, b1) s1 = Sigmoid(l1) l2 = Linear(s1, W2, b2) t1 = Tanh(l2) cost = MSE(y, t1) feed_dict = { X: X_, y: y_, W1: W1_, b1: b1_, W2: W2_, b2: b2_ } epochs = 10 m = X_.shape[0] batch_size = 1 steps_per_epoch = m // batch_size graph = topological_sort(feed_dict) trainables = [W1, b1, W2, b2] l_Mat_W1 = [w1_0] l_Mat_W2 = [w2_0] l_Mat_out = [] l_val = [] for i in range(epochs): loss = 0 for j in range(steps_per_epoch): X_batch, y_batch = resample(X_, y_, n_samples=batch_size) X.value = X_batch y.value = y_batch forward_and_backward(graph) sgd_update(trainables, 0.1) loss += graph[-1].value mat_W1 = [] mat_W2 = [] for i in graph: try: if (i.value.shape[0] == 3) and (i.value.shape[1] == 4): mat_W1 = i.value if (i.value.shape[0] == 4) and (i.value.shape[1] == 2): mat_W2 = i.value except: pass l_Mat_W1.append(mat_W1) l_Mat_W2.append(mat_W2) l_Mat_out.append(graph[9].value) </span></code></pre> 
   <p>来观察一下。当然还有更高级的可视化方法：<a href="https://jizhi.im/blog/post/v_nn_learn" rel="nofollow">https://jizhi.im/blog/post/v_nn_learn</a></p> 
   <pre><code class="lang-python"><span class="hljs-comment"># python import matplotlib.pyplot <span class="hljs-keyword">as plt %matplotlib inline fig = plt.figure( figsize=(<span class="hljs-number">14,<span class="hljs-number">10)) ax0 = fig.add_subplot(<span class="hljs-number">131) <span class="hljs-comment">#aax0 = fig.add_axes([0, 0, 0.3, 0.1]) c0 = ax0.imshow(np.<span class="hljs-keyword">array(l_Mat_out).reshape([<span class="hljs-number">-1,<span class="hljs-number">2]).T, interpolation=<span class="hljs-string">'nearest',aspect=<span class="hljs-string">'auto', cmap=<span class="hljs-string">"Reds", vmax=<span class="hljs-number">1, vmin=<span class="hljs-number">-1) ax0.set_title(<span class="hljs-string">"Output") cbar = fig.colorbar(c0, ticks=[<span class="hljs-number">-1, <span class="hljs-number">0, <span class="hljs-number">1]) ax1 = fig.add_subplot(<span class="hljs-number">132) c1 = ax1.imshow(np.<span class="hljs-keyword">array(l_Mat_W1).reshape(len(l_Mat_W1), <span class="hljs-number">12).T, interpolation=<span class="hljs-string">'nearest',aspect=<span class="hljs-string">'auto', cmap=<span class="hljs-string">"Reds") ax1.set_title(<span class="hljs-string">"w1") cbar = fig.colorbar(c1, ticks=[np.min(np.<span class="hljs-keyword">array(l_Mat_W1)), np.max(np.<span class="hljs-keyword">array(l_Mat_W1))]) ax2 = fig.add_subplot(<span class="hljs-number">133) c2 = ax2.imshow(np.<span class="hljs-keyword">array(l_Mat_W2).reshape(len(l_Mat_W2), <span class="hljs-number">8).T, interpolation=<span class="hljs-string">'nearest',aspect=<span class="hljs-string">'auto', cmap=<span class="hljs-string">"Reds") ax2.set_title(<span class="hljs-string">"w2") cbar = fig.colorbar(c2, ticks=[np.min(np.<span class="hljs-keyword">array(l_Mat_W2)), np.max(np.<span class="hljs-keyword">array(l_Mat_W2))]) ax0.set_yticks([<span class="hljs-number">0,<span class="hljs-number">1]) ax0.set_yticklabels([<span class="hljs-string">"out0", <span class="hljs-string">"out1"]) ax1.set_xlabel(<span class="hljs-string">"epochs") <span class="hljs-comment">#for i in range(len(l_Mat_W1)): </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></code></pre> 
   <p>我们注意到，随着训练轮数 Epoch 不断增多， Output 值从最初的&nbsp;<code>[0.72, -0.88]</code>&nbsp;不断接近&nbsp;<code>y = [-0.85, 0.72]</code>, 其背后的原因，是模型参数不断的从初始化的值变化、更新，如图中的 w1 w2 两个矩阵。</p> 
   <p><img src="https://yqfile.alicdn.com/img_fb6aa200c085d78d09c33bd9e959d7ca.png" alt="png"></p> 
   <p>好了，最简单的轮子已经造好了。 我们的轮子，实现了&nbsp;<code>Input</code>&nbsp;<code>Linear</code>&nbsp;<code>Sigmoid</code>&nbsp;<code>Tanh</code>&nbsp;以及&nbsp;<code>MSE</code>&nbsp;这几个模块。 接下来的内容，我们将基于现在最火的轮子 Tensorflow，详细介绍一下更多的模块。</p> 
   <p>最后，本篇只是造了个最基本的轮子，我们集智的知乎专栏上，有一个系列文章，正在介绍如何在Matlab上手写深度学习框架，传送门：&nbsp;<a href="https://zhuanlan.zhihu.com/p/26289883?refer=c_81843036" rel="nofollow">matDL框架开发直播:2——全连接层的实现和优化</a>，欢迎大家围观。</p> 
   <p>目前腾讯云 GPU 服务器还在内测阶段，暂时没有申请到内测资格的读者也可以使用普通的云服务器运行本讲的代码。但从第三讲开始，我们将逐渐开始使用 Tensorflow 框架分析相关数据，对应的计算量大大增加，必须租用&nbsp;<a href="https://www.qcloud.com/product/gpu" rel="nofollow">云GPU服务器</a>&nbsp;才可以快速算出结果。服务器的租用方式，以及 Python 编程环境的搭建，我们将以腾讯云 GPU 为例，在接下来的内容中和大家详细介绍。</p> 
   <div> 
    <div>
     【作者】
     <a href="http://www.cnblogs.com/sky-heaven/" rel="nofollow">张昺华</a> 
    </div> 
    <div>
     【出处】
     <a href="http://www.cnblogs.com/sky-heaven/" rel="nofollow">http://www.cnblogs.com/sky-heaven/</a> 
    </div> 
    <div>
     【博客园】 
     <a href="http://www.cnblogs.com/sky-heaven/" rel="nofollow">http://www.cnblogs.com/sky-heaven/</a> 
    </div> 
    <div>
     【新浪博客】 
     <a href="http://blog.sina.com.cn/u/2049150530" rel="nofollow">http://blog.sina.com.cn/u/2049150530</a> 
    </div> 
    <div>
     【知乎】 
     <a href="http://www.zhihu.com/people/zhang-bing-hua" rel="nofollow">http://www.zhihu.com/people/zhang-bing-hua</a> 
    </div> 
    <div>
     【我的作品---旋转倒立摆】 
     <a href="http://v.youku.com/v_show/id_XODM5NDAzNjQw.html?spm=a2hzp.8253869.0.0&amp;from=y1.7-2" rel="nofollow">http://v.youku.com/v_show/id_XODM5NDAzNjQw.html?spm=a2hzp.8253869.0.0&amp;from=y1.7-2</a> 
    </div> 
    <div>
     【我的作品---自平衡自动循迹车】 
     <a href="http://v.youku.com/v_show/id_XODM5MzYyNTIw.html?spm=a2hzp.8253869.0.0&amp;from=y1.7-2" rel="nofollow">http://v.youku.com/v_show/id_XODM5MzYyNTIw.html?spm=a2hzp.8253869.0.0&amp;from=y1.7-2</a> 
    </div> 
    <div>
     【新浪微博】 张昺华--sky
    </div> 
    <div>
     【twitter】 @sky2030_
    </div> 
    <div>
     【facebook】 张昺华 zhangbinghua
    </div> 
    <div>
     本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利.
    </div> 
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
