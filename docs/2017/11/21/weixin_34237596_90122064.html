<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Hadoop MapReduce编程 API入门系列之倒排索引（二十四） « NotBeCN</title>
  <meta name="description" content="                                                         2016-12-12 21:54:04,509 INFO [org.apache.hadoop.metrics.jvm.JvmMetrics] - Initializing JVM Metrics w...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/21/weixin_34237596_90122064.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Hadoop MapReduce编程 API入门系列之倒排索引（二十四）</h1>
    <p class="post-meta">Nov 21, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212214957901-1187886516.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215037386-720939514.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215232214-245210661.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215240854-1586426383.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215250433-1000347115.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215301636-1134938933.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215307573-1736495080.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215318792-1212947802.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215324792-542759851.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215357167-911966500.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215415604-1660417080.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">2016-12-12 21:54:04,509 INFO [org.apache.hadoop.metrics.jvm.JvmMetrics] - Initializing JVM Metrics with processName=JobTracker, sessionId=<br> 2016-12-12 21:54:05,166 WARN [org.apache.hadoop.mapreduce.JobSubmitter] - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.<br> 2016-12-12 21:54:05,169 WARN [org.apache.hadoop.mapreduce.JobSubmitter] - No job jar file set. User classes may not be found. See Job or Job#setJar(String).<br> 2016-12-12 21:54:05,477 INFO [org.apache.hadoop.mapreduce.lib.input.FileInputFormat] - Total input paths to process : 3<br> 2016-12-12 21:54:05,539 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:3<br> 2016-12-12 21:54:05,810 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - Submitting tokens for job: job_local1000661716_0001<br> 2016-12-12 21:54:06,184 INFO [org.apache.hadoop.mapreduce.Job] - The url to track the job: http://localhost:8080/<br> 2016-12-12 21:54:06,185 INFO [org.apache.hadoop.mapreduce.Job] - Running job: job_local1000661716_0001<br> 2016-12-12 21:54:06,193 INFO [org.apache.hadoop.mapred.LocalJobRunner] - OutputCommitter set in config null<br> 2016-12-12 21:54:06,220 INFO [org.apache.hadoop.mapred.LocalJobRunner] - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter<br> 2016-12-12 21:54:06,297 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Waiting for map tasks<br> 2016-12-12 21:54:06,314 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Starting task: attempt_local1000661716_0001_m_000000_0<br> 2016-12-12 21:54:06,374 INFO [org.apache.hadoop.yarn.util.ProcfsBasedProcessTree] - ProcfsBasedProcessTree currently is supported only on Linux.<br> 2016-12-12 21:54:06,433 INFO [org.apache.hadoop.mapred.Task] - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b4d160c<br> 2016-12-12 21:54:06,441 INFO [org.apache.hadoop.mapred.MapTask] - Processing split: file:/D:/Code/MyEclipseJavaCode/myMapReduce/data/inverseIndex/b.txt:0+35<br> 2016-12-12 21:54:06,515 INFO [org.apache.hadoop.mapred.MapTask] - (EQUATOR) 0 kvi 26214396(104857584)<br> 2016-12-12 21:54:06,516 INFO [org.apache.hadoop.mapred.MapTask] - mapreduce.task.io.sort.mb: 100<br> 2016-12-12 21:54:06,517 INFO [org.apache.hadoop.mapred.MapTask] - soft limit at 83886080<br> 2016-12-12 21:54:06,517 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufvoid = 104857600<br> 2016-12-12 21:54:06,517 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396; length = 6553600<br> 2016-12-12 21:54:06,544 INFO [org.apache.hadoop.mapred.MapTask] - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer<br> 2016-12-12 21:54:06,567 INFO [org.apache.hadoop.mapred.LocalJobRunner] -&nbsp;<br> 2016-12-12 21:54:06,567 INFO [org.apache.hadoop.mapred.MapTask] - Starting flush of map output<br> 2016-12-12 21:54:06,567 INFO [org.apache.hadoop.mapred.MapTask] - Spilling map output<br> 2016-12-12 21:54:06,568 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufend = 130; bufvoid = 104857600<br> 2016-12-12 21:54:06,568 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600<br> 2016-12-12 21:54:06,590 INFO [org.apache.hadoop.mapred.MapTask] - Finished spill 0<br> 2016-12-12 21:54:06,599 INFO [org.apache.hadoop.mapred.Task] - Task:attempt_local1000661716_0001_m_000000_0 is done. And is in the process of committing<br> 2016-12-12 21:54:06,631 INFO [org.apache.hadoop.mapred.LocalJobRunner] - map<br> 2016-12-12 21:54:06,631 INFO [org.apache.hadoop.mapred.Task] - Task 'attempt_local1000661716_0001_m_000000_0' done.<br> 2016-12-12 21:54:06,631 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Finishing task: attempt_local1000661716_0001_m_000000_0<br> 2016-12-12 21:54:06,631 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Starting task: attempt_local1000661716_0001_m_000001_0<br> 2016-12-12 21:54:06,637 INFO [org.apache.hadoop.yarn.util.ProcfsBasedProcessTree] - ProcfsBasedProcessTree currently is supported only on Linux.<br> 2016-12-12 21:54:06,687 INFO [org.apache.hadoop.mapred.Task] - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@418b04a5<br> 2016-12-12 21:54:06,691 INFO [org.apache.hadoop.mapred.MapTask] - Processing split: file:/D:/Code/MyEclipseJavaCode/myMapReduce/data/inverseIndex/a.txt:0+33<br> 2016-12-12 21:54:06,742 INFO [org.apache.hadoop.mapred.MapTask] - (EQUATOR) 0 kvi 26214396(104857584)<br> 2016-12-12 21:54:06,742 INFO [org.apache.hadoop.mapred.MapTask] - mapreduce.task.io.sort.mb: 100<br> 2016-12-12 21:54:06,742 INFO [org.apache.hadoop.mapred.MapTask] - soft limit at 83886080<br> 2016-12-12 21:54:06,742 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufvoid = 104857600<br> 2016-12-12 21:54:06,743 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396; length = 6553600<br> 2016-12-12 21:54:06,744 INFO [org.apache.hadoop.mapred.MapTask] - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer<br> 2016-12-12 21:54:06,747 INFO [org.apache.hadoop.mapred.LocalJobRunner] -&nbsp;<br> 2016-12-12 21:54:06,748 INFO [org.apache.hadoop.mapred.MapTask] - Starting flush of map output<br> 2016-12-12 21:54:06,748 INFO [org.apache.hadoop.mapred.MapTask] - Spilling map output<br> 2016-12-12 21:54:06,748 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufend = 128; bufvoid = 104857600<br> 2016-12-12 21:54:06,748 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600<br> 2016-12-12 21:54:06,756 INFO [org.apache.hadoop.mapred.MapTask] - Finished spill 0<br> 2016-12-12 21:54:06,761 INFO [org.apache.hadoop.mapred.Task] - Task:attempt_local1000661716_0001_m_000001_0 is done. And is in the process of committing<br> 2016-12-12 21:54:06,766 INFO [org.apache.hadoop.mapred.LocalJobRunner] - map<br> 2016-12-12 21:54:06,766 INFO [org.apache.hadoop.mapred.Task] - Task 'attempt_local1000661716_0001_m_000001_0' done.<br> 2016-12-12 21:54:06,766 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Finishing task: attempt_local1000661716_0001_m_000001_0<br> 2016-12-12 21:54:06,766 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Starting task: attempt_local1000661716_0001_m_000002_0<br> 2016-12-12 21:54:06,769 INFO [org.apache.hadoop.yarn.util.ProcfsBasedProcessTree] - ProcfsBasedProcessTree currently is supported only on Linux.<br> 2016-12-12 21:54:06,797 INFO [org.apache.hadoop.mapred.Task] - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@30616f6c<br> 2016-12-12 21:54:06,800 INFO [org.apache.hadoop.mapred.MapTask] - Processing split: file:/D:/Code/MyEclipseJavaCode/myMapReduce/data/inverseIndex/c.txt:0+22<br> 2016-12-12 21:54:06,879 INFO [org.apache.hadoop.mapred.MapTask] - (EQUATOR) 0 kvi 26214396(104857584)<br> 2016-12-12 21:54:06,879 INFO [org.apache.hadoop.mapred.MapTask] - mapreduce.task.io.sort.mb: 100<br> 2016-12-12 21:54:06,879 INFO [org.apache.hadoop.mapred.MapTask] - soft limit at 83886080<br> 2016-12-12 21:54:06,880 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufvoid = 104857600<br> 2016-12-12 21:54:06,880 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396; length = 6553600<br> 2016-12-12 21:54:06,881 INFO [org.apache.hadoop.mapred.MapTask] - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer<br> 2016-12-12 21:54:06,884 INFO [org.apache.hadoop.mapred.LocalJobRunner] -&nbsp;<br> 2016-12-12 21:54:06,884 INFO [org.apache.hadoop.mapred.MapTask] - Starting flush of map output<br> 2016-12-12 21:54:06,884 INFO [org.apache.hadoop.mapred.MapTask] - Spilling map output<br> 2016-12-12 21:54:06,884 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufend = 86; bufvoid = 104857600<br> 2016-12-12 21:54:06,884 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600<br> 2016-12-12 21:54:06,891 INFO [org.apache.hadoop.mapred.MapTask] - Finished spill 0<br> 2016-12-12 21:54:06,895 INFO [org.apache.hadoop.mapred.Task] - Task:attempt_local1000661716_0001_m_000002_0 is done. And is in the process of committing<br> 2016-12-12 21:54:06,898 INFO [org.apache.hadoop.mapred.LocalJobRunner] - map<br> 2016-12-12 21:54:06,898 INFO [org.apache.hadoop.mapred.Task] - Task 'attempt_local1000661716_0001_m_000002_0' done.<br> 2016-12-12 21:54:06,899 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Finishing task: attempt_local1000661716_0001_m_000002_0<br> 2016-12-12 21:54:06,899 INFO [org.apache.hadoop.mapred.LocalJobRunner] - map task executor complete.<br> 2016-12-12 21:54:06,903 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Waiting for reduce tasks<br> 2016-12-12 21:54:06,903 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Starting task: attempt_local1000661716_0001_r_000000_0<br> 2016-12-12 21:54:06,917 INFO [org.apache.hadoop.yarn.util.ProcfsBasedProcessTree] - ProcfsBasedProcessTree currently is supported only on Linux.<br> 2016-12-12 21:54:06,948 INFO [org.apache.hadoop.mapred.Task] - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@43234903<br> 2016-12-12 21:54:06,954 INFO [org.apache.hadoop.mapred.ReduceTask] - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@a609d4<br> 2016-12-12 21:54:06,979 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - MergerManager: memoryLimit=1327077760, maxSingleShuffleLimit=331769440, mergeThreshold=875871360, ioSortFactor=10, memToMemMergeOutputsThreshold=10<br> 2016-12-12 21:54:06,996 INFO [org.apache.hadoop.mapreduce.task.reduce.EventFetcher] - attempt_local1000661716_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events<br> 2016-12-12 21:54:07,040 INFO [org.apache.hadoop.mapreduce.task.reduce.LocalFetcher] - localfetcher#1 about to shuffle output of map attempt_local1000661716_0001_m_000000_0 decomp: 144 len: 148 to MEMORY<br> 2016-12-12 21:54:07,052 INFO [org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput] - Read 144 bytes from map-output for attempt_local1000661716_0001_m_000000_0<br> 2016-12-12 21:54:07,099 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - closeInMemoryFile -&gt; map-output of size: 144, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;144<br> 2016-12-12 21:54:07,103 INFO [org.apache.hadoop.mapreduce.task.reduce.LocalFetcher] - localfetcher#1 about to shuffle output of map attempt_local1000661716_0001_m_000001_0 decomp: 142 len: 146 to MEMORY<br> 2016-12-12 21:54:07,105 INFO [org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput] - Read 142 bytes from map-output for attempt_local1000661716_0001_m_000001_0<br> 2016-12-12 21:54:07,105 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - closeInMemoryFile -&gt; map-output of size: 142, inMemoryMapOutputs.size() -&gt; 2, commitMemory -&gt; 144, usedMemory -&gt;286<br> 2016-12-12 21:54:07,110 INFO [org.apache.hadoop.mapreduce.task.reduce.LocalFetcher] - localfetcher#1 about to shuffle output of map attempt_local1000661716_0001_m_000002_0 decomp: 96 len: 100 to MEMORY<br> 2016-12-12 21:54:07,112 INFO [org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput] - Read 96 bytes from map-output for attempt_local1000661716_0001_m_000002_0<br> 2016-12-12 21:54:07,112 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - closeInMemoryFile -&gt; map-output of size: 96, inMemoryMapOutputs.size() -&gt; 3, commitMemory -&gt; 286, usedMemory -&gt;382<br> 2016-12-12 21:54:07,113 INFO [org.apache.hadoop.mapreduce.task.reduce.EventFetcher] - EventFetcher is interrupted.. Returning<br> 2016-12-12 21:54:07,114 INFO [org.apache.hadoop.mapred.LocalJobRunner] - 3 / 3 copied.<br> 2016-12-12 21:54:07,115 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs<br> 2016-12-12 21:54:07,130 INFO [org.apache.hadoop.mapred.Merger] - Merging 3 sorted segments<br> 2016-12-12 21:54:07,131 INFO [org.apache.hadoop.mapred.Merger] - Down to the last merge-pass, with 3 segments left of total size: 334 bytes<br> 2016-12-12 21:54:07,133 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - Merged 3 segments, 382 bytes to disk to satisfy reduce memory limit<br> 2016-12-12 21:54:07,133 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - Merging 1 files, 382 bytes from disk<br> 2016-12-12 21:54:07,134 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - Merging 0 segments, 0 bytes from memory into reduce<br> 2016-12-12 21:54:07,134 INFO [org.apache.hadoop.mapred.Merger] - Merging 1 sorted segments<br> 2016-12-12 21:54:07,136 INFO [org.apache.hadoop.mapred.Merger] - Down to the last merge-pass, with 1 segments left of total size: 362 bytes<br> 2016-12-12 21:54:07,136 INFO [org.apache.hadoop.mapred.LocalJobRunner] - 3 / 3 copied.<br> 2016-12-12 21:54:07,144 INFO [org.apache.hadoop.conf.Configuration.deprecation] - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords<br> 2016-12-12 21:54:07,163 INFO [org.apache.hadoop.mapred.Task] - Task:attempt_local1000661716_0001_r_000000_0 is done. And is in the process of committing<br> 2016-12-12 21:54:07,166 INFO [org.apache.hadoop.mapred.LocalJobRunner] - 3 / 3 copied.<br> 2016-12-12 21:54:07,166 INFO [org.apache.hadoop.mapred.Task] - Task attempt_local1000661716_0001_r_000000_0 is allowed to commit now<br> 2016-12-12 21:54:07,172 INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_local1000661716_0001_r_000000_0' to file:/D:/Code/MyEclipseJavaCode/myMapReduce/out/InverseIndexStepOne/_temporary/0/task_local1000661716_0001_r_000000<br> 2016-12-12 21:54:07,173 INFO [org.apache.hadoop.mapred.LocalJobRunner] - reduce &gt; reduce<br> 2016-12-12 21:54:07,173 INFO [org.apache.hadoop.mapred.Task] - Task 'attempt_local1000661716_0001_r_000000_0' done.<br> 2016-12-12 21:54:07,174 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Finishing task: attempt_local1000661716_0001_r_000000_0<br> 2016-12-12 21:54:07,174 INFO [org.apache.hadoop.mapred.LocalJobRunner] - reduce task executor complete.<br> 2016-12-12 21:54:07,189 INFO [org.apache.hadoop.mapreduce.Job] - Job job_local1000661716_0001 running in uber mode : false<br> 2016-12-12 21:54:07,191 INFO [org.apache.hadoop.mapreduce.Job] - map 100% reduce 100%<br> 2016-12-12 21:54:07,193 INFO [org.apache.hadoop.mapreduce.Job] - Job job_local1000661716_0001 completed successfully<br> 2016-12-12 21:54:07,223 INFO [org.apache.hadoop.mapreduce.Job] - Counters: 33<br> File System Counters<br> FILE: Number of bytes read=5146<br> FILE: Number of bytes written=777798<br> FILE: Number of read operations=0<br> FILE: Number of large read operations=0<br> FILE: Number of write operations=0<br> Map-Reduce Framework<br> Map input records=8<br> Map output records=16<br> Map output bytes=344<br> Map output materialized bytes=394<br> Input split bytes=396<br> Combine input records=0<br> Combine output records=0<br> Reduce input groups=9<br> Reduce shuffle bytes=394<br> Reduce input records=16<br> Reduce output records=9<br> Spilled Records=32<br> Shuffled Maps =3<br> Failed Shuffles=0<br> Merged Map outputs=3<br> GC time elapsed (ms)=0<br> CPU time spent (ms)=0<br> Physical memory (bytes) snapshot=0<br> Virtual memory (bytes) snapshot=0<br> Total committed heap usage (bytes)=1460142080<br> Shuffle Errors<br> BAD_ID=0<br> CONNECTION=0<br> IO_ERROR=0<br> WRONG_LENGTH=0<br> WRONG_MAP=0<br> WRONG_REDUCE=0<br> File Input Format Counters&nbsp;<br> Bytes Read=90<br> File Output Format Counters&nbsp;<br> Bytes Written=150</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215436995-1927098727.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215447698-532532714.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215513386-741232109.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215520683-1312569689.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">2016-12-12 21:55:03,523 INFO [org.apache.hadoop.metrics.jvm.JvmMetrics] - Initializing JVM Metrics with processName=JobTracker, sessionId=<br> 2016-12-12 21:55:05,038 WARN [org.apache.hadoop.mapreduce.JobSubmitter] - Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.<br> 2016-12-12 21:55:05,044 WARN [org.apache.hadoop.mapreduce.JobSubmitter] - No job jar file set. User classes may not be found. See Job or Job#setJar(String).<br> 2016-12-12 21:55:05,350 INFO [org.apache.hadoop.mapreduce.lib.input.FileInputFormat] - Total input paths to process : 1<br> 2016-12-12 21:55:05,428 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:1<br> 2016-12-12 21:55:05,846 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - Submitting tokens for job: job_local549789154_0001<br> 2016-12-12 21:55:06,425 INFO [org.apache.hadoop.mapreduce.Job] - The url to track the job: http://localhost:8080/<br> 2016-12-12 21:55:06,427 INFO [org.apache.hadoop.mapreduce.Job] - Running job: job_local549789154_0001<br> 2016-12-12 21:55:06,488 INFO [org.apache.hadoop.mapred.LocalJobRunner] - OutputCommitter set in config null<br> 2016-12-12 21:55:06,510 INFO [org.apache.hadoop.mapred.LocalJobRunner] - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter<br> 2016-12-12 21:55:06,605 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Waiting for map tasks<br> 2016-12-12 21:55:06,609 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Starting task: attempt_local549789154_0001_m_000000_0<br> 2016-12-12 21:55:06,691 INFO [org.apache.hadoop.yarn.util.ProcfsBasedProcessTree] - ProcfsBasedProcessTree currently is supported only on Linux.<br> 2016-12-12 21:55:06,728 INFO [org.apache.hadoop.mapred.Task] - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@550aaabb<br> 2016-12-12 21:55:06,738 INFO [org.apache.hadoop.mapred.MapTask] - Processing split: file:/D:/Code/MyEclipseJavaCode/myMapReduce/out/InverseIndexStepOne/part-r-00000:0+138<br> 2016-12-12 21:55:06,821 INFO [org.apache.hadoop.mapred.MapTask] - (EQUATOR) 0 kvi 26214396(104857584)<br> 2016-12-12 21:55:06,821 INFO [org.apache.hadoop.mapred.MapTask] - mapreduce.task.io.sort.mb: 100<br> 2016-12-12 21:55:06,821 INFO [org.apache.hadoop.mapred.MapTask] - soft limit at 83886080<br> 2016-12-12 21:55:06,821 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufvoid = 104857600<br> 2016-12-12 21:55:06,821 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396; length = 6553600<br> 2016-12-12 21:55:06,828 INFO [org.apache.hadoop.mapred.MapTask] - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer<br> 2016-12-12 21:55:06,851 INFO [org.apache.hadoop.mapred.LocalJobRunner] -&nbsp;<br> 2016-12-12 21:55:06,852 INFO [org.apache.hadoop.mapred.MapTask] - Starting flush of map output<br> 2016-12-12 21:55:06,852 INFO [org.apache.hadoop.mapred.MapTask] - Spilling map output<br> 2016-12-12 21:55:06,852 INFO [org.apache.hadoop.mapred.MapTask] - bufstart = 0; bufend = 138; bufvoid = 104857600<br> 2016-12-12 21:55:06,852 INFO [org.apache.hadoop.mapred.MapTask] - kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600<br> 2016-12-12 21:55:06,882 INFO [org.apache.hadoop.mapred.MapTask] - Finished spill 0<br> 2016-12-12 21:55:06,895 INFO [org.apache.hadoop.mapred.Task] - Task:attempt_local549789154_0001_m_000000_0 is done. And is in the process of committing<br> 2016-12-12 21:55:06,919 INFO [org.apache.hadoop.mapred.LocalJobRunner] - map<br> 2016-12-12 21:55:06,920 INFO [org.apache.hadoop.mapred.Task] - Task 'attempt_local549789154_0001_m_000000_0' done.<br> 2016-12-12 21:55:06,920 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Finishing task: attempt_local549789154_0001_m_000000_0<br> 2016-12-12 21:55:06,921 INFO [org.apache.hadoop.mapred.LocalJobRunner] - map task executor complete.<br> 2016-12-12 21:55:06,927 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Waiting for reduce tasks<br> 2016-12-12 21:55:06,928 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Starting task: attempt_local549789154_0001_r_000000_0<br> 2016-12-12 21:55:06,948 INFO [org.apache.hadoop.yarn.util.ProcfsBasedProcessTree] - ProcfsBasedProcessTree currently is supported only on Linux.<br> 2016-12-12 21:55:06,996 INFO [org.apache.hadoop.mapred.Task] - Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1c50c5b8<br> 2016-12-12 21:55:07,002 INFO [org.apache.hadoop.mapred.ReduceTask] - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@311e2a2d<br> 2016-12-12 21:55:07,024 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - MergerManager: memoryLimit=1327077760, maxSingleShuffleLimit=331769440, mergeThreshold=875871360, ioSortFactor=10, memToMemMergeOutputsThreshold=10<br> 2016-12-12 21:55:07,029 INFO [org.apache.hadoop.mapreduce.task.reduce.EventFetcher] - attempt_local549789154_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events<br> 2016-12-12 21:55:07,073 INFO [org.apache.hadoop.mapreduce.task.reduce.LocalFetcher] - localfetcher#1 about to shuffle output of map attempt_local549789154_0001_m_000000_0 decomp: 158 len: 162 to MEMORY<br> 2016-12-12 21:55:07,079 INFO [org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput] - Read 158 bytes from map-output for attempt_local549789154_0001_m_000000_0<br> 2016-12-12 21:55:07,154 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - closeInMemoryFile -&gt; map-output of size: 158, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;158<br> 2016-12-12 21:55:07,156 INFO [org.apache.hadoop.mapreduce.task.reduce.EventFetcher] - EventFetcher is interrupted.. Returning<br> 2016-12-12 21:55:07,157 INFO [org.apache.hadoop.mapred.LocalJobRunner] - 1 / 1 copied.<br> 2016-12-12 21:55:07,158 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs<br> 2016-12-12 21:55:07,173 INFO [org.apache.hadoop.mapred.Merger] - Merging 1 sorted segments<br> 2016-12-12 21:55:07,173 INFO [org.apache.hadoop.mapred.Merger] - Down to the last merge-pass, with 1 segments left of total size: 150 bytes<br> 2016-12-12 21:55:07,175 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - Merged 1 segments, 158 bytes to disk to satisfy reduce memory limit<br> 2016-12-12 21:55:07,176 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - Merging 1 files, 162 bytes from disk<br> 2016-12-12 21:55:07,177 INFO [org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl] - Merging 0 segments, 0 bytes from memory into reduce<br> 2016-12-12 21:55:07,177 INFO [org.apache.hadoop.mapred.Merger] - Merging 1 sorted segments<br> 2016-12-12 21:55:07,179 INFO [org.apache.hadoop.mapred.Merger] - Down to the last merge-pass, with 1 segments left of total size: 150 bytes<br> 2016-12-12 21:55:07,180 INFO [org.apache.hadoop.mapred.LocalJobRunner] - 1 / 1 copied.<br> 2016-12-12 21:55:07,188 INFO [org.apache.hadoop.conf.Configuration.deprecation] - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords<br> 2016-12-12 21:55:07,202 INFO [org.apache.hadoop.mapred.Task] - Task:attempt_local549789154_0001_r_000000_0 is done. And is in the process of committing<br> 2016-12-12 21:55:07,206 INFO [org.apache.hadoop.mapred.LocalJobRunner] - 1 / 1 copied.<br> 2016-12-12 21:55:07,206 INFO [org.apache.hadoop.mapred.Task] - Task attempt_local549789154_0001_r_000000_0 is allowed to commit now<br> 2016-12-12 21:55:07,217 INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_local549789154_0001_r_000000_0' to file:/D:/Code/MyEclipseJavaCode/myMapReduce/out/InverseIndexStepTwo/_temporary/0/task_local549789154_0001_r_000000<br> 2016-12-12 21:55:07,219 INFO [org.apache.hadoop.mapred.LocalJobRunner] - reduce &gt; reduce<br> 2016-12-12 21:55:07,219 INFO [org.apache.hadoop.mapred.Task] - Task 'attempt_local549789154_0001_r_000000_0' done.<br> 2016-12-12 21:55:07,219 INFO [org.apache.hadoop.mapred.LocalJobRunner] - Finishing task: attempt_local549789154_0001_r_000000_0<br> 2016-12-12 21:55:07,223 INFO [org.apache.hadoop.mapred.LocalJobRunner] - reduce task executor complete.<br> 2016-12-12 21:55:07,431 INFO [org.apache.hadoop.mapreduce.Job] - Job job_local549789154_0001 running in uber mode : false<br> 2016-12-12 21:55:07,433 INFO [org.apache.hadoop.mapreduce.Job] - map 100% reduce 100%<br> 2016-12-12 21:55:07,435 INFO [org.apache.hadoop.mapreduce.Job] - Job job_local549789154_0001 completed successfully<br> 2016-12-12 21:55:07,453 INFO [org.apache.hadoop.mapreduce.Job] - Counters: 33<br> File System Counters<br> FILE: Number of bytes read=1072<br> FILE: Number of bytes written=386015<br> FILE: Number of read operations=0<br> FILE: Number of large read operations=0<br> FILE: Number of write operations=0<br> Map-Reduce Framework<br> Map input records=9<br> Map output records=9<br> Map output bytes=138<br> Map output materialized bytes=162<br> Input split bytes=145<br> Combine input records=0<br> Combine output records=0<br> Reduce input groups=3<br> Reduce shuffle bytes=162<br> Reduce input records=9<br> Reduce output records=3<br> Spilled Records=18<br> Shuffled Maps =1<br> Failed Shuffles=0<br> Merged Map outputs=1<br> GC time elapsed (ms)=0<br> CPU time spent (ms)=0<br> Physical memory (bytes) snapshot=0<br> Virtual memory (bytes) snapshot=0<br> Total committed heap usage (bytes)=466616320<br> Shuffle Errors<br> BAD_ID=0<br> CONNECTION=0<br> IO_ERROR=0<br> WRONG_LENGTH=0<br> WRONG_MAP=0<br> WRONG_REDUCE=0<br> File Input Format Counters&nbsp;<br> Bytes Read=158<br> File Output Format Counters&nbsp;<br> Bytes Written=121</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215546089-138894791.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201612/855959-20161212215614964-1857958289.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">代码</span></strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">package zhouls.bigdata.myMapReduce.InverseIndex;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">import java.io.IOException;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">import org.apache.commons.lang.StringUtils;<br> import org.apache.hadoop.conf.Configuration;<br> import org.apache.hadoop.fs.FileSystem;<br> import org.apache.hadoop.fs.Path;<br> import org.apache.hadoop.io.LongWritable;<br> import org.apache.hadoop.io.NullWritable;<br> import org.apache.hadoop.io.Text;<br> import org.apache.hadoop.mapreduce.InputSplit;<br> import org.apache.hadoop.mapreduce.Job;<br> import org.apache.hadoop.mapreduce.Mapper;<br> import org.apache.hadoop.mapreduce.Reducer;<br> import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br> import org.apache.hadoop.mapreduce.lib.input.FileSplit;<br> import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">/**<br> * 倒排索引步骤一job<br> *&nbsp;<br> *<br> */<br> public class InverseIndexStepOne {<br><br><br><br> public static class StepOneMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt;{<br><br> @Override<br> protected void map(LongWritable key, Text value,Context context)<br> throws IOException, InterruptedException {</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">//拿到一行数据<br> String line = value.toString();<br> //切分出各个单词<br> String[] fields = StringUtils.split(line, " ");<br><br> //获取这一行数据所在的文件切片<br> FileSplit inputSplit = (FileSplit) context.getInputSplit();<br> //从文件切片中获取文件名<br> String fileName = inputSplit.getPath().getName();<br><br> for(String field:fields){<br><br> //封装kv输出 ， k ： hello--&gt;a.txt v: 1<br> context.write(new Text(field+"--&gt;"+fileName), new LongWritable(1));<br><br> }<br><br> }<br><br><br> }<br><br><br> public static class StepOneReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;{<br><br> // &lt;hello--&gt;a.txt,{1,1,1....}&gt;<br> @Override<br> protected void reduce(Text key, Iterable&lt;LongWritable&gt; values,Context context)<br> throws IOException, InterruptedException {</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">long counter = 0;<br> for(LongWritable value:values){<br><br> counter += value.get();<br><br> }<br><br> context.write(key, new LongWritable(counter));<br> }<br><br><br> }<br><br> public static void main(String[] args) throws Exception {</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">Configuration conf = new Configuration(); <br> Job job = Job.getInstance(conf);<br><br> job.setJarByClass(InverseIndexStepOne.class);<br><br> job.setMapperClass(StepOneMapper.class);<br> job.setReducerClass(StepOneReducer.class);<br><br> job.setOutputKeyClass(Text.class);<br> job.setOutputValueClass(LongWritable.class);<br><br> // FileInputFormat.setInputPaths(job, new Path("hdfs://HadoopMaster:9000/inverseIndex/"));<br> // <br> // //检查一下参数所指定的输出路径是否存在，如果已存在，先删除<br> // Path output = new Path("hdfs://HadoopMaster:9000/out/InverseIndexStepOne/");<br> // <br><br> FileInputFormat.setInputPaths(job, new Path("./data/inverseIndex/"));<br><br> //检查一下参数所指定的输出路径是否存在，如果已存在，先删除<br> Path output = new Path("./out/InverseIndexStepOne");<br><br><br><br> FileSystem fs = FileSystem.get(conf);<br> if(fs.exists(output)){<br> fs.delete(output, true);<br> }<br><br> FileOutputFormat.setOutputPath(job, output);<br><br> System.exit(job.waitForCompletion(true)?0:1);<br><br><br><br><br> }</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">}</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">package zhouls.bigdata.myMapReduce.InverseIndex;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">import java.io.IOException;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">import org.apache.commons.lang.StringUtils;<br> import org.apache.hadoop.conf.Configuration;<br> import org.apache.hadoop.fs.FileSystem;<br> import org.apache.hadoop.fs.Path;<br> import org.apache.hadoop.io.LongWritable;<br> import org.apache.hadoop.io.Text;<br> import org.apache.hadoop.mapreduce.Job;<br> import org.apache.hadoop.mapreduce.Mapper;<br> import org.apache.hadoop.mapreduce.Mapper.Context;<br> import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br> import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br> import org.apache.hadoop.mapreduce.Reducer;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">import zhouls.bigdata.myMapReduce.InverseIndex.InverseIndexStepOne.StepOneMapper;<br> import zhouls.bigdata.myMapReduce.InverseIndex.InverseIndexStepOne.StepOneReducer;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">public class InverseIndexStepTwo {</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br> public static class StepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;{<br><br><br> //k: 行起始偏移量 v: {hello--&gt;a.txt 3}&nbsp;<br> @Override<br> protected void map(LongWritable key, Text value,Context context)<br> throws IOException, InterruptedException {<br><br> String line = value.toString();<br><br> String[] fields = StringUtils.split(line, "\t");<br> String[] wordAndfileName = StringUtils.split(fields[0], "--&gt;");<br><br> String word = wordAndfileName[0];<br> String fileName = wordAndfileName[1];<br> long count = Long.parseLong(fields[1]);<br><br><br> context.write(new Text(word), new Text(fileName+"--&gt;"+count)); <br> //map输出的结果是这个形式 ： &lt;hello,a.txt--&gt;3&gt;<br><br> }<br> }</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br> public static class StepTwoReducer extends Reducer&lt;Text, Text,Text, Text&gt;{<br><br> @Override<br> protected void reduce(Text key, Iterable&lt;Text&gt; values,Context context)<br> throws IOException, InterruptedException {</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">//拿到的数据 &lt;hello,{a.txt--&gt;3,b.txt--&gt;2,c.txt--&gt;1}&gt;<br><br> String result = "";<br><br> for(Text value:values){<br><br> result += value + " ";<br> }<br><br> context.write(key, new Text(result));<br> //输出的结果就是 k: hello v: a.txt--&gt;3 b.txt--&gt;2 c.txt--&gt;1&nbsp;<br><br> }<br><br> }</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">public static void main(String[] args) throws Exception {</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">Configuration conf = new Configuration(); <br><br> //先构造job_one<br> // Job job_one = Job.getInstance(conf);<br> // <br> // job_one.setJarByClass(InverseIndexStepTwo.class);<br> // job_one.setMapperClass(StepOneMapper.class);<br> // job_one.setReducerClass(StepOneReducer.class);<br> //......<br><br><br> //构造job_two<br> Job job_tow = Job.getInstance(conf);<br><br> job_tow.setJarByClass(InverseIndexStepTwo.class);<br><br> job_tow.setMapperClass(StepTwoMapper.class);<br> job_tow.setReducerClass(StepTwoReducer.class);<br><br> job_tow.setOutputKeyClass(Text.class);<br> job_tow.setOutputValueClass(Text.class);<br><br> // FileInputFormat.setInputPaths(job_tow, new Path("hdfs://HadoopMaster:9000/out/InverseIndexStepOne/"));<br> // <br> // //检查一下参数所指定的输出路径是否存在，如果已存在，先删除<br> // Path output = new Path("hdfs://HadoopMaster:9000/out/InverseIndexStepTwo/");<br><br><br><br> FileInputFormat.setInputPaths(job_tow, new Path("./out/InverseIndexStepOne"));<br><br> //检查一下参数所指定的输出路径是否存在，如果已存在，先删除<br> Path output = new Path("./out/InverseIndexStepTwo");<br><br><br> FileSystem fs = FileSystem.get(conf);<br> if(fs.exists(output)){<br> fs.delete(output, true);<br> }<br><br> FileOutputFormat.setOutputPath(job_tow, output);<br><br><br> //先提交job_one执行<br> // boolean one_result = job_one.waitForCompletion(true);<br> // if(one_result){<br> System.exit(job_tow.waitForCompletion(true)?0:1);<br> // }<br><br> }</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">}</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p><font><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/6166185.html，如需转载请自行联系原作者</span></font><br></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
