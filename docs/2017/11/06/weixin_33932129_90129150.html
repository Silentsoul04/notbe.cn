<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>深度学习笔记之CNN（卷积神经网络）基础 « NotBeCN</title>
  <meta name="description" content="             　卷积神经网络（ConvolutionalNeural Networks，简称CNN）提出于20世纪60年代，由Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现。    &nbsp; &nbsp; &nbsp;&nbsp;CNN是目前深度学习最大的一个流派，其...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/06/weixin_33932129_90129150.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">深度学习笔记之CNN（卷积神经网络）基础</h1>
    <p class="post-meta">Nov 6, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　<strong><span style="font-size:18pt;">卷积神经网络（ConvolutionalNeural Networks，简称CNN）</span></strong>提出于20世纪60年代，由Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp;&nbsp;<span style="font-size:18pt;"><strong><span style="color:rgb(255,0,0);">C</span></strong></span><span style="font-size:18pt;"><strong><span style="color:rgb(255,0,0);">NN是目前<a class="replace_word" title="深度学习知识库" href="http://lib.csdn.net/base/deeplearning" rel="nofollow" style="color:#000000;"><span style="color:rgb(255,0,0);">深度学习</span></a>最大的一个流派</span></strong></span>，其应用优点在于避免了对图像的复杂前期预处理，可以直接处理原始图像。<span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">CNN核心在于“卷积”</span></strong></span>，传统<a class="replace_word" title="机器学习知识库" href="http://lib.csdn.net/base/machinelearning" rel="nofollow" style="color:#000000;">机器学习</a>中LBP、HOG等特征都可以看作是卷积的一种特殊形式，“卷积”以不同的参数来描述不同的抽象程度特征，更接近于原始图像的“特征抽象”。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111142845474?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 如上图所示，利用一个滑动窗口（卷积核）对原始图像区域进行“筛选”，将对应像素相乘，再累加（ I*K），就得到了一个像素的卷积结果。可以说卷积就像一个筛子，按照一定规则（借助卷积核“乘累加”）对原始图像再加工（实际是一个积分），公式可以描述为：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111143139693?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　&nbsp; 来看一个典型的CNN实例，对于分辨率为28*28的图片处理过程：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111143218772?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 其中Conv为卷积层（采用5*5的卷积核，Step为1），实现数据特征提取；Pool为池化层，也称为降采样（Sample），实现数据降维；FC为全连接层，通过一定规则的计算（也可能为卷积计算）得到结果。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 我们把卷积层、池化层等中间层统一称为隐层。接下来深入了解每个层的含义来加深对CNN的理解。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111143344059?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-size:18pt;color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;卷积层与权值共享</span></strong></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 按照生物学神经元数据处理机制，每个神经元需要与前一层连接，用于特征提取，如下图所示，假设神经元数量为100万，那么对于100万像素的图像将建立10<sup>12</sup>个连接，这里面的权值的数量是非常大的。</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111143438757?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 那么如何解决大量的权值计算（参数爆炸）问题呢？大开一下脑洞，我们做两个假设：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;1）减少每个神经元（对应隐层像素）的作用范围，比如设为10*10，仅和局部的100个像素建立局部连接；</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;参数简化： 100（连接权值）*1M（神经元个数）</strong></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;2）所有神经元采用同一组权值进行计算，对应CNN就是用同一个卷积核去卷积图像，这就是<strong>权值共享</strong>。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;参数简化： 100（权值）</strong></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;没错，我们只需要100个参数就建立了两层之间的连接，这靠谱吗？相信第一点很好理解，每个神经元是独立工作的，问题在于第二点，<strong>权值共享</strong>&nbsp;这个玩笑开的确实有点大。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;事实上从生物学上解释，神经元确实是分类的，有的对边缘敏感，有的对亮度敏感，还有的感应颜色，每一类神经元的参数是近似一致的。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;想通了吗？关键就在于“<strong>分类</strong>”，每一类神经元对应一组权值，怕种类不够？</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong>&nbsp; &nbsp; &nbsp; &nbsp; 那就用100个卷积核来描述100种神经元</strong>。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 对于上面的例子，不同的卷积核如右图所示，我们得到最终的参数个数：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong>&nbsp; &nbsp; &nbsp; &nbsp; 参数简化： 100（权值）*100（卷积核）</strong></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;我们用1万个权值的卷积计算，近似模拟了神经元的处理过程，这个数量级的参数对于计算机来讲并不算多，特别是对于卷积这种已经能够做GPU并行计算的方法，这就是卷积的魅力。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;<strong>池化</strong></span></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 池化（降采样）较为容易理解，获取一个区域内的典型特征，比如n*n像素范围的像素最大值或平均值，其意义在于能够对输入进行抽象描述，对特征进行降维。</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111143658674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;一般来讲，对于输入的池化区域都是分块处理的，与卷积的区别在于块之间一般不重叠，这样就保证了信息能够更快的抽象，抽象的价值在于提取里面的“显著特征”，而忽略“细节特征”。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">常用的两种池化方法：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;1）最大值池化，应用最多，能够有效减少卷积层带来的均值偏差，对纹理特征的适应较好；</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;2）平均值池化，对卷积结果进一步平滑，纹理和边缘特征容易丢失，但能有效避免噪声。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="color:rgb(0,0,255);font-size:18pt;">•&nbsp;&nbsp;&nbsp;<strong>激活层</strong></span></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;神经网络真正的代表作出场了，在第一节已做过专题讲解，<strong>激活层</strong>&nbsp;真正反映了神经元的工作机制，可以说是神经网络的精髓。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 在CNN中被广泛采用的ReLU函数有效解决了梯度扩散问题，你可以不必太在意“稀疏”，你可以忘掉“逐层预训练”，总结一句话那就是“激活函数的一小步，深度学习的一大步”。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;激活层一般添加在卷积层或者池化层之后，没有明确的位置定义，一般对于简单的网络来讲，激活层通常可以不添加。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;<strong>Dropout层</strong></span></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 这个问题的提出源于神经网络的另一个大缺陷-“过拟合”，对应于前面导致“梯度扩散”问题的欠拟合。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">常规解决“过拟合”问题的方法是模型平均，通过训练多个网络进行加权组合来进行规避，这样带来的问题是更大的计算量。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Dropout方法最早由Hinton大神提出，针对一次训练过程，网络中的神经元节点按照一定的概率进行权值更新，也就是说，神经元有可能仅保留权值参数，下一个训练过程再更新。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;按照一定的随机策略，使每一次训练的神经元并不相同，也就是说神经元节点轮流工作，这种随机过程又向人脑前进了一步。</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111143824567?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 每一次的Dropout相当于对网络做了一次简化（如上图所示），仅简化后的网络节点参与权值更新，整个训练过程中多次Dropout，每个节点都参与了训练和学习，当然偶尔也“翘个班”，过程很简单，为什么能得到好的效果呢？我们来分析一下：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;1）“天天大鱼大肉，顿顿鲍鱼龙虾”，偶尔吃个方便面吧，想想都嘚瑟，这个小调剂的效果出奇的好；</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;2）班上一共20个学生，老师每次选择回答问题的都是那几个优秀的学生，Dropout避免了训练结果对某些节点的强依赖（权重过大）；</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><em>&nbsp; &nbsp; &nbsp;&nbsp;作者倾向于从遗传学的角度来进行解释，不同的基因组合方式能提高对于环境的适应能力，“优胜劣汰”的进化概率更高。</em></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">3）Dropout也可以理解为一种平均模型，区别在于，通过在多Step之间进行平均代替多个模型的组合。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;不管从哪个角度进行理解，Dropout本身都是直观有效的，这里面虽然没有严谨的数学公式推导，但源于生物学的直观理解，以及大量的实验足以证明。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;就好像有人问我深度学习为什么会有这么好的效果，为什么就能有效收敛？什么理论避免陷入局部最大值？我们对于某些问题的认知仅仅在这个层面上而已，有时候不需要纠结，能解决问题就可以了，至于其背后隐晦的林林种种，So&nbsp;What?</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;全连接层</span></strong></span></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;全连接层直观理解为简化的数据计算，其意义在于求解，对于网络本身的贡献值是比较低的，大多数情况下，全连接层放在网络最后面，有时也会被省略，这里不做过多解释，大家看到的时候能知道干什么的就行了。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;">•&nbsp;&nbsp;&nbsp;<strong>回归层</strong></span></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;回归层理论上不属于CNN的一部分，当然也有人将其合并到网络，或者说也可以看作是全连接层的一部分，anyway，我们只是在这里将过程穿起来而已。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;回归是个比较熟悉的概念，前面讲过的逻辑回归、高斯回归等都属于回归，说白了回归就是对特征进行分类，可以分两类，也可以分多类。</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;回归的作用也只是分类而已，你可以用最简单的SVM分类器进行分类，也可以采用复杂的多类划分，比如Softmax。Softmax是逻辑回归模型在多分类问题上的推广，本质上是将一个P维向量映射成另一个K维向量，其公式描述为：</p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111144103775?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 得到对应的类别代表的概率，这个概率正是我们要的分类结果。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br></p> 
   <p><font><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/6972966.html，如需转载请自行联系原作者</span></font><br></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
