<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Spark RDD概念学习系列之rdd持久化、广播、累加器（十八） « NotBeCN</title>
  <meta name="description" content="             1、rdd持久化    2、广播    3、累加器    &nbsp;    &nbsp;    1、rdd持久化    　　通过spark-shell，可以快速的验证我们的想法和操作！    &nbsp;    启动hdfs集群    spark@SparkSingleNode:/us...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/19/weixin_34198583_90136226.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Spark RDD概念学习系列之rdd持久化、广播、累加器（十八）</h1>
    <p class="post-meta">Nov 19, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>1、rdd持久化</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong><strong>2、广播</strong></strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>3、累加器</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>1、rdd持久化</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　通过spark-shell，可以快速的验证我们的想法和操作！</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">启动hdfs集群</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">spark@SparkSingleNode:/usr/local/hadoop/hadoop-2.6.0$&nbsp;<span style="color:rgb(255,0,0);">sbin/start-dfs.sh</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927230002110-387713347.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">启动spark集群</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">spark@SparkSingleNode:/usr/local/spark/spark-1.5.2-bin-hadoop2.6$&nbsp;<span style="color:rgb(255,0,0);">sbin/start-all.sh</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927230219047-1572688943.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;启动spark-shell</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">spark@SparkSingleNode:/usr/local/spark/spark-1.5.2-bin-hadoop2.6/bin$&nbsp;<span style="color:rgb(255,0,0);">./spark-shell --master spark://SparkSingleNode:7077 --executor-memory 1g</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928085531125-1035000493.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>reduce</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928085915547-1346499609.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">sc</span><br> res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@3bcc8f13</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt; val numbers = sc.parallelize<br> &lt;console&gt;:21: error: missing arguments for method parallelize in class SparkContext;<br> follow this method with `_' if you want to treat it as a partially applied function<br> val numbers = sc.parallelize<br> ^</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val numbers = sc.parallelize(1 to 100)</span><br> numbers: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:21</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">numbers.reduce(_+_)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928090000297-968378437.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 11.790246 s<br> res1: Int = 5050</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">可见，reduce是个action。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928090356766-1789010092.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val result = numbers.map(2*_)</span><br> result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:23</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val data = result.collect</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>reduce源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928091312391-1286704603.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Reduces the elements of this RDD using the specified commutative and<br>
* associative binary operator.<br>
*/<br>
def reduce(f: (T, T) =&gt; T): T = withScope {<br>
val cleanF = sc.clean(f)<br>
val reducePartition: Iterator[T] =&gt; Option[T] = iter =&gt; {<br>
if (iter.hasNext) {<br>
Some(iter.reduceLeft(cleanF))<br>
} else {<br>
None<br>
}<br>
}<br>
var jobResult: Option[T] = None<br>
val mergeResult = (index: Int, taskResult: Option[T]) =&gt; {<br>
if (taskResult.isDefined) {<br>
jobResult = jobResult match {<br>
case Some(value) =&gt; Some(f(value, taskResult.get))<br>
case None =&gt; taskResult<br>
}<br>
}<br>
}<br>
sc.runJob(this, reducePartition, mergeResult)<br>
// Get the final result out of our Option, or throw an exception if the RDD was empty<br>
jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))<br>
}</pre> 
   <pre>可见,这也是一个action操作。</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>collect</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928090424156-1376709125.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">data: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>collect源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928090749360-865174085.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an array that contains all of the elements in this RDD.<br>
*/<br>
def collect(): Array[T] = withScope {<br>
val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)<br>
Array.concat(results: _*)<br>
}</pre> 
   <pre>可见,这也是一个action操作。<br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928091107875-600007989.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　从收集结果的角度来说，如果想要在命令行终端中，看到执行结果，就必须collect。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;　　从源码的角度来说，凡是action级别的操作，都会触发sc.rubJob。这点，spark里是一个应用程序允许有多个Job,而hadoop里一个应用程序只能一个Job。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>count</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928091650266-854000077.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">numbers</span><br> res2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:21</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">1 to 100</span><br> res3: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;numbers.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928091730750-1252806648.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 0.649005 s<br> res4: Long = 100</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>count源码&nbsp;</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928091801406-2140697256.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return the number of elements in the RDD.<br>
*/<br>
def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum<br><br></pre> 
   <pre>可见,这也是一个action操作。<br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>take</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928092018547-414547269.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val topN = numbers.take(5)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928092048625-788088044.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">topN: Array[Int] = Array(1, 2, 3, 4, 5)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>take源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928092142563-862362598.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Take the first num elements of the RDD. It works by first scanning one partition, and use the<br>
* results from that partition to estimate the number of additional partitions needed to satisfy<br>
* the limit.<br>
*<br>
* @note due to complications in the internal implementation, this method will raise<br>
* an exception if called on an RDD of `Nothing` or `Null`.<br>
*/<br>
def take(num: Int): Array[T] = withScope {<br>
if (num == 0) {<br>
new Array[T](0)<br>
} else {<br>
val buf = new ArrayBuffer[T]<br>
val totalParts = this.partitions.length<br>
var partsScanned = 0<br>
while (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) {<br>
// The number of partitions to try in this iteration. It is ok for this number to be<br>
// greater than totalParts because we actually cap it at totalParts in runJob.<br>
var numPartsToTry = 1<br>
if (partsScanned &gt; 0) {<br>
// If we didn't find any rows after the previous iteration, quadruple and retry.<br>
// Otherwise, interpolate the number of partitions we need to try, but overestimate<br>
// it by 50%. We also cap the estimation in the end.<br>
if (buf.size == 0) {<br>
numPartsToTry = partsScanned * 4<br>
} else {<br>
// the left side of max is &gt;=1 whenever partsScanned &gt;= 2<br>
numPartsToTry = Math.max((1.5 * num * partsScanned / buf.size).toInt - partsScanned, 1)<br>
numPartsToTry = Math.min(numPartsToTry, partsScanned * 4)<br>
}<br>
}<br><br>
val left = num - buf.size<br>
val p = partsScanned until math.min(partsScanned + numPartsToTry, totalParts)<br>
val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p)<br><br>
res.foreach(buf ++= _.take(num - buf.size))<br>
partsScanned += numPartsToTry<br>
}<br><br>
buf.toArray<br>
}<br>
}</pre> 
   <pre>可见,这也是一个action操作。</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-size:18pt;">countByKey</span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928094926610-1683896778.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val scores = Array(Tuple2(1,100),Tuple2(1,100),Tuple2(2,100),Tuple2(2,100),Tuple2(3,100))</span><br> scores: Array[(Int, Int)] = Array((1,100), (1,100), (2,100), (2,100), (3,100))</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt; val content = sc.parallelize&nbsp;<br> &lt;console&gt;:21: error: missing arguments for method parallelize in class SparkContext;<br> follow this method with `_' if you want to treat it as a partially applied function<br> val content = sc.parallelize&nbsp;<br> ^</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val content = sc.parallelize(scores)</span><br> content: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val data = content.countByKey()</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928095024500-593835710.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 10.556634 s<br> data: scala.collection.Map[Int,Long] = Map(2 -&gt; 2, 1 -&gt; 2, 3 -&gt; 1)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>countByKey源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160928095226281-1788275819.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Count the number of elements for each key, collecting the results to a local Map.<br>
*<br>
* Note that this method should only be used if the resulting map is expected to be small, as<br>
* the whole thing is loaded into the driver's memory.<br>
* To handle very large results, consider using rdd.mapValues(_ =&gt; 1L).reduceByKey(_ + _), which<br>
* returns an RDD[T, Long] instead of a map.<br>
*/<br>
def countByKey(): Map[K, Long] = self.withScope {<br>
self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap<br>
}<br><br>
可见,这也是一个action操作。<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>saveAsTextFile<br></strong></span></pre> 
   <pre>之前，在  <a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/zlslch/p/5913334.html" rel="nofollow" style="color:#000000;">rdd实战（rdd基本操作实战及transformation和action流程图）（源码）</a></pre> 
   <pre>scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val partitionsReadmeRdd = &nbsp;sc.textFile("hdfs://SparkSingleNode:9000/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).saveAsTextFile("~/partition1README.txt")</span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929145756860-108477025.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929145807172-597923280.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929145813110-675474391.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">这里呢。</p> 
   <pre>scala&gt;&nbsp;val partitionsReadmeRdd = &nbsp;sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).saveAsTextFile("/partition1README.txt")</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929145957875-1892302817.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929150105531-1551079346.png" alt="" style="border:0px;"></p> 
   <pre>scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val partitionsReadmeRdd = &nbsp;sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).saveAsTextFile("/partition1README.txt")</span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929150155672-2132228029.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929150211297-1249204881.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><span style="font-size:18pt;"><strong>saveAsTextFile源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929150242219-1489408056.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Save this RDD as a text file, using string representations of elements.<br>
*/<br>
def saveAsTextFile(path: String): Unit = withScope {<br>
// https://issues.apache.org/jira/browse/SPARK-2075<br>
//<br>
// NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit<br>
// Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`<br>
// in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an<br>
// Ordering for `NullWritable`. That's why the compiler will generate different anonymous<br>
// classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.<br>
//<br>
// Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate<br>
// same bytecodes for `saveAsTextFile`.<br>
val nullWritableClassTag = implicitly[ClassTag[NullWritable]]<br>
val textClassTag = implicitly[ClassTag[Text]]<br>
val r = this.mapPartitions { iter =&gt;<br>
val text = new Text()<br>
iter.map { x =&gt;<br>
text.set(x.toString)<br>
(NullWritable.get(), text)<br>
}<br>
}<br>
RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)<br>
.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)<br>
}<br><br>
/**<br>
* Save this RDD as a compressed text file, using string representations of elements.<br>
*/<br>
def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit = withScope {<br>
// https://issues.apache.org/jira/browse/SPARK-2075<br>
val nullWritableClassTag = implicitly[ClassTag[NullWritable]]<br>
val textClassTag = implicitly[ClassTag[Text]]<br>
val r = this.mapPartitions { iter =&gt;<br>
val text = new Text()<br>
iter.map { x =&gt;<br>
text.set(x.toString)<br>
(NullWritable.get(), text)<br>
}<br>
}<br>
RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)<br>
.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec)<br>
}<br><br><br></pre> 
   <pre>saveAsTextFile不仅，可保存在集群里，也可以保存到本地，这就要看hadoop的运行模式。<br>
由此可见，它也是个action操作。</pre> 
   <pre><br><span style="font-size:18pt;"><strong>以上是rdd持久化的第一个方面，就是action级别的操作。<br></strong></span></pre> 
   <pre><span style="font-size:18pt;"><strong>rdd持久化的第二个方面，就是通过persist。<br><span style="font-size:12px;">为什么在spark里，随处可见</span></strong></span><strong>persist的身影呢？<br>
原因一：spark在默认情况下，数据是放在内存中，适合高速迭代。比如在一个stage里，有1000个步骤，它其实只在第1个步骤输入数据，在第1000个步骤输出数据，在中间不产生临时数据。但是，分布式系统，分享非常高，所以，容出错，设计到容错。<br><br>
由于，rdd是有血统继承关系的，即lineager。如果后面的rdd数据分片出错了或rdd本身出错了，则，可根据其前面依赖的lineager，算出来。<br>
但是，假设1000个步骤，如果之前，没有父rdd进行</strong><strong>persist或cache的话，则要重头开始了。亲！<br><br><br>
什么时候，该</strong><strong>persist？</strong></pre> 
   <pre><strong>1、在某个步骤非常费时的情况下，不好使  　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　（<span style="color:rgb(255,0,0);">手动</span>）<br>
2、计算链条特别长的情况下   　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</strong><strong>（<span style="color:rgb(255,0,0);">手动</span>）</strong></pre> 
   <pre><strong>3、checkpoint所在的rdd也一定要持久化数据      （注意：在</strong><strong>checkpoint之前，进行</strong><strong>persist）    　　　　　　</strong><strong>（<span style="color:rgb(255,0,0);">手动</span>）</strong></pre> 
   <pre><strong>checkpoint是rdd的算子，</strong></pre> 
   <pre>　　先写，<span>某个具体rdd</span>.<strong>checkpoint  或   </strong><span>某个具体rdd</span>.<strong>cache ，再写，  </strong>某个具体rdd.<strong>persist</strong></pre> 
   <pre><strong>4、shuffle之后   （因为shuffle之后，要网络传输，风险大）  　　　　　　　　　　　　　　　　　　　　　　　　</strong><strong>（<span style="color:rgb(255,0,0);">手动</span>）</strong></pre> 
   <pre><strong>5、shuffle之前    （框架，默认给我们做的，把数据持久化到本地磁盘）<br><br><br><br></strong></pre> 
   <pre><span style="font-size:18pt;"><strong>checkpoint源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929152323438-1836110071.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint<br>
* directory set with `SparkContext#setCheckpointDir` and all references to its parent<br>
* RDDs will be removed. This function must be called before any job has been<br>
* executed on this RDD. It is strongly recommended that this RDD is persisted in<br>
* memory, otherwise saving it on a file will require recomputation.<br>
*/<br>
def checkpoint(): Unit = RDDCheckpointData.synchronized {<br>
// NOTE: we use a global lock here due to complexities downstream with ensuring<br>
// children RDD partitions point to the correct parent partitions. In the future<br>
// we should revisit this consideration.<br>
if (context.checkpointDir.isEmpty) {<br>
throw new SparkException("Checkpoint directory has not been set in the SparkContext")<br>
} else if (checkpointData.isEmpty) {<br>
checkpointData = Some(new ReliableRDDCheckpointData(this))<br>
}<br>
}<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>persist源码<br></strong></span></pre> 
   <pre>/**<br>
* Mark this RDD for persisting using the specified level.<br>
*<br>
* @param newLevel the target storage level<br>
* @param allowOverride whether to override any existing level with the new one<br>
*/<br>
private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = {<br>
// TODO: Handle changes of StorageLevel<br>
if (storageLevel != StorageLevel.NONE &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) {<br>
throw new UnsupportedOperationException(<br>
"Cannot change storage level of an RDD after it was already assigned a level")<br>
}<br>
// If this is the first time this RDD is marked for persisting, register it<br>
// with the SparkContext for cleanups and accounting. Do this only once.<br>
if (storageLevel == StorageLevel.NONE) {<br>
sc.cleaner.foreach(_.registerRDDForCleanup(this))<br>
sc.persistRDD(this)<br>
}<br>
storageLevel = newLevel<br>
this<br>
}<br><br>
/**<br>
* Set this RDD's storage level to persist its values across operations after the first time<br>
* it is computed. This can only be used to assign a new storage level if the RDD does not<br>
* have a storage level set yet. Local checkpointing is an exception.<br>
*/<br>
def persist(newLevel: StorageLevel): this.type = {<br>
if (isLocallyCheckpointed) {<br>
// This means the user previously called localCheckpoint(), which should have already<br>
// marked this RDD for persisting. Here we should override the old storage level with<br>
// one that is explicitly requested by the user (after adapting it to use disk).<br>
persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true)<br>
} else {<br>
persist(newLevel, allowOverride = false)<br>
}<br>
}<br><br>
/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */<br>
def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)<br><br>
/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */<br>
def cache(): this.type = persist()<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong></strong></span></pre> 
   <pre>StorageLevel里有很多类型</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">这里，牵扯到序列化。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">问，为什么要序列化？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答：节省空间，减少体积。内存不够时，把MEMORY中的数据，进行序列化。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　当然，也有不好一面，序列化时，会反序列化，反序列化耗cpu。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><span style="font-size:18pt;"><strong>MEMORY_AND_DISK <br> 假设，我们制定数据存储方式是，</strong></span>MEMORY_AND_DISK。则，是不是同时，存储到内存和磁盘呢？<br>
答：不是啊，亲。spark一定是优先考虑内存的啊，只要内存足够，不会考虑磁盘。若内存不够了，则才放部分数据到磁盘。<br><br>
极大地减少数据丢失概率发生。<br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>MEMORY_ONLY</strong></span></pre> 
   <pre><strong><span style="font-size:18pt;">假设，我们制定数据存储方式是</span>，</strong>MEMORY_<strong>ONLY</strong>。则，<span style="font-size:18pt;"><strong>只放到内存。当内存不够了，会出现OOM。或数据丢失。<br><br></strong></span></pre> 
   <pre><span style="font-size:18pt;"><strong>OFF_HEAP</strong></span></pre> 
   <pre><span style="font-size:18pt;"><span style="font-size:12px;">这牵扯到Tachyon，基于内存的分布式系统</span><strong><br><br></strong></span></pre> 
   <pre><em><span style="font-size:18pt;"><strong></strong></span></em></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">为什么有2分副本？好处是？</p> 
   <pre><span style="font-size:12px;">假设，一个计算特别耗时，而且，又是基于内存，如果其中一份副本崩溃掉，则可迅速切换到另一份副本去计算。这就是“空间换时间”！非常重要</span><em><span style="font-size:18pt;"><strong><br> 这不是并行计算，这是计算后的结果，放2份副本。<br><br><br><br></strong></span></em></pre> 
   <pre></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929154808125-456785861.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val partitionsReadmeRdd = sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929154843953-1872478714.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 6.270138 s</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929154941516-864753976.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val partitionsReadmeRdd = &nbsp;sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929155010703-43724733.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 4.147545 s</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929155137375-98361811.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val partitionsReadmeRdd = sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929155156391-977531443.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 4.914212 s</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929155500969-101032762.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val cacheRdd = sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;<span style="color:rgb(255,0,0);">&nbsp;cacheRdd.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929155947766-1800959277.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;took 3.371621</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160056969-1083380519.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val cacheRdd = sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;&nbsp;<span style="color:rgb(255,0,0);">cacheRdd.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160136594-916820328.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 0.943499 s</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">我的天啊！</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160407063-561152105.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;<span style="color:rgb(255,0,0);">&nbsp;&nbsp;sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160456000-711482151.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 5.603903</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160536094-1159430893.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;&nbsp;&nbsp;<span style="color:rgb(255,0,0);">sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160611828-1624749694.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 4.146627</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160633453-601429129.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;&nbsp;<span style="color:rgb(255,0,0);">sc.textFile("/README.md").flatMap(_.split(" ")).map(word =&gt;(word,1)).reduceByKey(_+_,1).cache.count</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929160701094-1679711629.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 3.071122</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="color:rgb(255,0,0);font-size:18pt;">cache之后，一定不能立即有其他算子！</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">实际工程中， cache之后，如果有其他算子，则会，重新触发这个工作过程。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;color:rgb(255,0,0);">注意:cache，不是action</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929161537250-6241772.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;cache缓存，怎么让它失效？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答：unpersist</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">persist是lazy级别的，unpersist是eager级别的。cache是persist的一个特殊情况。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">cache和persist的区别？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答：persist可以放到磁盘、放到内存、同时放到内存和磁盘。以及多份副本</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　cache只能放到内存，以及只能一份副本。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929164101875-552143392.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929164132313-55459638.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">persisit在内存不够时，保存在磁盘的哪个目录？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答：local的process。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>好的，以上是，rdd持久化的两个方面。</strong></span></p> 
   <pre><strong>rdd持久化的第一个方面，就是常用的action级别的操作。</strong></pre> 
   <pre><strong>rdd持久化的第二个方面，就是持久化的不同方式，以及它内部的运行情况<br></strong></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　小知识：cache之后，一定不能立即有其他算子！实际工程中， cache之后，如果有其他算子，则会，重新触发这个工作过程。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;一般都不会跨机器抓内存，宁愿排队。宁愿数据不动代码动。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>2、广播</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;　　为什么要有，rdd广播？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　答：大变量、join、冗余、减少数据移动、通信、状态、集群消息、共享、网络传输慢要提前、数据量大耗网络、减少通信、要同步。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;　　为什么大变量，需要广播呢？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;　　答：原因是，每个task运行，读取全集数据时，task本身执行时，每次都要拷贝一份数据副本，如果变量比较大，如一百万，则要拷贝一百万。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    <strong>（2）广播（线程中共享）不必每个task都拷贝一份副本，因为它是全局唯一的，极大的减少oom，减少通信，冗余、共享变量等。广播是将数据广播到Executor的内存中，其内部所以的任务都会只享有全局唯一的变量，减少网络传输。</strong>
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;text在读取数据时候，拷贝一份的数据副本（变量），因为函数式编程（变量不变），不拷贝状态容易被改变，数据量小（1、引用较小2、数据本身小），变量大容易产生oom（task拷贝数据 在内存中运行），网络传输慢，要提前，冗余、共享，减少通信。
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    <strong>广播变量：</strong>
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p>广播变量允许程序员将一个只读的变量缓存在每台机器上，而不用在任务之间传递变量。广播变量可被用于有效地给每个节点一个大输入数据集的副本。Spark还尝试使用高效地广播算法来分发变量，进而减少通信的开销。</p> 
    <p>Spark的动作通过一系列的步骤执行，这些步骤由分布式的洗牌操作分开。Spark自动地广播每个步骤每个任务需要的通用数据。这些广播数据被序列化地缓存，在运行任务之前被反序列化出来。这意味着当我们需要在多个阶段的任务之间使用相同的数据，或者以反序列化形式缓存数据是十分重要的时候，显式地创建广播变量才有用。</p> 
    <p>（本段摘自：<a href="http://blog.csdn.net/happyanger6/article/details/46576831" rel="nofollow" style="color:#000000;">http://blog.csdn.net/happyanger6/article/details/46576831</a>）</p> 
    <p>&nbsp;</p> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929170645172-171386706.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　　　　　　　　　　　　　　　　　广播工作机制图&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929172236703-420237803.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;　　　　　　　　　　　　　　　　　　　　　　　　　　　　广播工作机制图&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">参考：　http://blog.csdn.net/kxr0502/article/details/50574561</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">问：读广播，会消耗网络传输吗？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答：不消耗，广播是放在内存中。读取它，不消耗。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">问：广播变量是不是就是向每一个executor，广播一份数据，而不是向每一个task，广播一份数据？这样对吗?</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答：对</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　<strong>广播</strong>是由Driver发给当前Application分配的所有Executor内存级别的全局只读变量，Executor中的线程池中的线程共享该全局变量，极大的减少了网络传输（否则的话每个Task都要传输一次该变量）并极大的节省了内存，当然也隐形的提高的CPU的有效工作。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;">&nbsp;<strong>实战创建广播：</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929173252625-492775278.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val number = 10</span><br> number: Int = 10</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val broadcastNumber = sc.broadcast(number)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">16/09/29 17:26:47 INFO storage.MemoryStore: ensureFreeSpace(40) called with curMem=1782734, maxMem=560497950<br> 16/09/29 17:26:47 INFO storage.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 40.0 B, free 532.8 MB)<br> 16/09/29 17:26:48 INFO storage.MemoryStore: ensureFreeSpace(97) called with curMem=1782774, maxMem=560497950<br> 16/09/29 17:26:48 INFO storage.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 97.0 B, free 532.8 MB)<br> 16/09/29 17:26:48 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 192.168.80.128:40914 (size: 97.0 B, free: 534.4 MB)<br> 16/09/29 17:26:48 INFO spark.SparkContext: Created broadcast 38 from broadcast at &lt;console&gt;:23<br><strong>broadcastNumber: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(38)</strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929173337031-1510175822.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt; val data = sc.parallelize<br> &lt;console&gt;:21: error: missing arguments for method parallelize in class SparkContext;<br> follow this method with `_' if you want to treat it as a partially applied function<br> val data = sc.parallelize<br> ^</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val data = sc.parallelize(1 to 100)</span><br> data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[61] at parallelize at &lt;console&gt;:21</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val bn = data.map(_* broadcastNumber.value)</span>&nbsp;<br> bn: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[62] at map at &lt;console&gt;:27</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　我们知道，是test是要广播变量，但，我们编程，对rdd。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">//通过在一个变量v上调用SparkContext.broadcast(v)可以创建广播变量。广播变量是围绕着v的封装，可以通过value方法访问这个变量。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">问：广播变量里有很多变量吗？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">答:当然可以有很多，用java bin或scala封装，就可以了。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　如，在这里。广播变量是，broadcastNumber， 里，有变量value等。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;val broadcastNumber = sc.broadcast(number)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;scala&gt;&nbsp;val bn = data.map(_* broadcastNumber.value)&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929174214297-683687198.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;&nbsp;<span style="color:rgb(255,0,0);">bn.collect</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929174237469-866395069.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">res12: Array[Int] = Array(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<strong>由此，可见，通过机制、流程图和实战，深度剖析对广播全面详解！</strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>broadcast源码分析</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>&nbsp;参考：&nbsp;http://www.cnblogs.com/seaspring/p/5682053.html</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>BroadcastManager源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929174844469-737699460.png" alt="" style="border:0px;"></strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929174917235-1640401533.png" alt="" style="border:0px;"></p> 
   <pre>/*<br>
* Licensed to the Apache Software Foundation (ASF) under one or more<br>
* contributor license agreements.  See the NOTICE file distributed with<br>
* this work for additional information regarding copyright ownership.<br>
* The ASF licenses this file to You under the Apache License, Version 2.0<br>
* (the "License"); you may not use this file except in compliance with<br>
* the License.  You may obtain a copy of the License at<br>
*<br>
*    http://www.apache.org/licenses/LICENSE-2.0<br>
*<br>
* Unless required by applicable law or agreed to in writing, software<br>
* distributed under the License is distributed on an "AS IS" BASIS,<br>
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>
* See the License for the specific language governing permissions and<br>
* limitations under the License.<br>
*/<br><br>
package org.apache.spark.broadcast<br><br>
import java.util.concurrent.atomic.AtomicLong<br><br>
import scala.reflect.ClassTag<br><br>
import org.apache.spark._<br>
import org.apache.spark.util.Utils<br><br>
private[spark] class BroadcastManager(<br>
val isDriver: Boolean,<br>
conf: SparkConf,<br>
securityManager: SecurityManager)<br>
extends Logging {<br><br>
private var initialized = false<br>
private var broadcastFactory: BroadcastFactory = null<br><br>
initialize()<br><br>
// Called by SparkContext or Executor before using Broadcast<br>
private def initialize() {<br>
synchronized {<br>
if (!initialized) {<br>
val broadcastFactoryClass =<br>
conf.get("spark.broadcast.factory", "org.apache.spark.broadcast.TorrentBroadcastFactory")<br><br>
broadcastFactory =<br>
Utils.classForName(broadcastFactoryClass).newInstance.asInstanceOf[BroadcastFactory]<br><br>
// Initialize appropriate BroadcastFactory and BroadcastObject<br>
broadcastFactory.initialize(isDriver, conf, securityManager)<br><br>
initialized = true<br>
}<br>
}<br>
}<br><br>
def stop() {<br>
broadcastFactory.stop()<br>
}<br><br>
private val nextBroadcastId = new AtomicLong(0)<br><br>
def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean): Broadcast[T] = {<br>
broadcastFactory.newBroadcast[T](value_, isLocal, nextBroadcastId.getAndIncrement())<br>
}<br><br>
def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean) {<br>
broadcastFactory.unbroadcast(id, removeFromDriver, blocking)<br>
}<br>
}<br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929175211735-1464349350.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929175242063-787924908.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><span style="font-size:18pt;"><strong>Broadcast源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929175325985-55867703.png" alt="" style="border:0px;"></p> 
   <pre>/*<br>
* Licensed to the Apache Software Foundation (ASF) under one or more<br>
* contributor license agreements.  See the NOTICE file distributed with<br>
* this work for additional information regarding copyright ownership.<br>
* The ASF licenses this file to You under the Apache License, Version 2.0<br>
* (the "License"); you may not use this file except in compliance with<br>
* the License.  You may obtain a copy of the License at<br>
*<br>
*    http://www.apache.org/licenses/LICENSE-2.0<br>
*<br>
* Unless required by applicable law or agreed to in writing, software<br>
* distributed under the License is distributed on an "AS IS" BASIS,<br>
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>
* See the License for the specific language governing permissions and<br>
* limitations under the License.<br>
*/<br><br>
package org.apache.spark.broadcast<br><br>
import java.io.Serializable<br><br>
import org.apache.spark.SparkException<br>
import org.apache.spark.Logging<br>
import org.apache.spark.util.Utils<br><br>
import scala.reflect.ClassTag<br><br>
/**<br>
* A broadcast variable. Broadcast variables allow the programmer to keep a read-only variable<br>
* cached on each machine rather than shipping a copy of it with tasks. They can be used, for<br>
* example, to give every node a copy of a large input dataset in an efficient manner. Spark also<br>
* attempts to distribute broadcast variables using efficient broadcast algorithms to reduce<br>
* communication cost.<br>
*<br>
* Broadcast variables are created from a variable `v` by calling<br>
* [[org.apache.spark.SparkContext#broadcast]].<br>
* The broadcast variable is a wrapper around `v`, and its value can be accessed by calling the<br>
* `value` method. The interpreter session below shows this:<br>
*<br>
* {{{<br>
* scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))<br>
* broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)<br>
*<br>
* scala&gt; broadcastVar.value<br>
* res0: Array[Int] = Array(1, 2, 3)<br>
* }}}<br>
*<br>
* After the broadcast variable is created, it should be used instead of the value `v` in any<br>
* functions run on the cluster so that `v` is not shipped to the nodes more than once.<br>
* In addition, the object `v` should not be modified after it is broadcast in order to ensure<br>
* that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped<br>
* to a new node later).<br>
*<br>
* @param id A unique identifier for the broadcast variable.<br>
* @tparam T Type of the data contained in the broadcast variable.<br>
*/<br>
abstract class Broadcast[T: ClassTag](val id: Long) extends Serializable with Logging {<br><br>
/**<br>
* Flag signifying whether the broadcast variable is valid<br>
* (that is, not already destroyed) or not.<br>
*/<br>
@volatile private var _isValid = true<br><br>
private var _destroySite = ""<br><br>
/** Get the broadcasted value. */<br>
def value: T = {<br>
assertValid()<br>
getValue()<br>
}<br><br>
/**<br>
* Asynchronously delete cached copies of this broadcast on the executors.<br>
* If the broadcast is used after this is called, it will need to be re-sent to each executor.<br>
*/<br>
def unpersist() {<br>
unpersist(blocking = false)<br>
}<br><br>
/**<br>
* Delete cached copies of this broadcast on the executors. If the broadcast is used after<br>
* this is called, it will need to be re-sent to each executor.<br>
* @param blocking Whether to block until unpersisting has completed<br>
*/<br>
def unpersist(blocking: Boolean) {<br>
assertValid()<br>
doUnpersist(blocking)<br>
}<br><br><br>
/**<br>
* Destroy all data and metadata related to this broadcast variable. Use this with caution;<br>
* once a broadcast variable has been destroyed, it cannot be used again.<br>
* This method blocks until destroy has completed<br>
*/<br>
def destroy() {<br>
destroy(blocking = true)<br>
}<br><br>
/**<br>
* Destroy all data and metadata related to this broadcast variable. Use this with caution;<br>
* once a broadcast variable has been destroyed, it cannot be used again.<br>
* @param blocking Whether to block until destroy has completed<br>
*/<br>
private[spark] def destroy(blocking: Boolean) {<br>
assertValid()<br>
_isValid = false<br>
_destroySite = Utils.getCallSite().shortForm<br>
logInfo("Destroying %s (from %s)".format(toString, _destroySite))<br>
doDestroy(blocking)<br>
}<br><br>
/**<br>
* Whether this Broadcast is actually usable. This should be false once persisted state is<br>
* removed from the driver.<br>
*/<br>
private[spark] def isValid: Boolean = {<br>
_isValid<br>
}<br><br>
/**<br>
* Actually get the broadcasted value. Concrete implementations of Broadcast class must<br>
* define their own way to get the value.<br>
*/<br>
protected def getValue(): T<br><br>
/**<br>
* Actually unpersist the broadcasted value on the executors. Concrete implementations of<br>
* Broadcast class must define their own logic to unpersist their own data.<br>
*/<br>
protected def doUnpersist(blocking: Boolean)<br><br>
/**<br>
* Actually destroy all data and metadata related to this broadcast variable.<br>
* Implementation of Broadcast class must define their own logic to destroy their own<br>
* state.<br>
*/<br>
protected def doDestroy(blocking: Boolean)<br><br>
/** Check if this broadcast is valid. If not valid, exception is thrown. */<br>
protected def assertValid() {<br>
if (!_isValid) {<br>
throw new SparkException(<br>
"Attempted to use %s after it was destroyed (%s) ".format(toString, _destroySite))<br>
}<br>
}<br><br>
override def toString: String = "Broadcast(" + id + ")"<br>
}<br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;其他的，不一一赘述了。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929175549672-378501007.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>&nbsp;3、累加器</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;　　为什么需要，累加器？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　 答：第一种情况，是，test把数据副本运行起来。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　 &nbsp; &nbsp; &nbsp; &nbsp;第二种情况，有全局变量和局部变量，有了广播，为什么还需要累加器？</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <strong>（3）累加器</strong>（获取全局唯一的状态对象，SparkContext创建，被Driver控制，在Text实际运行的时候，每次都可以保证修改之后获取全局唯一的对象，Driver中可读，Executor可读）
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p>&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;累加器是仅仅被相关操作累加的变量，因此可以在并行中被有效地支持。它可以被用来实现计数器和总和。Spark原生地只支持数字类型的累加器，编程者可以添加新类型的支持。如果创建累加器时指定了名字，可以在Spark的UI界面看到。这有利于理解每个执行阶段的进程。（对于python还不支持）</p> 
    <p>&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;累加器通过对一个初始化了的变量v调用SparkContext.accumulator(v)来创建。在集群上运行的任务可以通过add或者"+="方法在累加器上进行累加操作。但是，它们不能读取它的值。只有驱动程序能够读取它的值，通过累加器的value方法。</p> 
    <p>&nbsp;　　累加器的特征：全局的，Accumulator：对于Executor只能修改但不可读，只对Driver可读（因为通过Driver控制整个集群的状态），不同的executor 修改，不会彼此覆盖（枷锁机制）</p> 
    <p>&nbsp;　　</p> 
    <p>&nbsp;<strong>累加器实战：</strong></p> 
    <p>&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929190910985-2040072427.png" alt="" style="border:0px;"></p> 
    <p>scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val sum = sc.accumulator(0)</span><br> sum: org.apache.spark.Accumulator[Int] = 0</p> 
    <p>scala&gt;<span style="color:rgb(255,0,0);">&nbsp;val data = sc.parallelize(1 to 100)</span><br> data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[63] at parallelize at &lt;console&gt;:21</p> 
    <p>scala&gt;&nbsp;<span style="color:rgb(255,0,0);">val result = data.foreach(item =&gt;sum += item)</span></p> 
    <p>&nbsp;</p> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929191007000-994370335.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">took 6.548568 s<br> result: Unit = ()</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">scala&gt; println(sum)&nbsp;<br> 5050</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    累加器&nbsp;在记录集群全局唯一的状态的时候极其重要，保持唯一的全局状态的变量，所以其重要性不言而喻。
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    Driver中取值，Executor中计算，
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div>
     1、累计器全局（全集群）唯一，只增不减（Executor中的task去修改，即累加）；
    </div> 
    <div>
     2、累加器是Executor共享；
    </div> 
   </blockquote> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    我的理解应该是对的，集群全局变量，谁操作，从driver上拿去操作，然后下个Executor在用的时候，拿上个Executor执行的结果，也就是从Driver那里拿。
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>&nbsp;accumulator源码</strong></span><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929191327891-1512796977.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160929192120188-776362525.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre>/*<br>
* Licensed to the Apache Software Foundation (ASF) under one or more<br>
* contributor license agreements.  See the NOTICE file distributed with<br>
* this work for additional information regarding copyright ownership.<br>
* The ASF licenses this file to You under the Apache License, Version 2.0<br>
* (the "License"); you may not use this file except in compliance with<br>
* the License.  You may obtain a copy of the License at<br>
*<br>
*    http://www.apache.org/licenses/LICENSE-2.0<br>
*<br>
* Unless required by applicable law or agreed to in writing, software<br>
* distributed under the License is distributed on an "AS IS" BASIS,<br>
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br>
* See the License for the specific language governing permissions and<br>
* limitations under the License.<br>
*/<br><br>
package org.apache.spark<br><br>
import java.io.{ObjectInputStream, Serializable}<br><br>
import scala.collection.generic.Growable<br>
import scala.collection.Map<br>
import scala.collection.mutable<br>
import scala.ref.WeakReference<br>
import scala.reflect.ClassTag<br><br>
import org.apache.spark.serializer.JavaSerializer<br>
import org.apache.spark.util.Utils<br><br>
/**<br>
* A data type that can be accumulated, ie has an commutative and associative "add" operation,<br>
* but where the result type, `R`, may be different from the element type being added, `T`.<br>
*<br>
* You must define how to add data, and how to merge two of these together.  For some data types,<br>
* such as a counter, these might be the same operation. In that case, you can use the simpler<br>
* [[org.apache.spark.Accumulator]]. They won't always be the same, though -- e.g., imagine you are<br>
* accumulating a set. You will add items to the set, and you will union two sets together.<br>
*<br>
* @param initialValue initial value of accumulator<br>
* @param param helper object defining how to add elements of type `R` and `T`<br>
* @param name human-readable name for use in Spark's web UI<br>
* @param internal if this [[Accumulable]] is internal. Internal [[Accumulable]]s will be reported<br>
*                 to the driver via heartbeats. For internal [[Accumulable]]s, `R` must be<br>
*                 thread safe so that they can be reported correctly.<br>
* @tparam R the full accumulated data (result type)<br>
* @tparam T partial data that can be added in<br>
*/<br>
class Accumulable[R, T] private[spark] (<br>
@transient initialValue: R,<br>
param: AccumulableParam[R, T],<br>
val name: Option[String],<br>
internal: Boolean)<br>
extends Serializable {<br><br>
private[spark] def this(<br>
@transient initialValue: R, param: AccumulableParam[R, T], internal: Boolean) = {<br>
this(initialValue, param, None, internal)<br>
}<br><br>
def this(@transient initialValue: R, param: AccumulableParam[R, T], name: Option[String]) =<br>
this(initialValue, param, name, false)<br><br>
def this(@transient initialValue: R, param: AccumulableParam[R, T]) =<br>
this(initialValue, param, None)<br><br>
val id: Long = Accumulators.newId<br><br>
@volatile @transient private var value_ : R = initialValue // Current value on master<br>
val zero = param.zero(initialValue)  // Zero value to be passed to workers<br>
private var deserialized = false<br><br>
Accumulators.register(this)<br><br>
/**<br>
* If this [[Accumulable]] is internal. Internal [[Accumulable]]s will be reported to the driver<br>
* via heartbeats. For internal [[Accumulable]]s, `R` must be thread safe so that they can be<br>
* reported correctly.<br>
*/<br>
private[spark] def isInternal: Boolean = internal<br><br>
/**<br>
* Add more data to this accumulator / accumulable<br>
* @param term the data to add<br>
*/<br>
def += (term: T) { value_ = param.addAccumulator(value_, term) }<br><br>
/**<br>
* Add more data to this accumulator / accumulable<br>
* @param term the data to add<br>
*/<br>
def add(term: T) { value_ = param.addAccumulator(value_, term) }<br><br>
/**<br>
* Merge two accumulable objects together<br>
*<br>
* Normally, a user will not want to use this version, but will instead call `+=`.<br>
* @param term the other `R` that will get merged with this<br>
*/<br>
def ++= (term: R) { value_ = param.addInPlace(value_, term)}<br><br>
/**<br>
* Merge two accumulable objects together<br>
*<br>
* Normally, a user will not want to use this version, but will instead call `add`.<br>
* @param term the other `R` that will get merged with this<br>
*/<br>
def merge(term: R) { value_ = param.addInPlace(value_, term)}<br><br>
/**<br>
* Access the accumulator's current value; only allowed on master.<br>
*/<br>
def value: R = {<br>
if (!deserialized) {<br>
value_<br>
} else {<br>
throw new UnsupportedOperationException("Can't read accumulator value in task")<br>
}<br>
}<br><br>
/**<br>
* Get the current value of this accumulator from within a task.<br>
*<br>
* This is NOT the global value of the accumulator.  To get the global value after a<br>
* completed operation on the dataset, call `value`.<br>
*<br>
* The typical use of this method is to directly mutate the local value, eg., to add<br>
* an element to a Set.<br>
*/<br>
def localValue: R = value_<br><br>
/**<br>
* Set the accumulator's value; only allowed on master.<br>
*/<br>
def value_= (newValue: R) {<br>
if (!deserialized) {<br>
value_ = newValue<br>
} else {<br>
throw new UnsupportedOperationException("Can't assign accumulator value in task")<br>
}<br>
}<br><br>
/**<br>
* Set the accumulator's value; only allowed on master<br>
*/<br>
def setValue(newValue: R) {<br>
this.value = newValue<br>
}<br><br>
// Called by Java when deserializing an object<br>
private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {<br>
in.defaultReadObject()<br>
value_ = zero<br>
deserialized = true<br>
// Automatically register the accumulator when it is deserialized with the task closure.<br>
//<br>
// Note internal accumulators sent with task are deserialized before the TaskContext is created<br>
// and are registered in the TaskContext constructor. Other internal accumulators, such SQL<br>
// metrics, still need to register here.<br>
val taskContext = TaskContext.get()<br>
if (taskContext != null) {<br>
taskContext.registerAccumulator(this)<br>
}<br>
}<br><br>
override def toString: String = if (value_ == null) "null" else value_.toString<br>
}<br><br>
/**<br>
* Helper object defining how to accumulate values of a particular type. An implicit<br>
* AccumulableParam needs to be available when you create [[Accumulable]]s of a specific type.<br>
*<br>
* @tparam R the full accumulated data (result type)<br>
* @tparam T partial data that can be added in<br>
*/<br>
trait AccumulableParam[R, T] extends Serializable {<br>
/**<br>
* Add additional data to the accumulator value. Is allowed to modify and return `r`<br>
* for efficiency (to avoid allocating objects).<br>
*<br>
* @param r the current value of the accumulator<br>
* @param t the data to be added to the accumulator<br>
* @return the new value of the accumulator<br>
*/<br>
def addAccumulator(r: R, t: T): R<br><br>
/**<br>
* Merge two accumulated values together. Is allowed to modify and return the first value<br>
* for efficiency (to avoid allocating objects).<br>
*<br>
* @param r1 one set of accumulated data<br>
* @param r2 another set of accumulated data<br>
* @return both data sets merged together<br>
*/<br>
def addInPlace(r1: R, r2: R): R<br><br>
/**<br>
* Return the "zero" (identity) value for an accumulator type, given its initial value. For<br>
* example, if R was a vector of N dimensions, this would return a vector of N zeroes.<br>
*/<br>
def zero(initialValue: R): R<br>
}<br><br>
private[spark] class<br>
GrowableAccumulableParam[R &lt;% Growable[T] with TraversableOnce[T] with Serializable: ClassTag, T]<br>
extends AccumulableParam[R, T] {<br><br>
def addAccumulator(growable: R, elem: T): R = {<br>
growable += elem<br>
growable<br>
}<br><br>
def addInPlace(t1: R, t2: R): R = {<br>
t1 ++= t2<br>
t1<br>
}<br><br>
def zero(initialValue: R): R = {<br>
// We need to clone initialValue, but it's hard to specify that R should also be Cloneable.<br>
// Instead we'll serialize it to a buffer and load it back.<br>
val ser = new JavaSerializer(new SparkConf(false)).newInstance()<br>
val copy = ser.deserialize[R](ser.serialize(initialValue))<br>
copy.clear()   // In case it contained stuff<br>
copy<br>
}<br>
}<br><br>
/**<br>
* A simpler value of [[Accumulable]] where the result type being accumulated is the same<br>
* as the types of elements being merged, i.e. variables that are only "added" to through an<br>
* associative operation and can therefore be efficiently supported in parallel. They can be used<br>
* to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric<br>
* value types, and programmers can add support for new types.<br>
*<br>
* An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].<br>
* Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.<br>
* However, they cannot read its value. Only the driver program can read the accumulator's value,<br>
* using its value method.<br>
*<br>
* The interpreter session below shows an accumulator being used to add up the elements of an array:<br>
*<br>
* {{{<br>
* scala&gt; val accum = sc.accumulator(0)<br>
* accum: spark.Accumulator[Int] = 0<br>
*<br>
* scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum += x)<br>
* ...<br>
* 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s<br>
*<br>
* scala&gt; accum.value<br>
* res2: Int = 10<br>
* }}}<br>
*<br>
* @param initialValue initial value of accumulator<br>
* @param param helper object defining how to add elements of type `T`<br>
* @tparam T result type<br>
*/<br>
class Accumulator[T] private[spark] (<br>
@transient private[spark] val initialValue: T,<br>
param: AccumulatorParam[T],<br>
name: Option[String],<br>
internal: Boolean)<br>
extends Accumulable[T, T](initialValue, param, name, internal) {<br><br>
def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = {<br>
this(initialValue, param, name, false)<br>
}<br><br>
def this(initialValue: T, param: AccumulatorParam[T]) = {<br>
this(initialValue, param, None, false)<br>
}<br>
}<br><br>
/**<br>
* A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add<br>
* in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be<br>
* available when you create Accumulators of a specific type.<br>
*<br>
* @tparam T type of value to accumulate<br>
*/<br>
trait AccumulatorParam[T] extends AccumulableParam[T, T] {<br>
def addAccumulator(t1: T, t2: T): T = {<br>
addInPlace(t1, t2)<br>
}<br>
}<br><br>
object AccumulatorParam {<br><br>
// The following implicit objects were in SparkContext before 1.2 and users had to<br>
// `import SparkContext._` to enable them. Now we move them here to make the compiler find<br>
// them automatically. However, as there are duplicate codes in SparkContext for backward<br>
// compatibility, please update them accordingly if you modify the following implicit objects.<br><br>
implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] {<br>
def addInPlace(t1: Double, t2: Double): Double = t1 + t2<br>
def zero(initialValue: Double): Double = 0.0<br>
}<br><br>
implicit object IntAccumulatorParam extends AccumulatorParam[Int] {<br>
def addInPlace(t1: Int, t2: Int): Int = t1 + t2<br>
def zero(initialValue: Int): Int = 0<br>
}<br><br>
implicit object LongAccumulatorParam extends AccumulatorParam[Long] {<br>
def addInPlace(t1: Long, t2: Long): Long = t1 + t2<br>
def zero(initialValue: Long): Long = 0L<br>
}<br><br>
implicit object FloatAccumulatorParam extends AccumulatorParam[Float] {<br>
def addInPlace(t1: Float, t2: Float): Float = t1 + t2<br>
def zero(initialValue: Float): Float = 0f<br>
}<br><br>
// TODO: Add AccumulatorParams for other types, e.g. lists and strings<br>
}<br><br>
// TODO: The multi-thread support in accumulators is kind of lame; check<br>
// if there's a more intuitive way of doing it right<br>
private[spark] object Accumulators extends Logging {<br>
/**<br>
* This global map holds the original accumulator objects that are created on the driver.<br>
* It keeps weak references to these objects so that accumulators can be garbage-collected<br>
* once the RDDs and user-code that reference them are cleaned up.<br>
*/<br>
val originals = mutable.Map[Long, WeakReference[Accumulable[_, _]]]()<br><br>
private var lastId: Long = 0<br><br>
def newId(): Long = synchronized {<br>
lastId += 1<br>
lastId<br>
}<br><br>
def register(a: Accumulable[_, _]): Unit = synchronized {<br>
originals(a.id) = new WeakReference[Accumulable[_, _]](a)<br>
}<br><br>
def remove(accId: Long) {<br>
synchronized {<br>
originals.remove(accId)<br>
}<br>
}<br><br>
// Add values to the original accumulators with some given IDs<br>
def add(values: Map[Long, Any]): Unit = synchronized {<br>
for ((id, value) &lt;- values) {<br>
if (originals.contains(id)) {<br>
// Since we are now storing weak references, we must check whether the underlying data<br>
// is valid.<br>
originals(id).get match {<br>
case Some(accum) =&gt; accum.asInstanceOf[Accumulable[Any, Any]] ++= value<br>
case None =&gt;<br>
throw new IllegalAccessError("Attempted to access garbage collected Accumulator.")<br>
}<br>
} else {<br>
logWarning(s"Ignoring accumulator update for unknown accumulator id $id")<br>
}<br>
}<br>
}<br><br>
}<br><br>
private[spark] object InternalAccumulator {<br>
val PEAK_EXECUTION_MEMORY = "peakExecutionMemory"<br>
val TEST_ACCUMULATOR = "testAccumulator"<br><br>
// For testing only.<br>
// This needs to be a def since we don't want to reuse the same accumulator across stages.<br>
private def maybeTestAccumulator: Option[Accumulator[Long]] = {<br>
if (sys.props.contains("spark.testing")) {<br>
Some(new Accumulator(<br>
0L, AccumulatorParam.LongAccumulatorParam, Some(TEST_ACCUMULATOR), internal = true))<br>
} else {<br>
None<br>
}<br>
}<br><br>
/**<br>
* Accumulators for tracking internal metrics.<br>
*<br>
* These accumulators are created with the stage such that all tasks in the stage will<br>
* add to the same set of accumulators. We do this to report the distribution of accumulator<br>
* values across all tasks within each stage.<br>
*/<br>
def create(sc: SparkContext): Seq[Accumulator[Long]] = {<br>
val internalAccumulators = Seq(<br>
// Execution memory refers to the memory used by internal data structures created<br>
// during shuffles, aggregations and joins. The value of this accumulator should be<br>
// approximately the sum of the peak sizes across all such data structures created<br>
// in this task. For SQL jobs, this only tracks all unsafe operators and ExternalSort.<br>
new Accumulator(<br>
0L, AccumulatorParam.LongAccumulatorParam, Some(PEAK_EXECUTION_MEMORY), internal = true)<br>
) ++ maybeTestAccumulator.toSeq<br>
internalAccumulators.foreach { accumulator =&gt;<br>
sc.cleaner.foreach(_.registerAccumulatorForCleanup(accumulator))<br>
}<br>
internalAccumulators<br>
}<br>
}</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br></p> 
   <p><font><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/5914696.html，如需转载请自行联系原作者</span></font><br></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
