<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>RDD.scala（源码） « NotBeCN</title>
  <meta name="description" content="             ---- map、    --- flatMap、fliter、distinct、repartition、coalesce、sample、randomSplit、randomSampleWithRange、takeSample、union、++、sortBy、intersection  ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/19/weixin_33711647_90135712.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">RDD.scala（源码）</h1>
    <p class="post-meta">Nov 19, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>---- map、</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>--- flatMap、fliter、distinct、repartition、coalesce、sample、randomSplit、randomSampleWithRange、takeSample、union、++、sortBy、intersection</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>map源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927115318922-551605228.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD by applying a function to all elements of this RDD.<br>
*/<br>
def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))<br>
}<br><br><br><br><span style="font-size:18pt;"><strong>flatMap源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927115405938-562847010.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
*  Return a new RDD by first applying a function to all elements of this<br>
*  RDD, and then flattening the results.<br>
*/<br>
def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))<br>
}<br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>fliter源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927115523891-832350610.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD containing only the elements that satisfy a predicate.<br>
*/<br>
def filter(f: T =&gt; Boolean): RDD[T] = withScope {<br>
val cleanF = sc.clean(f)<br>
new MapPartitionsRDD[T, T](<br>
this,<br>
(context, pid, iter) =&gt; iter.filter(cleanF),<br>
preservesPartitioning = true)<br>
}<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>distinct源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927115722750-1289801717.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD containing the distinct elements in this RDD.<br>
*/<br>
def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {<br>
map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)<br>
}<br><br>
/**<br>
* Return a new RDD containing the distinct elements in this RDD.<br>
*/<br>
def distinct(): RDD[T] = withScope {<br>
distinct(partitions.length)<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>repartition源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927115743016-982812790.png" alt="" style="border:0px;"></p> 
   <pre><br>
/**<br>
* Return a new RDD that has exactly numPartitions partitions.<br>
*<br>
* Can increase or decrease the level of parallelism in this RDD. Internally, this uses<br>
* a shuffle to redistribute data.<br>
*<br>
* If you are decreasing the number of partitions in this RDD, consider using `coalesce`,<br>
* which can avoid performing a shuffle.<br>
*/<br>
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {<br>
coalesce(numPartitions, shuffle = true)<br>
}<br><br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>coalesce源码</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927115947453-763887413.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD that is reduced into `numPartitions` partitions.<br>
*<br>
* This results in a narrow dependency, e.g. if you go from 1000 partitions<br>
* to 100 partitions, there will not be a shuffle, instead each of the 100<br>
* new partitions will claim 10 of the current partitions.<br>
*<br>
* However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,<br>
* this may result in your computation taking place on fewer nodes than<br>
* you like (e.g. one node in the case of numPartitions = 1). To avoid this,<br>
* you can pass shuffle = true. This will add a shuffle step, but means the<br>
* current upstream partitions will be executed in parallel (per whatever<br>
* the current partitioning is).<br>
*<br>
* Note: With shuffle = true, you can actually coalesce to a larger number<br>
* of partitions. This is useful if you have a small number of partitions,<br>
* say 100, potentially with a few partitions being abnormally large. Calling<br>
* coalesce(1000, shuffle = true) will result in 1000 partitions with the<br>
* data distributed using a hash partitioner.<br>
*/<br>
def coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null)<br>
: RDD[T] = withScope {<br>
if (shuffle) {<br>
/** Distributes elements evenly across output partitions, starting from a random partition. */<br>
val distributePartition = (index: Int, items: Iterator[T]) =&gt; {<br>
var position = (new Random(index)).nextInt(numPartitions)<br>
items.map { t =&gt;<br>
// Note that the hash code of the key will just be the key itself. The HashPartitioner<br>
// will mod it with the number of total partitions.<br>
position = position + 1<br>
(position, t)<br>
}<br>
} : Iterator[(Int, T)]<br><br>
// include a shuffle step so that our upstream tasks are still distributed<br>
new CoalescedRDD(<br>
new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),<br>
new HashPartitioner(numPartitions)),<br>
numPartitions).values<br>
} else {<br>
new CoalescedRDD(this, numPartitions)<br>
}<br>
}<br><br><br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>sample源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120039735-335385391.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a sampled subset of this RDD.<br>
*<br>
* @param withReplacement can elements be sampled multiple times (replaced when sampled out)<br>
* @param fraction expected size of the sample as a fraction of this RDD's size<br>
*  without replacement: probability that each element is chosen; fraction must be [0, 1]<br>
*  with replacement: expected number of times each element is chosen; fraction must be &gt;= 0<br>
* @param seed seed for the random number generator<br>
*/<br>
def sample(<br>
withReplacement: Boolean,<br>
fraction: Double,<br>
seed: Long = Utils.random.nextLong): RDD[T] = withScope {<br>
require(fraction &gt;= 0.0, "Negative fraction value: " + fraction)<br>
if (withReplacement) {<br>
new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)<br>
} else {<br>
new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)<br>
}<br>
}<br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><span style="font-size:18pt;"><strong>randomSplit源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120111969-222483090.png" alt="" style="border:0px;"></p> 
   <pre><br>
/**<br>
* Randomly splits this RDD with the provided weights.<br>
*<br>
* @param weights weights for splits, will be normalized if they don't sum to 1<br>
* @param seed random seed<br>
*<br>
* @return split RDDs in an array<br>
*/<br>
def randomSplit(<br>
weights: Array[Double],<br>
seed: Long = Utils.random.nextLong): Array[RDD[T]] = withScope {<br>
val sum = weights.sum<br>
val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)<br>
normalizedCumWeights.sliding(2).map { x =&gt;<br>
randomSampleWithRange(x(0), x(1), seed)<br>
}.toArray<br>
}<br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>randomSampleWithRange源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120334156-1389225005.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability<br>
* range.<br>
* @param lb lower bound to use for the Bernoulli sampler<br>
* @param ub upper bound to use for the Bernoulli sampler<br>
* @param seed the seed for the Random number generator<br>
* @return A random sub-sample of the RDD without replacement.<br>
*/<br>
private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = {<br>
this.mapPartitionsWithIndex( { (index, partition) =&gt;<br>
val sampler = new BernoulliCellSampler[T](lb, ub)<br>
sampler.setSeed(seed + index)<br>
sampler.sample(partition)<br>
}, preservesPartitioning = true)<br>
}<br><br><br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>union源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120536547-1606041307.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return the union of this RDD and another one. Any identical elements will appear multiple<br>
* times (use `.distinct()` to eliminate them).<br>
*/<br>
def union(other: RDD[T]): RDD[T] = withScope {<br>
if (partitioner.isDefined &amp;&amp; other.partitioner == partitioner) {<br>
new PartitionerAwareUnionRDD(sc, Array(this, other))<br>
} else {<br>
new UnionRDD(sc, Array(this, other))<br>
}<br>
}<br><br><br><br><span style="font-size:18pt;"><strong>++源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120557688-947976213.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return the union of this RDD and another one. Any identical elements will appear multiple<br>
* times (use `.distinct()` to eliminate them).<br>
*/<br>
def ++(other: RDD[T]): RDD[T] = withScope {<br>
this.union(other)<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>sortBy源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120637531-105934959.png" alt="" style="border:0px;"></p> 
   <pre><br>
/**<br>
* Return this RDD sorted by the given key function.<br>
*/<br>
def sortBy[K](<br>
f: (T) =&gt; K,<br>
ascending: Boolean = true,<br>
numPartitions: Int = this.partitions.length)<br>
(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope {<br>
this.keyBy[K](f)<br>
.sortByKey(ascending, numPartitions)<br>
.values<br>
}<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>intersection源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120718610-604670651.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre>/**<br>
* Return the intersection of this RDD and another one. The output will not contain any duplicate<br>
* elements, even if the input RDDs did.<br>
*<br>
* Note that this method performs a shuffle internally.<br>
*/<br>
def intersection(other: RDD[T]): RDD[T] = withScope {<br>
this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null)))<br>
.filter { case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty }<br>
.keys<br>
}<br><br>
/**<br>
* Return the intersection of this RDD and another one. The output will not contain any duplicate<br>
* elements, even if the input RDDs did.<br>
*<br>
* Note that this method performs a shuffle internally.<br>
*<br>
* @param partitioner Partitioner to use for the resulting RDD<br>
*/<br>
def intersection(<br>
other: RDD[T],<br>
partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope {<br>
this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null)), partitioner)<br>
.filter { case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty }<br>
.keys<br>
}<br><br>
/**<br>
* Return the intersection of this RDD and another one. The output will not contain any duplicate<br>
* elements, even if the input RDDs did.  Performs a hash partition across the cluster<br>
*<br>
* Note that this method performs a shuffle internally.<br>
*<br>
* @param numPartitions How many partitions to use in the resulting RDD<br>
*/<br>
def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope {<br>
intersection(other, new HashPartitioner(numPartitions))<br>
}<br><br><br><span style="font-size:18pt;"><strong>glom源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120854235-303483589.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an RDD created by coalescing all elements within each partition into an array.<br>
*/<br>
def glom(): RDD[Array[T]] = withScope {<br>
new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) =&gt; Iterator(iter.toArray))<br>
}<br><br><br><br></pre> 
   <pre>cartesian源码</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927120930735-2114932777.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of<br>
* elements (a, b) where a is in `this` and b is in `other`.<br>
*/<br>
def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {<br>
new CartesianRDD(sc, this, other)<br>
}<br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>groupBy源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121002641-1134579908.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an RDD of grouped items. Each group consists of a key and a sequence of elements<br>
* mapping to that key. The ordering of elements within each group is not guaranteed, and<br>
* may even differ each time the resulting RDD is evaluated.<br>
*<br>
* Note: This operation may be very expensive. If you are grouping in order to perform an<br>
* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]<br>
* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.<br>
*/<br>
def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {<br>
groupBy[K](f, defaultPartitioner(this))<br>
}<br><br>
/**<br>
* Return an RDD of grouped elements. Each group consists of a key and a sequence of elements<br>
* mapping to that key. The ordering of elements within each group is not guaranteed, and<br>
* may even differ each time the resulting RDD is evaluated.<br>
*<br>
* Note: This operation may be very expensive. If you are grouping in order to perform an<br>
* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]<br>
* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.<br>
*/<br>
def groupBy[K](<br>
f: T =&gt; K,<br>
numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {<br>
groupBy(f, new HashPartitioner(numPartitions))<br>
}<br><br>
/**<br>
* Return an RDD of grouped items. Each group consists of a key and a sequence of elements<br>
* mapping to that key. The ordering of elements within each group is not guaranteed, and<br>
* may even differ each time the resulting RDD is evaluated.<br>
*<br>
* Note: This operation may be very expensive. If you are grouping in order to perform an<br>
* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]<br>
* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.<br>
*/<br>
def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)<br>
: RDD[(K, Iterable[T])] = withScope {<br>
val cleanF = sc.clean(f)<br>
this.map(t =&gt; (cleanF(t), t)).groupByKey(p)<br>
}<br><br><br><br><span style="font-size:14pt;"><strong>pipe源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121044141-1452581221.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an RDD created by piping elements to a forked external process.<br>
*/<br>
def pipe(command: String): RDD[String] = withScope {<br>
new PipedRDD(this, command)<br>
}<br><br>
/**<br>
* Return an RDD created by piping elements to a forked external process.<br>
*/<br>
def pipe(command: String, env: Map[String, String]): RDD[String] = withScope {<br>
new PipedRDD(this, command, env)<br>
}<br><br>
/**<br>
* Return an RDD created by piping elements to a forked external process.<br>
* The print behavior can be customized by providing two functions.<br>
*<br>
* @param command command to run in forked process.<br>
* @param env environment variables to set.<br>
* @param printPipeContext Before piping elements, this function is called as an opportunity<br>
*                         to pipe context data. Print line function (like out.println) will be<br>
*                         passed as printPipeContext's parameter.<br>
* @param printRDDElement Use this function to customize how to pipe elements. This function<br>
*                        will be called with each RDD element as the 1st parameter, and the<br>
*                        print line function (like out.println()) as the 2nd parameter.<br>
*                        An example of pipe the RDD data of groupBy() in a streaming way,<br>
*                        instead of constructing a huge String to concat all the elements:<br>
*                        def printRDDElement(record:(String, Seq[String]), f:String=&amp;gt;Unit) =<br>
*                          for (e &amp;lt;- record._2){f(e)}<br>
* @param separateWorkingDir Use separate working directories for each task.<br>
* @return the result RDD<br>
*/<br>
def pipe(<br>
command: Seq[String],<br>
env: Map[String, String] = Map(),<br>
printPipeContext: (String =&gt; Unit) =&gt; Unit = null,<br>
printRDDElement: (T, String =&gt; Unit) =&gt; Unit = null,<br>
separateWorkingDir: Boolean = false): RDD[String] = withScope {<br>
new PipedRDD(this, command, env,<br>
if (printPipeContext ne null) sc.clean(printPipeContext) else null,<br>
if (printRDDElement ne null) sc.clean(printRDDElement) else null,<br>
separateWorkingDir)<br>
}<br><br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>mapPartitions源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121119094-1501196058.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD by applying a function to each partition of this RDD.<br>
*<br>
* `preservesPartitioning` indicates whether the input function preserves the partitioner, which<br>
* should be `false` unless this is a pair RDD and the input function doesn't modify the keys.<br>
*/<br>
def mapPartitions[U: ClassTag](<br>
f: Iterator[T] =&gt; Iterator[U],<br>
preservesPartitioning: Boolean = false): RDD[U] = withScope {<br>
val cleanedF = sc.clean(f)<br>
new MapPartitionsRDD(<br>
this,<br>
(context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(iter),<br>
preservesPartitioning)<br>
}<br><br><br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>mapPartitionsWithIndex源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121159703-340709990.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD by applying a function to each partition of this RDD, while tracking the index<br>
* of the original partition.<br>
*<br>
* `preservesPartitioning` indicates whether the input function preserves the partitioner, which<br>
* should be `false` unless this is a pair RDD and the input function doesn't modify the keys.<br>
*/<br>
def mapPartitionsWithIndex[U: ClassTag](<br>
f: (Int, Iterator[T]) =&gt; Iterator[U],<br>
preservesPartitioning: Boolean = false): RDD[U] = withScope {<br>
val cleanedF = sc.clean(f)<br>
new MapPartitionsRDD(<br>
this,<br>
(context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(index, iter),<br>
preservesPartitioning)<br>
}<br><br><br><br></pre> 
   <pre></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>mapPartitionsWithContext源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121231719-179280214.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* :: DeveloperApi ::<br>
* Return a new RDD by applying a function to each partition of this RDD. This is a variant of<br>
* mapPartitions that also passes the TaskContext into the closure.<br>
*<br>
* `preservesPartitioning` indicates whether the input function preserves the partitioner, which<br>
* should be `false` unless this is a pair RDD and the input function doesn't modify the keys.<br>
*/<br>
@DeveloperApi<br>
@deprecated("use TaskContext.get", "1.2.0")<br>
def mapPartitionsWithContext[U: ClassTag](<br>
f: (TaskContext, Iterator[T]) =&gt; Iterator[U],<br>
preservesPartitioning: Boolean = false): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
val func = (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanF(context, iter)<br>
new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)<br>
}<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>mapPartitionsWithSplit源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121311438-1383949738.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD by applying a function to each partition of this RDD, while tracking the index<br>
* of the original partition.<br>
*/<br>
@deprecated("use mapPartitionsWithIndex", "0.7.0")<br>
def mapPartitionsWithSplit[U: ClassTag](<br>
f: (Int, Iterator[T]) =&gt; Iterator[U],<br>
preservesPartitioning: Boolean = false): RDD[U] = withScope {<br>
mapPartitionsWithIndex(f, preservesPartitioning)<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>mapWith源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121353125-98926171.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Maps f over this RDD, where f takes an additional parameter of type A.  This<br>
* additional parameter is produced by constructA, which is called in each<br>
* partition with the index of that partition.<br>
*/<br>
@deprecated("use mapPartitionsWithIndex", "1.0.0")<br>
def mapWith[A, U: ClassTag]<br>
(constructA: Int =&gt; A, preservesPartitioning: Boolean = false)<br>
(f: (T, A) =&gt; U): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
val cleanA = sc.clean(constructA)<br>
mapPartitionsWithIndex((index, iter) =&gt; {<br>
val a = cleanA(index)<br>
iter.map(t =&gt; cleanF(t, a))<br>
}, preservesPartitioning)<br>
}<br><br><br><br></pre> 
   <pre><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>flatMapWith源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121415250-697631848.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* FlatMaps f over this RDD, where f takes an additional parameter of type A.  This<br>
* additional parameter is produced by constructA, which is called in each<br>
* partition with the index of that partition.<br>
*/<br>
@deprecated("use mapPartitionsWithIndex and flatMap", "1.0.0")<br>
def flatMapWith[A, U: ClassTag]<br>
(constructA: Int =&gt; A, preservesPartitioning: Boolean = false)<br>
(f: (T, A) =&gt; Seq[U]): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
val cleanA = sc.clean(constructA)<br>
mapPartitionsWithIndex((index, iter) =&gt; {<br>
val a = cleanA(index)<br>
iter.flatMap(t =&gt; cleanF(t, a))<br>
}, preservesPartitioning)<br>
}<br><br></pre> 
   <pre></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>foreachWith源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121509375-250123332.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Applies f to each element of this RDD, where f takes an additional parameter of type A.<br>
* This additional parameter is produced by constructA, which is called in each<br>
* partition with the index of that partition.<br>
*/<br>
@deprecated("use mapPartitionsWithIndex and foreach", "1.0.0")<br>
def foreachWith[A](constructA: Int =&gt; A)(f: (T, A) =&gt; Unit): Unit = withScope {<br>
val cleanF = sc.clean(f)<br>
val cleanA = sc.clean(constructA)<br>
mapPartitionsWithIndex { (index, iter) =&gt;<br>
val a = cleanA(index)<br>
iter.map(t =&gt; {cleanF(t, a); t})<br>
}<br>
}<br><br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>filterWith源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927121629891-1011515335.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Filters this RDD with p, where p takes an additional parameter of type A.  This<br>
* additional parameter is produced by constructA, which is called in each<br>
* partition with the index of that partition.<br>
*/<br>
@deprecated("use mapPartitionsWithIndex and filter", "1.0.0")<br>
def filterWith[A](constructA: Int =&gt; A)(p: (T, A) =&gt; Boolean): RDD[T] = withScope {<br>
val cleanP = sc.clean(p)<br>
val cleanA = sc.clean(constructA)<br>
mapPartitionsWithIndex((index, iter) =&gt; {<br>
val a = cleanA(index)<br>
iter.filter(t =&gt; cleanP(t, a))<br>
}, preservesPartitioning = true)<br>
}<br><br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>zip源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927142251922-1098587253.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Zips this RDD with another one, returning key-value pairs with the first element in each RDD,<br>
* second element in each RDD, etc. Assumes that the two RDDs have the *same number of<br>
* partitions* and the *same number of elements in each partition* (e.g. one was made through<br>
* a map on the other).<br>
*/<br>
def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {<br>
zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) =&gt;<br>
new Iterator[(T, U)] {<br>
def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match {<br>
case (true, true) =&gt; true<br>
case (false, false) =&gt; false<br>
case _ =&gt; throw new SparkException("Can only zip RDDs with " +<br>
"same number of elements in each partition")<br>
}<br>
def next(): (T, U) = (thisIter.next(), otherIter.next())<br>
}<br>
}<br>
}<br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>zipPartitions源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927142322016-850262813.png" alt="" style="border:0px;"></p> 
   <pre><br>
/**<br>
* Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by<br>
* applying a function to the zipped partitions. Assumes that all the RDDs have the<br>
* *same number of partitions*, but does *not* require them to have the same number<br>
* of elements in each partition.<br>
*/<br>
def zipPartitions[B: ClassTag, V: ClassTag]<br>
(rdd2: RDD[B], preservesPartitioning: Boolean)<br>
(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V]): RDD[V] = withScope {<br>
new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning)<br>
}<br><br>
def zipPartitions[B: ClassTag, V: ClassTag]<br>
(rdd2: RDD[B])<br>
(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V]): RDD[V] = withScope {<br>
zipPartitions(rdd2, preservesPartitioning = false)(f)<br>
}<br><br>
def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag]<br>
(rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)<br>
(f: (Iterator[T], Iterator[B], Iterator[C]) =&gt; Iterator[V]): RDD[V] = withScope {<br>
new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning)<br>
}<br><br>
def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag]<br>
(rdd2: RDD[B], rdd3: RDD[C])<br>
(f: (Iterator[T], Iterator[B], Iterator[C]) =&gt; Iterator[V]): RDD[V] = withScope {<br>
zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f)<br>
}<br><br>
def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag]<br>
(rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)<br>
(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&gt; Iterator[V]): RDD[V] = withScope {<br>
new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning)<br>
}<br><br>
def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag]<br>
(rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])<br>
(f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) =&gt; Iterator[V]): RDD[V] = withScope {<br>
zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f)<br>
}<br><br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>foreach源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927142500485-1516181054.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Applies a function f to all elements of this RDD.<br>
*/<br>
def foreach(f: T =&gt; Unit): Unit = withScope {<br>
val cleanF = sc.clean(f)<br>
sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))<br>
}<br><br>
/**<br>
* Applies a function f to each partition of this RDD.<br>
*/<br>
def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope {<br>
val cleanF = sc.clean(f)<br>
sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>collect源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927142534485-1932704228.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an array that contains all of the elements in this RDD.<br>
*/<br>
def collect(): Array[T] = withScope {<br>
val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)<br>
Array.concat(results: _*)<br>
}<br><br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>collectPartition源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927144432766-835102751.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an iterator that contains all of the elements in this RDD.<br>
*<br>
* The iterator will consume as much memory as the largest partition in this RDD.<br>
*<br>
* Note: this results in multiple Spark jobs, and if the input RDD is the result<br>
* of a wide transformation (e.g. join with different partitioners), to avoid<br>
* recomputing the input RDD should be cached first.<br>
*/<br>
def toLocalIterator: Iterator[T] = withScope {<br>
def collectPartition(p: Int): Array[T] = {<br>
sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray, Seq(p)).head<br>
}<br>
(0 until partitions.length).iterator.flatMap(i =&gt; collectPartition(i))<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>toArray源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927144528453-1521624085.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return an array that contains all of the elements in this RDD.<br>
*/<br>
@deprecated("use collect", "1.0.0")<br>
def toArray(): Array[T] = withScope {<br>
collect()<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>collect源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927144604406-808718157.png" alt="" style="border:0px;"></p> 
   <pre><span style="font-size:14px;">/**<br>
* Return an RDD that contains all matching values by applying `f`.<br>
*/<br>
def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
filter(cleanF.isDefinedAt).map(cleanF)<br>
}<br></span>
</pre> 
   <p style="font-size:14px;"><br></p> 
   <p style="font-size:14px;"><br></p> 
   <p><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/5912331.html，如需转载请自行联系原作者<br></span></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
