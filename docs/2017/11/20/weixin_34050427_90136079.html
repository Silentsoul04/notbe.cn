<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Spark RDD/Core 编程 API入门系列 之rdd案例（map、filter、flatMap、groupByKey、reduceByKey、join、cogroupy等）（四）... « NotBeCN</title>
  <meta name="description" content="             声明：    　　大数据中，最重要的算子操作是：join &nbsp;!!!    &nbsp;    &nbsp;    &nbsp;    典型的transformation和action        &nbsp;    &nbsp;    &nbsp;              ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/20/weixin_34050427_90136079.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Spark RDD/Core 编程 API入门系列 之rdd案例（map、filter、flatMap、groupByKey、reduceByKey、join、cogroupy等）（四）...</h1>
    <p class="post-meta">Nov 20, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'times new roman', times;font-size:18pt;color:rgb(255,0,0);"><strong>声明：</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'times new roman', times;font-size:18pt;color:rgb(255,0,0);"><strong>　　大数据中，最重要的算子操作是：join &nbsp;!!!</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="color:rgb(255,0,0);font-size:18pt;"><strong>典型的transformation和action</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927175332766-505727948.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927180833875-1998171387.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927181024453-1104000375.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927181050360-1675532963.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927195550078-1139092265.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927181424906-1089728900.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927200024719-1676025074.png" alt="" style="border:0px;"></p> 
   <pre>val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>map适用于</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927200114125-233784826.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927200710313-23830485.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br>
val mapped = nums.map(item =&gt; 2 + item)<br>
mapped.collect.foreach(println)<br>
}<br>
}<br><br><span style="font-size:18pt;"><strong>map源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927222353141-997647255.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD by applying a function to all elements of this RDD.<br>
*/<br>
def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))<br>
}<br><br></pre> 
   <pre><br><br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><strong><span style="font-size:18pt;">filter适用于</span></strong></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927201229250-1519212368.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927201529469-1189064973.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br>
val mapped = nums.map(item =&gt; 2 + item)<br>
val filtered = nums.filter(item =&gt; item%2 == 0)<br>
filtered.collect.foreach(println)<br>
}<br>
}<br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>filter源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927222502797-1033115551.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Return a new RDD containing only the elements that satisfy a predicate.<br>
*/<br>
def filter(f: T =&gt; Boolean): RDD[T] = withScope {<br>
val cleanF = sc.clean(f)<br>
new MapPartitionsRDD[T, T](<br>
this,<br>
(context, pid, iter) =&gt; iter.filter(cleanF),<br>
preservesPartitioning = true)<br>
}<br><br><br></pre> 
   <pre><br><br><br><br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>flatMap适用于</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927201852438-834750694.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927202338781-185707081.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br><br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br><br>
val mapped = nums.map(item =&gt; 2 + item)<br>
//    mapped.collect.foreach(println)<br><br>
val filtered = nums.filter(item =&gt; item%2 == 0)<br>
//    filtered.collect.foreach(println)<br><br>
val bigData = Array("Scala Spark","Java Hadoop","Java Tachyon")<br>
val bigDataString = sc.parallelize(bigData)<br>
val words = bigDataString.flatMap(line =&gt; line.split(" "))<br>
words.collect.foreach(println)<br>
sc.stop()<br>
}<br>
}<br><br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927202457703-2125738206.png" alt="" style="border:0px;"></p> 
   <pre><span style="font-size:18pt;"><strong><br> flatMap源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927222638094-813358423.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
*  Return a new RDD by first applying a function to all elements of this<br>
*  RDD, and then flattening the results.<br>
*/<br>
def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope {<br>
val cleanF = sc.clean(f)<br>
new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))<br>
}<br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>成为大牛，必写的写法 -&gt;</strong></span></p> 
   <pre><span style="font-size:18pt;"><strong>groupByKey适用于</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927210421985-1009409887.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927210811703-546971686.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927210733594-854484296.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val sc = sparkContext("Transformations Operations") //创建SparkContext<br>
//    mapTransformation(sc)//map案例<br>
//    filterTransformation(sc)//filter案例<br>
//    flatMapTransformation(sc)//flatMap案例<br>
groupByKeyTransformation(sc)<br><br>
sc.stop() //停止sparkContext，释放相关的Driver对象，释放资源<br>
}<br>
def sparkContext(name:String)={<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br>
sc<br>
}<br><br>
def mapTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br>
val mapped = nums.map(item =&gt; 2 * item) //map适用于任何类型的元素且对其作用的集合中的每一个元素循环遍历并调用其作为参数的函数对每一个遍历的元素进行具体化处理<br>
mapped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def filterTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 20) //根据集合创建RDD<br>
val filtered = nums.filter(item =&gt; item%2 == 0)//根据filter中作为参数的函数Boolean来判断符合条件的元素，并基于这些元素构成新的MapPartitionsRDD。<br>
filtered.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def flatMapTransformation(sc:SparkContext){<br>
val bigData = Array("Scala Spark","Java Hadoop","Java Tachyon")//实例化字符串类型的Array<br>
val bigDataString = sc.parallelize(bigData)//创建以字符串为元素类型的MapPartitionsRDD<br>
val words = bigDataString.flatMap(line =&gt; line.split(" "))//首先是通过传入的作为参数的函数来作用于RDD的每个字符串进行单词切分（是以集合的方式存在的），然后把切分后的结果合并成一个大的集合，是{Scala Spark Java Hadoop Java Tachyon}<br>
words.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def groupByKeyTransformation(sc:SparkContext){<br>
val data = Array(Tuple2(100,"Spark"),Tuple2(100,"Tachyon"),Tuple2(70,"Hadoop"),Tuple2(80,"Kafka"),Tuple2(80,"HBase"))<br>
val dataRDD = sc.parallelize(data)<br>
val grouped = dataRDD.groupByKey()<br>
grouped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
}<br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>groupByKey源码<br></strong></span></pre> 
   <pre>**<br>
* Group the values for each key in the RDD into a single sequence. Allows controlling the<br>
* partitioning of the resulting key-value pair RDD by passing a Partitioner.<br>
* The ordering of elements within each group is not guaranteed, and may even differ<br>
* each time the resulting RDD is evaluated.<br>
*<br>
* Note: This operation may be very expensive. If you are grouping in order to perform an<br>
* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]<br>
* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.<br>
*<br>
* Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any<br>
* key in memory. If a key has too many values, it can result in an [[OutOfMemoryError]].<br>
*/<br>
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope {<br>
// groupByKey shouldn't use map side combine because map side combine does not<br>
// reduce the amount of data shuffled and requires all map side data be inserted<br>
// into a hash table, leading to more objects in the old gen.<br>
val createCombiner = (v: V) =&gt; CompactBuffer(v)<br>
val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v<br>
val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2<br>
val bufs = combineByKey[CompactBuffer[V]](<br>
createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false)<br>
bufs.asInstanceOf[RDD[(K, Iterable[V])]]<br>
}<br><br>
/**<br>
* Group the values for each key in the RDD into a single sequence. Hash-partitions the<br>
* resulting RDD with into `numPartitions` partitions. The ordering of elements within<br>
* each group is not guaranteed, and may even differ each time the resulting RDD is evaluated.<br>
*<br>
* Note: This operation may be very expensive. If you are grouping in order to perform an<br>
* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]<br>
* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.<br>
*<br>
* Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any<br>
* key in memory. If a key has too many values, it can result in an [[OutOfMemoryError]].<br>
*/<br>
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] = self.withScope {<br>
groupByKey(new HashPartitioner(numPartitions))<br>
}<br><br></pre> 
   <pre><span style="font-size:18pt;"><strong><br><br></strong></span></pre> 
   <pre><br><br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre><span style="font-size:18pt;"><strong>reduceByKey适用于</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927211546719-383920409.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val sc = sparkContext("Transformations Operations") //创建SparkContext<br>
//    mapTransformation(sc)//map案例<br>
//    filterTransformation(sc)//filter案例<br>
//    flatMapTransformation(sc)//flatMap案例<br>
//    groupByKeyTransformation(sc)//groupByKey案例<br>
reduceByKeyTransformation(sc)//reduceByKey案例<br>
sc.stop() //停止sparkContext，释放相关的Driver对象，释放资源<br>
}<br>
def sparkContext(name:String)={<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br>
sc<br>
}<br><br>
def mapTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br>
val mapped = nums.map(item =&gt; 2 * item) //map适用于任何类型的元素且对其作用的集合中的每一个元素循环遍历并调用其作为参数的函数对每一个遍历的元素进行具体化处理<br>
mapped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def filterTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 20) //根据集合创建RDD<br>
val filtered = nums.filter(item =&gt; item%2 == 0)//根据filter中作为参数的函数Boolean来判断符合条件的元素，并基于这些元素构成新的MapPartitionsRDD。<br>
filtered.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def flatMapTransformation(sc:SparkContext){<br>
val bigData = Array("Scala Spark","Java Hadoop","Java Tachyon")//实例化字符串类型的Array<br>
val bigDataString = sc.parallelize(bigData)//创建以字符串为元素类型的MapPartitionsRDD<br>
val words = bigDataString.flatMap(line =&gt; line.split(" "))//首先是通过传入的作为参数的函数来作用于RDD的每个字符串进行单词切分（是以集合的方式存在的），然后把切分后的结果合并成一个大的集合，是{Scala Spark Java Hadoop Java Tachyon}<br>
words.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def groupByKeyTransformation(sc:SparkContext){<br>
val data = Array(Tuple2(100,"Spark"),Tuple2(100,"Tachyon"),Tuple2(70,"Hadoop"),Tuple2(80,"Kafka"),Tuple2(80,"HBase"))//准备数据<br>
val dataRDD = sc.parallelize(data)//根据集合创建RDD<br>
val grouped = dataRDD.groupByKey()//按照相同的key对value进行分组，分组后的value是一个集合<br>
grouped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def reduceByKeyTransformation(sc:SparkContext){<br>
val lines = sc.textFile("D://SoftWare//spark-1.6.2-bin-hadoop2.6//README.md")<br>
val words = lines.flatMap{ line =&gt; line.split(" ")}<br>
val pairs = words.map { word =&gt; (word,1) }<br>
val wordCountsOdered =  pairs.reduceByKey(_+_)//对相同的key，进行value的累计（包括local和reducer级别同时reduce）<br>
wordCountsOdered.collect.foreach(wordNumberPair =&gt; println(wordNumberPair._1 + ":" + wordNumberPair._2))//收集计算结果并通过foreach循环打印<br>
}<br>
}<br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>reduceByKey源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927222912156-107637755.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Merge the values for each key using an associative reduce function. This will also perform<br>
* the merging locally on each mapper before sending results to a reducer, similarly to a<br>
* "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/<br>
* parallelism level.<br>
*/<br>
def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope {<br>
reduceByKey(defaultPartitioner(self), func)<br>
}<br><br></pre> 
   <pre><br><br><br><br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>join适用于</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927214847656-1746128758.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val sc = sparkContext("Transformations Operations") //创建SparkContext<br>
//    mapTransformation(sc)//map案例<br>
//    filterTransformation(sc)//filter案例<br>
//    flatMapTransformation(sc)//flatMap案例<br>
//    groupByKeyTransformation(sc)//groupByKey案例<br>
//    reduceByKeyTransformation(sc)//reduceByKey案例<br>
joinTransformation(sc)//join案例<br>
sc.stop() //停止sparkContext，释放相关的Driver对象，释放资源<br>
}<br>
def sparkContext(name:String)={<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br>
sc<br>
}<br><br>
def mapTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br>
val mapped = nums.map(item =&gt; 2 * item) //map适用于任何类型的元素且对其作用的集合中的每一个元素循环遍历并调用其作为参数的函数对每一个遍历的元素进行具体化处理<br>
mapped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def filterTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 20) //根据集合创建RDD<br>
val filtered = nums.filter(item =&gt; item%2 == 0)//根据filter中作为参数的函数Boolean来判断符合条件的元素，并基于这些元素构成新的MapPartitionsRDD。<br>
filtered.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def flatMapTransformation(sc:SparkContext){<br>
val bigData = Array("Scala Spark","Java Hadoop","Java Tachyon")//实例化字符串类型的Array<br>
val bigDataString = sc.parallelize(bigData)//创建以字符串为元素类型的MapPartitionsRDD<br>
val words = bigDataString.flatMap(line =&gt; line.split(" "))//首先是通过传入的作为参数的函数来作用于RDD的每个字符串进行单词切分（是以集合的方式存在的），然后把切分后的结果合并成一个大的集合，是{Scala Spark Java Hadoop Java Tachyon}<br>
words.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def groupByKeyTransformation(sc:SparkContext){<br>
val data = Array(Tuple2(100,"Spark"),Tuple2(100,"Tachyon"),Tuple2(70,"Hadoop"),Tuple2(80,"Kafka"),Tuple2(80,"HBase"))//准备数据<br>
val dataRDD = sc.parallelize(data)//根据集合创建RDD<br>
val grouped = dataRDD.groupByKey()//按照相同的key对value进行分组，分组后的value是一个集合<br>
grouped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def reduceByKeyTransformation(sc:SparkContext){<br>
val lines = sc.textFile("D://SoftWare//spark-1.6.2-bin-hadoop2.6//README.md")<br>
val words = lines.flatMap{ line =&gt; line.split(" ")}<br>
val pairs = words.map { word =&gt; (word,1) }<br>
val wordCountsOdered =  pairs.reduceByKey(_+_)//对相同的key，进行value的累计（包括local和reducer级别同时reduce）<br>
wordCountsOdered.collect.foreach(wordNumberPair =&gt; println(wordNumberPair._1 + ":" + wordNumberPair._2))//收集计算结果并通过foreach循环打印<br>
}<br>
def joinTransformation(sc:SparkContext){<br>
val studentNames = Array(Tuple2(1,"Spark"),Tuple2(2,"Tachyon"),Tuple2(3,"Hadoop"))<br>
val studentScores = Array(Tuple2(1,100),Tuple2(2,95),Tuple2(3,65))<br>
val names = sc.parallelize(studentNames)<br>
val scores = sc.parallelize(studentScores)<br>
val studentNamesAndScores = names.join(scores)<br>
studentNamesAndScores.collect.foreach(println)//收集计算结果并通过foreach循环打印<br><br>
}<br>
}<br><br></pre> 
   <pre><span style="font-size:18pt;"><strong>join源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927223213031-1920516122.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* Cartesian join with another [[DataFrame]].<br>
*<br>
* Note that cartesian joins are very expensive without an extra filter that can be pushed down.<br>
*<br>
* @param right Right side of the join operation.<br>
* @group dfops<br>
* @since 1.3.0<br>
*/<br>
def join(right: DataFrame): DataFrame = {<br>
Join(logicalPlan, right.logicalPlan, joinType = Inner, None)<br>
}<br><br>
/**<br>
* Inner equi-join with another [[DataFrame]] using the given column.<br>
*<br>
* Different from other join functions, the join column will only appear once in the output,<br>
* i.e. similar to SQL's `JOIN USING` syntax.<br>
*<br>
* {{{<br>
*   // Joining df1 and df2 using the column "user_id"<br>
*   df1.join(df2, "user_id")<br>
* }}}<br>
*<br>
* Note that if you perform a self-join using this function without aliasing the input<br>
* [[DataFrame]]s, you will NOT be able to reference any columns after the join, since<br>
* there is no way to disambiguate which side of the join you would like to reference.<br>
*<br>
* @param right Right side of the join operation.<br>
* @param usingColumn Name of the column to join on. This column must exist on both sides.<br>
* @group dfops<br>
* @since 1.4.0<br>
*/<br>
def join(right: DataFrame, usingColumn: String): DataFrame = {<br>
join(right, Seq(usingColumn))<br>
}<br><br>
/**<br>
* Inner equi-join with another [[DataFrame]] using the given columns.<br>
*<br>
* Different from other join functions, the join columns will only appear once in the output,<br>
* i.e. similar to SQL's `JOIN USING` syntax.<br>
*<br>
* {{{<br>
*   // Joining df1 and df2 using the columns "user_id" and "user_name"<br>
*   df1.join(df2, Seq("user_id", "user_name"))<br>
* }}}<br>
*<br>
* Note that if you perform a self-join using this function without aliasing the input<br>
* [[DataFrame]]s, you will NOT be able to reference any columns after the join, since<br>
* there is no way to disambiguate which side of the join you would like to reference.<br>
*<br>
* @param right Right side of the join operation.<br>
* @param usingColumns Names of the columns to join on. This columns must exist on both sides.<br>
* @group dfops<br>
* @since 1.4.0<br>
*/<br>
def join(right: DataFrame, usingColumns: Seq[String]): DataFrame = {<br>
// Analyze the self join. The assumption is that the analyzer will disambiguate left vs right<br>
// by creating a new instance for one of the branch.<br>
val joined = sqlContext.executePlan(<br>
Join(logicalPlan, right.logicalPlan, joinType = Inner, None)).analyzed.asInstanceOf[Join]<br><br>
// Project only one of the join columns.<br>
val joinedCols = usingColumns.map(col =&gt; joined.right.resolve(col))<br>
val condition = usingColumns.map { col =&gt;<br>
catalyst.expressions.EqualTo(joined.left.resolve(col), joined.right.resolve(col))<br>
}.reduceLeftOption[catalyst.expressions.BinaryExpression] { (cond, eqTo) =&gt;<br>
catalyst.expressions.And(cond, eqTo)<br>
}<br><br>
Project(<br>
joined.output.filterNot(joinedCols.contains(_)),<br>
Join(<br>
joined.left,<br>
joined.right,<br>
joinType = Inner,<br>
condition)<br>
)<br>
}<br><br>
/**<br>
* Inner join with another [[DataFrame]], using the given join expression.<br>
*<br>
* {{{<br>
*   // The following two are equivalent:<br>
*   df1.join(df2, $"df1Key" === $"df2Key")<br>
*   df1.join(df2).where($"df1Key" === $"df2Key")<br>
* }}}<br>
* @group dfops<br>
* @since 1.3.0<br>
*/<br>
def join(right: DataFrame, joinExprs: Column): DataFrame = join(right, joinExprs, "inner")<br><br>
/**<br>
* Join with another [[DataFrame]], using the given join expression. The following performs<br>
* a full outer join between `df1` and `df2`.<br>
*<br>
* {{{<br>
*   // Scala:<br>
*   import org.apache.spark.sql.functions._<br>
*   df1.join(df2, $"df1Key" === $"df2Key", "outer")<br>
*<br>
*   // Java:<br>
*   import static org.apache.spark.sql.functions.*;<br>
*   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");<br>
* }}}<br>
*<br>
* @param right Right side of the join.<br>
* @param joinExprs Join expression.<br>
* @param joinType One of: `inner`, `outer`, `left_outer`, `right_outer`, `leftsemi`.<br>
* @group dfops<br>
* @since 1.3.0<br>
*/<br>
def join(right: DataFrame, joinExprs: Column, joinType: String): DataFrame = {<br>
// Note that in this function, we introduce a hack in the case of self-join to automatically<br>
// resolve ambiguous join conditions into ones that might make sense [SPARK-6231].<br>
// Consider this case: df.join(df, df("key") === df("key"))<br>
// Since df("key") === df("key") is a trivially true condition, this actually becomes a<br>
// cartesian join. However, most likely users expect to perform a self join using "key".<br>
// With that assumption, this hack turns the trivially true condition into equality on join<br>
// keys that are resolved to both sides.<br><br>
// Trigger analysis so in the case of self-join, the analyzer will clone the plan.<br>
// After the cloning, left and right side will have distinct expression ids.<br>
val plan = Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr))<br>
.queryExecution.analyzed.asInstanceOf[Join]<br><br>
// If auto self join alias is disabled, return the plan.<br>
if (!sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) {<br>
return plan<br>
}<br><br>
// If left/right have no output set intersection, return the plan.<br>
val lanalyzed = this.logicalPlan.queryExecution.analyzed<br>
val ranalyzed = right.logicalPlan.queryExecution.analyzed<br>
if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {<br>
return plan<br>
}<br><br>
// Otherwise, find the trivially true predicates and automatically resolves them to both sides.<br>
// By the time we get here, since we have already run analysis, all attributes should've been<br>
// resolved and become AttributeReference.<br>
val cond = plan.condition.map { _.transform {<br>
case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)<br>
if a.sameRef(b) =&gt;<br>
catalyst.expressions.EqualTo(plan.left.resolve(a.name), plan.right.resolve(b.name))<br>
}}<br>
plan.copy(condition = cond)<br>
}<br><br><br><br></pre> 
   <pre><br><br></pre> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>cogroup的scala版，适用于</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927221829297-756209557.png" alt="" style="border:0px;"></p> 
   <pre>package com.zhouls.spark.cores<br><br>
import org.apache.spark.{SparkConf, SparkContext}<br><br>
/**<br>
* Created by Administrator on 2016/9/27.<br>
*/<br>
object Transformations {<br>
def main(args: Array[String]) {<br>
val sc = sparkContext("Transformations Operations") //创建SparkContext<br>
//    mapTransformation(sc)//map案例<br>
//    filterTransformation(sc)//filter案例<br>
//    flatMapTransformation(sc)//flatMap案例<br>
//    groupByKeyTransformation(sc)//groupByKey案例<br>
//    reduceByKeyTransformation(sc)//reduceByKey案例<br>
//    joinTransformation(sc)//join案例<br>
cogroupTransformation(sc)//cogroup案例<br>
sc.stop() //停止sparkContext，释放相关的Driver对象，释放资源<br>
}<br>
def sparkContext(name:String)={<br>
val conf = new SparkConf().setAppName("Transformations").setMaster("local")<br>
val sc = new SparkContext(conf)<br>
sc<br>
}<br><br>
def mapTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 10) //根据集合创建RDD<br>
val mapped = nums.map(item =&gt; 2 * item) //map适用于任何类型的元素且对其作用的集合中的每一个元素循环遍历并调用其作为参数的函数对每一个遍历的元素进行具体化处理<br>
mapped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def filterTransformation(sc:SparkContext){<br>
val nums = sc.parallelize(1 to 20) //根据集合创建RDD<br>
val filtered = nums.filter(item =&gt; item%2 == 0)//根据filter中作为参数的函数Boolean来判断符合条件的元素，并基于这些元素构成新的MapPartitionsRDD。<br>
filtered.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def flatMapTransformation(sc:SparkContext){<br>
val bigData = Array("Scala Spark","Java Hadoop","Java Tachyon")//实例化字符串类型的Array<br>
val bigDataString = sc.parallelize(bigData)//创建以字符串为元素类型的MapPartitionsRDD<br>
val words = bigDataString.flatMap(line =&gt; line.split(" "))//首先是通过传入的作为参数的函数来作用于RDD的每个字符串进行单词切分（是以集合的方式存在的），然后把切分后的结果合并成一个大的集合，是{Scala Spark Java Hadoop Java Tachyon}<br>
words.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def groupByKeyTransformation(sc:SparkContext){<br>
val data = Array(Tuple2(100,"Spark"),Tuple2(100,"Tachyon"),Tuple2(70,"Hadoop"),Tuple2(80,"Kafka"),Tuple2(80,"HBase"))//准备数据<br>
val dataRDD = sc.parallelize(data)//根据集合创建RDD<br>
val grouped = dataRDD.groupByKey()//按照相同的key对value进行分组，分组后的value是一个集合<br>
grouped.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def reduceByKeyTransformation(sc:SparkContext){<br>
val lines = sc.textFile("D://SoftWare//spark-1.6.2-bin-hadoop2.6//README.md")<br>
val words = lines.flatMap{ line =&gt; line.split(" ")}<br>
val pairs = words.map { word =&gt; (word,1) }<br>
val wordCountsOdered =  pairs.reduceByKey(_+_)//对相同的key，进行value的累计（包括local和reducer级别同时reduce）<br>
wordCountsOdered.collect.foreach(wordNumberPair =&gt; println(wordNumberPair._1 + ":" + wordNumberPair._2))//收集计算结果并通过foreach循环打印<br>
}<br>
def joinTransformation(sc:SparkContext){<br>
val studentNames = Array(Tuple2(1,"Spark"),Tuple2(2,"Tachyon"),Tuple2(3,"Hadoop"))<br>
val studentScores = Array(Tuple2(1,100),Tuple2(2,95),Tuple2(3,65))<br>
val names = sc.parallelize(studentNames)<br>
val scores = sc.parallelize(studentScores)<br>
val studentNamesAndScores = names.join(scores)<br>
studentNamesAndScores.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
def cogroupTransformation(sc:SparkContext){<br>
val namesLists = Array(Tuple2(1,"xiaoming"),Tuple2(2,"xiaozhou"),Tuple2(3,"xiaoliu"))<br>
val scoresLists = Array(Tuple2(1,100),Tuple2(2,95),Tuple2(3,85),Tuple2(1,75),Tuple2(2,65),Tuple2(3,55))<br>
val names = sc.parallelize(namesLists)<br>
val scores = sc.parallelize(scoresLists)<br>
val namesListsAndScores = names.cogroup(scores)<br>
namesListsAndScores.collect.foreach(println)//收集计算结果并通过foreach循环打印<br>
}<br>
}<br><br></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927221228094-2130060815.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <pre></pre> 
   <pre><span style="font-size:18pt;"><strong>cogroup源码</strong></span></pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/855959/201609/855959-20160927223351953-1178693331.png" alt="" style="border:0px;"></p> 
   <pre>/**<br>
* For each key k in `this` or `other1` or `other2` or `other3`,<br>
* return a resulting RDD that contains a tuple with the list of values<br>
* for that key in `this`, `other1`, `other2` and `other3`.<br>
*/<br>
def cogroup[W1, W2, W3](other1: RDD[(K, W1)],<br>
other2: RDD[(K, W2)],<br>
other3: RDD[(K, W3)],<br>
partitioner: Partitioner)<br>
: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))] = self.withScope {<br>
if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) {<br>
throw new SparkException("Default partitioner cannot partition array keys.")<br>
}<br>
val cg = new CoGroupedRDD[K](Seq(self, other1, other2, other3), partitioner)<br>
cg.mapValues { case Array(vs, w1s, w2s, w3s) =&gt;<br>
(vs.asInstanceOf[Iterable[V]],<br>
w1s.asInstanceOf[Iterable[W1]],<br>
w2s.asInstanceOf[Iterable[W2]],<br>
w3s.asInstanceOf[Iterable[W3]])<br>
}<br>
}<br><br>
/**<br>
* For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the<br>
* list of values for that key in `this` as well as `other`.<br>
*/<br>
def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner)<br>
: RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope {<br>
if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) {<br>
throw new SparkException("Default partitioner cannot partition array keys.")<br>
}<br>
val cg = new CoGroupedRDD[K](Seq(self, other), partitioner)<br>
cg.mapValues { case Array(vs, w1s) =&gt;<br>
(vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W]])<br>
}<br>
}<br><br>
/**<br>
* For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a<br>
* tuple with the list of values for that key in `this`, `other1` and `other2`.<br>
*/<br>
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], partitioner: Partitioner)<br>
: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))] = self.withScope {<br>
if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) {<br>
throw new SparkException("Default partitioner cannot partition array keys.")<br>
}<br>
val cg = new CoGroupedRDD[K](Seq(self, other1, other2), partitioner)<br>
cg.mapValues { case Array(vs, w1s, w2s) =&gt;<br>
(vs.asInstanceOf[Iterable[V]],<br>
w1s.asInstanceOf[Iterable[W1]],<br>
w2s.asInstanceOf[Iterable[W2]])<br>
}<br>
}<br><br>
/**<br>
* For each key k in `this` or `other1` or `other2` or `other3`,<br>
* return a resulting RDD that contains a tuple with the list of values<br>
* for that key in `this`, `other1`, `other2` and `other3`.<br>
*/<br>
def cogroup[W1, W2, W3](other1: RDD[(K, W1)], other2: RDD[(K, W2)], other3: RDD[(K, W3)])<br>
: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))] = self.withScope {<br>
cogroup(other1, other2, other3, defaultPartitioner(self, other1, other2, other3))<br>
}<br><br>
/**<br>
* For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the<br>
* list of values for that key in `this` as well as `other`.<br>
*/<br>
def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope {<br>
cogroup(other, defaultPartitioner(self, other))<br>
}<br><br>
/**<br>
* For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a<br>
* tuple with the list of values for that key in `this`, `other1` and `other2`.<br>
*/<br>
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)])<br>
: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))] = self.withScope {<br>
cogroup(other1, other2, defaultPartitioner(self, other1, other2))<br>
}<br><br>
/**<br>
* For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the<br>
* list of values for that key in `this` as well as `other`.<br>
*/<br>
def cogroup[W](<br>
other: RDD[(K, W)],<br>
numPartitions: Int): RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope {<br>
cogroup(other, new HashPartitioner(numPartitions))<br>
}<br><br>
/**<br>
* For each key k in `this` or `other1` or `other2`, return a resulting RDD that contains a<br>
* tuple with the list of values for that key in `this`, `other1` and `other2`.<br>
*/<br>
def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], numPartitions: Int)<br>
: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))] = self.withScope {<br>
cogroup(other1, other2, new HashPartitioner(numPartitions))<br>
}<br><br>
/**<br>
* For each key k in `this` or `other1` or `other2` or `other3`,<br>
* return a resulting RDD that contains a tuple with the list of values<br>
* for that key in `this`, `other1`, `other2` and `other3`.<br>
*/<br>
def cogroup[W1, W2, W3](other1: RDD[(K, W1)],<br>
other2: RDD[(K, W2)],<br>
other3: RDD[(K, W3)],<br>
numPartitions: Int)<br>
: RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))] = self.withScope {<br>
cogroup(other1, other2, other3, new HashPartitioner(numPartitions))<br>
}</pre> 
   <pre><span style="font-size:14px;"><br><br></span>
</pre> 
   <p style="font-size:14px;"><br></p> 
   <p style="font-size:14px;"><br></p> 
   <p><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/5913846.html，如需转载请自行联系原作者<br></span></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
