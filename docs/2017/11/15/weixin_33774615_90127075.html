<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>感知机的简单理解 « NotBeCN</title>
  <meta name="description" content="             一，感知机模型    1，超平面的定义    令w1，w2，...wn，v都是实数(R)&nbsp;，其中至少有一个wi不为零，由所有满足线性方程w1*x1+w2*x2+...+wn*xn=v    的点X=[x1,x2,...xn]组成的集合，称为空间R的超平面。    从定义可以看出...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/15/weixin_33774615_90127075.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">感知机的简单理解</h1>
    <p class="post-meta">Nov 15, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-family:'Microsoft YaHei';font-size:18px;">一，感知机模型</span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-family:'Microsoft YaHei';font-size:16px;">1，超平面的定义</span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">令w<sub>1</sub>，w<sub>2</sub>，...w<sub>n</sub>，v都是实数(R)&nbsp;，其中至少有一个w<sub>i</sub>不为零，由所有满足线性方程w<sub>1</sub>*x<sub>1</sub>+w<sub>2</sub>*x<sub>2</sub>+...+w<sub>n</sub>*x<sub>n</sub>=v</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">的点X=[x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>]组成的集合，称为空间R的超平面。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">从定义可以看出：超平面就是点的集合。集合中的某一点X，与向量w=[w<sub>1</sub>，w<sub>2</sub>，...w<sub>n</sub>]的内积，等于v</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">特殊地，如果令v等于0，对于训练集中某个点X：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">w*X=w<sub>1</sub>*x<sub>1</sub>+w<sub>2</sub>*x<sub>2</sub>+...+w<sub>n</sub>*x<sub>n</sub>&gt;0，将X标记为一类</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">w*X=w<sub>1</sub>*x<sub>1</sub>+w<sub>2</sub>*x<sub>2</sub>+...+w<sub>n</sub>*x<sub>n</sub>&lt;0，将X标记为另一类</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;">2，数据集的线性可分</span></span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;">对于数据集T={(X<sub>1</sub>, y<sub>1</sub>),(X<sub>2</sub>, y<sub>2</sub>)...(X<sub>N</sub>, y<sub>N</sub>)}，X<sub>i</sub>&nbsp;belongs to R<sup>n</sup>，y<sub>i</sub>&nbsp;belongs to {-1, 1}，i=1,2,...N</span></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;">若存在某个超平面S：w*X=0</span></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;">将数据集中的<strong>所有</strong>样本点<strong>正确地</strong>分类，则称数据集T线性可分。</span></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;">所谓正确地分类，就是：如果w*X<sub>i</sub>&gt;0，那么样本点(X<sub>i</sub>, y<sub>i</sub>)中的 y<sub>i</sub>&nbsp;等于1</span></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';"><span style="font-size:16px;">如果w*X<sub>i</sub>&lt;0，那么样本点(X<sub>i</sub>, y<sub>i</sub>)中的 y<sub>i</sub>&nbsp;等于-1</span></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">因此，给定超平面 w*X=0，对于数据集 T中<strong>任何一个</strong>点(X<sub>i</sub>, y<sub>i</sub>)，都有<span style="color:rgb(0,0,255);">y<sub>i</sub>(w*X<sub>i</sub>)&gt;0</span>，这样T中所有的样本点都被<span style="color:rgb(0,0,255);">正确地分类</span>了。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">如果有某个点(X<sub>i</sub>, y<sub>i</sub>)，使得<span style="color:rgb(255,0,0);">y<sub>i</sub>(w*X<sub>i</sub>)&lt;0</span>，则称超平面w*X对该点<span style="color:rgb(255,0,0);">分类失败</span>，这个点就是一个误分类的点。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-family:'Microsoft YaHei';font-size:16px;">3，感知机模型</span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">f(X)=sign(w*X+b)，其中sign是<a href="http://baike.baidu.com/link?url=TFHiQ9ofdMZS55mKmgc3wutxha--3_wCaOZdLYjF3WA6w8lQVB5yXqRKt4hFXApVR1D2TCFsn_ZYEgHr7rlQIxYG2lz2Do-9y_8lOPC137g6zxTG6LE9BlMQvtr6PDtq" rel="nofollow" style="color:#000000;">符号函数</a>。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">感知机模型，对应着一个超平面w*X+b=0，这个超平面的参数是(w,b)，w是超平面的法向量，b是超平面的截距。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">我们的目标是，找到一个(w,b)，能够将线性可分的数据集T中的所有的样本点正确地分成两类。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">如何找到(w,b)？---感知机学习算法做的事</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18px;"><strong><span style="font-family:'Microsoft YaHei';">二，感知机学习算法</span></strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">既然我们的目标是将T中所有的样本点正确地分类，那么对于某个(w,b)，先看一下它在哪些点上分类失败了。由前面所述：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">如果有某个点(X<sub>i</sub>, y<sub>i</sub>)，使得y<sub>i</sub>(w*X<sub>i</sub>)&lt;0，则称超平面w*X对该点分类失败。采用所有<span style="color:rgb(255,0,0);"><strong>误分类的点到超平面的距离</strong></span>来衡量分类失败的程度。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">n维空间R<sup>n</sup>中一点X<sub>i</sub>到超平面w*X+b=0的距离公式：<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415165513330-431620552.jpg" alt="" width="132" height="52" style="border:0px;">，可参考<a href="http://www.cnblogs.com/graphics/archive/2010/07/10/1774809.html" rel="nofollow" style="color:#000000;">点到平面的距离公式</a>。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">对于误分类的点，有y<sub>i</sub>(w*X<sub>i</sub>)&lt;0。而距离总是大于0的，因此，某个误分类的点到超平面的距离表示为：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;"><img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415170501689-554504948.jpg" alt="" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">||w||是一个常数，对最小化L(w,b)没有影响，故忽略之。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">假设所有的误分类点都在集合M中，得到的损失函数如下：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;"><img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415170954345-1357532215.jpg" alt="" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">最终，将寻找(w,b)问题转化为最小化损失函数，即转化为一个最优化问题。（损失函数越小，说明误分类的样本点“越少”---或者说分类失败的程度越低）</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">而这个优化问题可以用梯度下降法来寻找最优的(w,b)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">1，为什么使用梯度下降？(没有<a href="http://baike.baidu.com/link?url=YARrru7gdqKRUPVt9-CABcxiCcmPSYJGui6FpHkPtcm2iYtCBLU-csCLtHGw1MWWDAlw4fxjYqlLe9v9LWAuq2jr19sEFCjQHGaPlVpBL92NzWQpZ3AXqJLZYtoat0nl" rel="nofollow" style="color:#000000;">解析解</a>，用数值解来求解)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">minL(w,b)没有 analytical solution，我的理解是：不能通过数学公式各种推导，最终求得一个关于最优的w 和 b 的解。比如，Stanford CS229课程中cs229-notes1.pdf 中求解 linear regression 的代价函数J(θ)的参数θ，可以使用Normal Equation 方法：<span style="text-decoration:underline;">直接根据公式得到θ</span></span></p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-family:'Courier New';font-size:12px;">
    <pre><span style="font-size:16px;line-height:1.5;">Gradient descent gives one way of minimizing J. Let’s discuss a second way of doing so, </span><br><span style="font-size:16px;line-height:1.5;">this time performing the minimization explicitly and without resorting to an <span style="color:rgb(255,0,0);font-size:12px;line-height:1.5;">iterative algorithm</span>.</span> </pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">上面说的second way of doing so. second way就是Normal Equation方法</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">梯度下降，则是一种迭代算法，使用不断迭代(w,b)的方式，使得L(w,b)变得越来越小。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">另外一个原因是（不知道对不对）：在《机器学习基石》视频中讲到NP-Hard问题。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">x<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415165446751-1057524964.jpg" alt="" width="575" height="208" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">梯度方向是函数L(w,b)增加最快的方向，而现在要最小化L(w,b)，在当前的(w,b)点，从<strong>梯度的负方向</strong>搜索下一个(w,b)点，那么找到的(w,b)就是使L(w,b)减少的最快的方向上的那个点(还与搜索的步长有关)了。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">现在证明为什么梯度方向是函数f(x)增加最快的方向，这里涉及到几个概念：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">①梯度</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">梯度是个向量，由于x=[x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>]是n维的，梯度就是对x的各个分量x<sub>i</sub>求偏导数：<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415173049564-2077570933.jpg" alt="" width="253" height="49" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">②增加的方向---方向导数</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">给定一个函数f(x)，通俗地讲方向导数就是决定 自变量x 可以往哪个方向移动。比如在f(x)边界上，x往某个方向移动时，就不在定义域的范围内了，那x是不能往该方向变化的。假设 d 是f(x)的一个可行方向，||d||=1，那么函数f 的值在 x 处 沿着方向d 的增长率increase_rate，可以用内积表示：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;"><img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415173523626-1468248876.jpg" alt="" width="338" height="41" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">那么现在，到底往哪个方向增长，能够使 increase_rate 最大呢？也即，d取哪个方向，使 increase_rate 最大？</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">根据<a href="https://zh.wikipedia.org/wiki/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%E4%B8%8D%E7%AD%89%E5%BC%8F" rel="nofollow" style="color:#000000;">柯西-施瓦兹不等式</a>有：(后面算法的收敛性证明也会用到)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415184747830-2064869765.jpg" alt="" width="375" height="37" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">也就是说，increase_rate的最大值为：<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415184845767-185044960.jpg" alt="" width="89" height="29" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">而当 d =<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415185045298-1648349918.jpg" alt="" width="78" height="59" style="border:0px;">&nbsp;时，根据内积的定义：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">increase_rate等于：<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415185309783-2028331079.jpg" alt="" width="266" height="52" style="border:0px;">&nbsp; &nbsp; 此时，increase_rate取最大值，且可行方向d 就是梯度方向！</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<span style="font-family:'Microsoft YaHei';font-size:16px;">因此，若要最小化L(w,b)，往梯度的负方向搜索(w,b)，能使L(w,b)下降得最快。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">根据前面的介绍，梯度实际上是求偏导数，因此L(w,b)分别对w 和 b 求偏导数：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415185853251-1066540004.jpg" alt="" width="208" height="125" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">而在感知机中是一次只选择一个误分类点对(w,b)进行迭代更新，选择梯度的<span style="color:rgb(255,0,0);"><strong>负</strong></span>方向进行更新，故更新的公式如下所示：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415190036361-941517890.jpg" alt="" width="138" height="58" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-family:'Microsoft YaHei';font-size:18px;">三，感知机学习算法的收敛性证明</span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">由于数据集T是线性可分的，假设存在一个理想的超平面w<sub>opt</sub>，且<span style="color:rgb(51,102,255);">||w<sub>opt</sub>||=1</span>，w<sub>opt</sub>能够将T中的所有样本点正确地分类。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">如前所述，若正确分类，则对于数据集 T中<strong>任何一个</strong>点(X<sub>i</sub>, y<sub>i</sub>)，都有y<sub>i</sub>(w*X<sub>i</sub>)&gt;0</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;"><span style="color:rgb(255,153,0);">取r=min{y<sub>i</sub>(w<sub>opt</sub>*X<sub>i</sub>)}</span>&nbsp;，</span><span style="font-family:'Microsoft YaHei';font-size:16px;">并且令R=max||x<sub>i</sub>|| &nbsp; ，则迭代次数k满足下列不等式：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;"><img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415191313642-1113167744.jpg" alt="" width="93" height="53" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">具体证明过程见《统计学习方法》p32-p33页</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">在证明过程中，推导出了两个不等式，<strong>一个是：w<sub>k</sub>*w<sub>opt</sub>&nbsp;&nbsp;&gt;= &nbsp;knr</strong>&nbsp;&nbsp;(k是迭代次数，n是迭代步长，<span style="color:rgb(255,153,0);">r是min{y<sub>i</sub>(w<sub>opt</sub>*X<sub>i</sub>)</span>)</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">超平面w<sub>opt</sub>是理想的超平面，能够完美地将所有的样本点正确地分开。w<sub>k</sub>是采用感知机学习算法使用梯度下降不断迭代求解的超平面，二者之间的内积，用来衡量这两个超平面的接近程度。因为<span style="color:rgb(255,0,0);"><strong>两个向量的内积越大，说明这两个向量越相似(接近),</strong><span style="color:rgb(0,0,0);">也就是说:不断迭代后的w<sub>k</sub>越来越接近理想的超平面w<sub>opt</sub>了。</span></span>（向量的模为1，||w<sub>opt</sub>||=1）</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="text-decoration:underline;"><em><span style="font-family:'Microsoft YaHei';font-size:16px;">第二个不等式是：<img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415193424798-325717351.jpg" alt="" width="142" height="43" style="border:0px;"></span></em></span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">结合上面的二个不等式，有：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;"><img src="https://images2015.cnblogs.com/blog/715283/201704/715283-20170415193335626-690882581.jpg" alt="" width="364" height="39" style="border:0px;"></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">其中第一个不等号成立是因为：<strong>w<sub>k</sub>*w<sub>opt</sub>&nbsp;&nbsp;&gt;= &nbsp;knr</strong>&nbsp;</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">第二个不等号成立是因为：柯西-施瓦兹不等式</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">第三个不等号成立是因为：<strong><em>第二个不等式</em></strong>&nbsp;和 ||w<sub>opt</sub>||=1</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18px;"><strong><span style="font-family:'Microsoft YaHei';">四，参考文献</span></strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">《统计学习方法》</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-family:'Microsoft YaHei';font-size:16px;">《最优化导论》</span></p> 
   <p><span><font>本文转自hapjin博客园博客，原文链接：http://www.cnblogs.com/hapjin/p/6714526.html，如需转载请自行联系原作者</font><br></span></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
