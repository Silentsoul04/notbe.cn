<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>运用mapreduce计算tf-idf « NotBeCN</title>
  <meta name="description" content="             问题描写叙述：给定一个大文件，文件里的内容每一行为：文档名，文档内容。    input    文档名1，word1 Word2 .......    文档名2，word1 Word2 .......    output    word &nbsp;文档名 &nbsp;tfidf值   ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/28/weixin_33720956_90129962.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">运用mapreduce计算tf-idf</h1>
    <p class="post-meta">Nov 28, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;">问题描写叙述：给定一个大文件，文件里的内容每一行为：文档名，文档内容。</p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;">input</p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="line-height:1.8;"></span>文档名1，word1 Word2 .......</p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="line-height:1.8;">文档名</span>2<span style="line-height:1.8;">，word1 Word2 .......</span></p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;">output</p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="line-height:1.8;"></span>word &nbsp;文档名 &nbsp;tfidf值</p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="line-height:1.8;"></span></p> 
   <pre><code class="language-java">package com.elex.mapreduce;

import java.io.IOException;
import java.net.URI;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Map;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import com.elex.mapreduce.TFIDF_4.IDFMap;
import com.elex.mapreduce.TFIDF_4.IDFReduce;
import com.elex.utils.DataClean;
import com.google.common.io.Closeables;

public class TFIDF_5 {
	public static String hdfsURL = "hdfs://namenode:8020";
	public static String fileURL = "/tmp/usercount";

	public static class TFMap extends Mapper&lt;Object, Text, Text, Text&gt; {
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {
			String userWordstmp = value.toString();
			StringTokenizer userWords = new StringTokenizer(userWordstmp, "\n");
			while (userWords.hasMoreTokens()) {
				String userWordFragtmp = userWords.nextToken();
				StringTokenizer userWordFrag = new StringTokenizer(
						userWordFragtmp, ",");
				String user = userWordFrag.nextToken();
				Text outputKey = new Text();
				Text outputValue = new Text();
				while (userWordFrag.hasMoreTokens()) {
					String words = userWordFrag.nextToken();
					HashMap&lt;String, Integer&gt; wordMap = DataClean.clean(words,
							"!total");
					int wordTotal = wordMap.get("!total");
					wordMap.remove("!total");
					for (Map.Entry&lt;String, Integer&gt; wordEntry : wordMap
							.entrySet()) {
						String word = wordEntry.getKey();
						int wordCount = wordEntry.getValue();
						float tf = (float) wordCount / (float) wordTotal;
						String outputStr = word + " " + Float.toString(tf)
								+ ",";
						byte[] bytes = outputStr.getBytes();
						outputValue.append(bytes, 0, bytes.length);
					}
				}
				outputKey.set(user);
				context.write(outputKey, outputValue);
			}
		}
	}

	public static class TFReduce extends Reducer&lt;Text, Text, Text, Text&gt; {
		public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				throws IOException, InterruptedException {
			// StringBuffer sb = new StringBuffer();
			Iterator&lt;Text&gt; iter = values.iterator();
			while (iter.hasNext()) {
				// sb.append(iter.next().toString() + "\t");
				context.write(key, iter.next());
			}
			// Text outputValue = new Text();
			// outputValue.set(sb.toString());
			// context.write(key, outputValue);
		}
	}

	public static class IDFMap extends Mapper&lt;Object, Text, Text, Text&gt; {
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {
			String valuesTmp = value.toString();
			StringTokenizer userWordFrag = new StringTokenizer(valuesTmp, "\n");
			while (userWordFrag.hasMoreTokens()) {
				// String userWordtmp = userWordFrag.nextToken();
				StringTokenizer userWords = new StringTokenizer(
						userWordFrag.nextToken(), "\t");
				String user = userWords.nextToken();
				while (userWords.hasMoreTokens()) {
					StringTokenizer wordTFs = new StringTokenizer(
							userWords.nextToken(), ",");
					while (wordTFs.hasMoreTokens()) {
						StringTokenizer wordTF = new StringTokenizer(
								wordTFs.nextToken());
						String word = wordTF.nextToken();
						String tf = wordTF.nextToken();
						Text outputKey = new Text();
						Text outputValue = new Text();
						outputKey.set(word);
						outputValue.set(user + "\t" + tf);
						context.write(outputKey, outputValue);
					}
				}
			}

		}
	}

	public static class IDFReduce extends Reducer&lt;Text, Text, Text, Text&gt; {
		long userCount = 0;

		public void setup(Context context) throws IOException {
			Configuration conf = context.getConfiguration();
			Path path = new Path(fileURL);
			FileSystem fs = FileSystem.get(URI.create(hdfsURL), conf);
			if (!fs.isFile(path)) {
				FSDataOutputStream output = fs.create(path, true);
				output.close();
			}
			FSDataInputStream input = fs.open(path);
			StringBuffer sb = new StringBuffer();
			byte[] bytes = new byte[1024];
			int status = input.read(bytes);
			while (status != -1) {
				sb.append(new String(bytes));
				status = input.read(bytes);
			}
			if (!"".equals(sb.toString())) {
				userCount = Long.parseLong(sb.toString().trim());
			}
			input.close();
		}

		public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				throws IOException, InterruptedException {
			LinkedList&lt;String&gt; userList = new LinkedList&lt;String&gt;();
			Iterator&lt;Text&gt; iter = values.iterator();
			long wordCount = 0;
			while (iter.hasNext()) {
				wordCount++;
				userList.add(iter.next().toString());
			}
			float idf = (float) Math.log((float) userCount
					/ (float) (wordCount + 1));
			Iterator&lt;String&gt; userIter = userList.iterator();
			Text outputValue = new Text();
			while (userIter.hasNext()) {
				String usertftmp = userIter.next();
				StringTokenizer usertf = new StringTokenizer(usertftmp, "\t");
				String user = usertf.nextToken();
				String tfStr = usertf.nextToken();
				float tf = Float.parseFloat(tfStr.trim().toString());
				float tfidf = tf * idf;
				String outputTmp = user + "\t" + tfidf;
				outputValue.set(outputTmp);
				context.write(key, outputValue);
			}
		}
	}

	public static class UserCountMap extends Mapper&lt;Object, Text, Text, Text&gt; {

		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {
			String userWordtmp = value.toString();
			StringTokenizer userWord = new StringTokenizer(userWordtmp, "\n");
			while (userWord.hasMoreTokens()) {
				userWord.nextToken();
				Text outputKey = new Text();
				outputKey.set("usercount");
				Text one = new Text();
				one.set("1");
				context.write(outputKey, one);
			}
		}
	}

	public static class UserCountCombine extends
			Reducer&lt;Text, Text, Text, Text&gt; {
		public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				throws IOException, InterruptedException {
			long user = 0;
			for (Text value : values) {
				String valueTmp = value.toString();
				user += Long.parseLong(valueTmp);
			}
			Text outputValue = new Text();
			outputValue.set(Long.toString(user));
			context.write(key, outputValue);
		}
	}

	public static class UserCountReduce extends Reducer&lt;Text, Text, Text, Text&gt; {
		int userCount = 0;

		public void reduce(Text key, Iterable&lt;Text&gt; values, Context context)
				throws IOException, InterruptedException {
			for (Text value : values) {
				String valueTmp = value.toString();
				userCount += Long.parseLong(valueTmp);
			}
		}

		public void cleanup(Context context) throws IOException {
			Configuration conf = context.getConfiguration();
			FileSystem fs = FileSystem.get(URI.create(hdfsURL), conf);
			Path path = new Path(fileURL);
			FSDataOutputStream output = fs.create(path, true);
			String content = Long.toString(userCount);
			output.write(content.getBytes());
			output.flush();
			output.close();
		}
	}

	public static void main(String[] args) throws IOException,
			ClassNotFoundException, InterruptedException {
		// TODO Auto-generated method stub
		Configuration conf = new Configuration();
		// conf.set("mapred.child.java.opts", "-Xmx4096m");
		Job tfJob = Job.getInstance(conf, "tfjob");
		tfJob.setJarByClass(TFIDF_5.class);
		tfJob.setMapperClass(TFMap.class);
		// tfJob.setCombinerClass(TFCombine.class);
		tfJob.setReducerClass(TFReduce.class);
		tfJob.setOutputKeyClass(Text.class);
		tfJob.setOutputValueClass(Text.class);
		FileInputFormat.setInputPaths(tfJob, new Path(args[0]));
		FileOutputFormat.setOutputPath(tfJob, new Path(args[1]));
		tfJob.waitForCompletion(true);

		// Job userCountJob = Job.getInstance(conf, "usercountjob");
		// userCountJob.setJarByClass(TFIDF_5.class);
		// userCountJob.setMapperClass(UserCountMap.class);
		// userCountJob.setCombinerClass(UserCountCombine.class);
		// userCountJob.setReducerClass(UserCountReduce.class);
		// userCountJob.setOutputKeyClass(Text.class);
		// userCountJob.setOutputValueClass(Text.class);
		// FileInputFormat.setInputPaths(userCountJob, new Path(args[1]));
		// FileOutputFormat.setOutputPath(userCountJob, new Path(args[2]));
		// userCountJob.waitForCompletion(true);</code></pre> 
   <pre><code class="language-java">&lt;span style="white-space: pre;"&gt;		&lt;/span&gt;//计算文档数，并暂时储存到hdfs上</code></pre> 
   <pre><code class="language-java">		Counter ct = tfJob.getCounters().findCounter(
				"org.apache.hadoop.mapreduce.TaskCounter", "MAP_INPUT_RECORDS");
		System.out.println(ct.getValue());
		Iterable&lt;String&gt; groupNames = tfJob.getCounters().getGroupNames();
		for (String groupName : groupNames) {
			System.out.println(groupName);
		}
		FileSystem fs = FileSystem.get(URI.create(hdfsURL), conf);
		Path path = new Path(fileURL);
		FSDataOutputStream output = fs.create(path, true);
		String content = Long.toString(ct.getValue());
		output.write(content.getBytes());
		output.flush();
		output.close();

		Job idfJob = Job.getInstance(conf, "idfjob");
		idfJob.setJarByClass(TFIDF_5.class);
		idfJob.setMapperClass(IDFMap.class);
		idfJob.setReducerClass(IDFReduce.class);
		idfJob.setOutputKeyClass(Text.class);
		idfJob.setOutputValueClass(Text.class);
		FileInputFormat.setInputPaths(idfJob, new Path(args[1]));
		FileOutputFormat.setOutputPath(idfJob, new Path(args[3]));
		System.exit(idfJob.waitForCompletion(true) ? 0 : 1);

	}

}
</code></pre> 
   <br style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;">
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;"></p> 
   <p style="color:rgb(51,51,51);font-family:verdana, Arial, Helvetica, sans-serif;font-size:14px;">最初运用了一个单独的job计算文档数，后面经过公司前辈的指点，能够通过计算tf的时候运用输入数据的条数来巧妙的计算文档数。</p> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div>
    <br>
   </div> 
   <div> 
    <div>
     <br>
    </div> 
    <div>
     本文转自mfrbuaa博客园博客，原文链接：http://www.cnblogs.com/mfrbuaa/p/5127332.html，如需转载请自行联系原作者
    </div> 
   </div> 
   <div>
    <br>
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
