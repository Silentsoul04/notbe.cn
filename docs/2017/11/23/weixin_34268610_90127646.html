<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>用scikit-learn学习主成分分析(PCA) « NotBeCN</title>
  <meta name="description" content="             1. scikit-learn PCA类介绍    　　　　在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类就是sklearn.decomposition.PCA，我们下面主要也会讲解基于这个类的使用的方法。    　　　...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/23/weixin_34268610_90127646.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">用scikit-learn学习主成分分析(PCA)</h1>
    <p class="post-meta">Nov 23, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">1. scikit-learn PCA类介绍</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类就是sklearn.decomposition.PCA，我们下面主要也会讲解基于这个类的使用的方法。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　除了PCA类以外，最常用的PCA相关类还有KernelPCA类，在原理篇我们也讲到了，它主要用于非线性数据的降维，需要用到核技巧。因此在使用的时候需要选择合适的核函数并对核函数的参数进行调参。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　另外一个常用的PCA相关类是IncrementalPCA类，它主要是为了解决单机内存限制的。有时候我们的样本量可能是上百万+，维度可能也是上千，直接去拟合数据可能会让内存爆掉， 此时我们可以用IncrementalPCA类来解决这个问题。IncrementalPCA先将数据分成多个batch，然后对每个batch依次递增调用partial_fit函数，这样一步步的得到最终的样本最优降维。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　此外还有SparsePCA和MiniBatchSparsePCA。他们和上面讲到的PCA类的区别主要是使用了L1的正则化，这样可以将很多非主要成分的影响度降为0，这样在PCA降维的时候我们仅仅需要对那些相对比较主要的成分进行PCA降维，避免了一些噪声之类的因素对我们PCA降维的影响。SparsePCA和MiniBatchSparsePCA之间的区别则是MiniBatchSparsePCA通过使用一部分样本特征和给定的迭代次数来进行PCA降维，以解决在大样本时特征分解过慢的问题，当然，代价就是PCA降维的精确度可能会降低。使用SparsePCA和MiniBatchSparsePCA需要对L1正则化参数进行调参。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">2. sklearn.decomposition.PCA参数介绍</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　下面我们主要基于sklearn.decomposition.PCA来讲解如何使用scikit-learn进行PCA降维。PCA类基本不需要调参，一般来说，我们只需要指定我们需要降维到的维度，或者我们希望降维后的主成分的方差和占原始维度所有特征方差和的比例阈值就可以了。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　现在我们对sklearn.decomposition.PCA的主要参数做一个介绍：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　1）<strong>n_components</strong>：这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。当然，我们还可以将参数设置为"mle", 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　2）<strong>whiten</strong>&nbsp;：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　3）<strong>svd_solver</strong>：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。&nbsp;full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　除了这些输入参数外，有两个PCA类的成员值得关注。第一个是<strong>explained_variance_</strong>，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是<strong>explained_variance_ratio_</strong>，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">3. PCA实例</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　下面我们用一个实例来学习下scikit-learn中的PCA类使用。为了方便的可视化让大家有一个直观的认识，我们这里使用了三维的数据来降维。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　首先我们生成随机数据并可视化，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> numpy as np
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> matplotlib.pyplot as plt
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> mpl_toolkits.mplot3d <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> Axes3D
</span>%<span style="line-height:1.5;">matplotlib inline
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.datasets.samples_generator <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> make_blobs
</span><span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;"> X为样本特征，Y为样本簇类别， 共1000个样本，每个样本3个特征，共4个簇</span>
X, y = make_blobs(n_samples=10000, n_features=3, centers=[[3,3, 3], [0,0,0], [1,1,1], [2,2,2]], cluster_std=[0.2, 0.1, 0.2, 0.2<span style="line-height:1.5;">], 
                  random_state </span>=9<span style="line-height:1.5;">)
fig </span>=<span style="line-height:1.5;"> plt.figure()
ax </span>= Axes3D(fig, rect=[0, 0, 1, 1], elev=30, azim=20<span style="line-height:1.5;">)
plt.scatter(X[:, 0], X[:, </span>1], X[:, 2],marker=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">o</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>)</pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　三维数据的分布图如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170102203715206-1501576889.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　我们先不降维，只对数据进行投影，看看投影后的三个维度的方差分布，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.decomposition <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> PCA
pca </span>= PCA(n_components=3<span style="line-height:1.5;">)
pca.fit(X)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_ratio_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> pca.explained_variance_</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">[ 0.98318212&nbsp; 0.00850037&nbsp; 0.00831751]<br> [ 3.78483785&nbsp; 0.03272285&nbsp; 0.03201892]</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可以看出投影后三个特征维度的方差比例大约为98.3%：0.8%：0.8%。投影后第一个特征占了绝大多数的主成分比例。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　现在我们来进行降维，从三维降到2维，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>pca = PCA(n_components=2<span style="line-height:1.5;">)
pca.fit(X)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_ratio_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> pca.explained_variance_</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">[ 0.98318212&nbsp; 0.00850037]<br> [ 3.78483785&nbsp; 0.03272285]</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　这个结果其实可以预料，因为上面三个投影后的特征维度的方差分别为：[ 3.78483785&nbsp; 0.03272285&nbsp; 0.03201892]，投影到二维后选择的肯定是前两个特征，而抛弃第三个特征。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　为了有个直观的认识，我们看看此时转化后的数据分布，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>X_new =<span style="line-height:1.5;"> pca.transform(X)
plt.scatter(X_new[:, 0], X_new[:, </span>1],marker=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">o</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="line-height:1.5;">)
plt.show()</span></pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出的图如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170102204623581-74947365.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可见降维后的数据依然可以很清楚的看到我们之前三维图中的4个簇。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　现在我们看看不直接指定降维的维度，而指定降维后的主成分方差和比例。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>pca = PCA(n_components=0.95<span style="line-height:1.5;">)
pca.fit(X)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_ratio_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> pca.n_components_</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　我们指定了主成分至少占95%，输出如下：</p> 
   <div class="output" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"> 
    <div class="output_area"> 
     <div class="output_subarea output_text output_stream output_stdout">
      <pre>[ 0.98318212]
[ 3.78483785]
1</pre>
     </div> 
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可见只有第一个投影特征被保留。这也很好理解，我们的第一个主成分占投影特征的方差比例高达98%。只选择这一个特征维度便可以满足95%的阈值。我们现在选择阈值99%看看，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>pca = PCA(n_components=0.99<span style="line-height:1.5;">)
pca.fit(X)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_ratio_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> pca.n_components_</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　此时的输出如下：</p> 
   <pre>[ 0.98318212  0.00850037]
[ 3.78483785  0.03272285]
2
</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　这个结果也很好理解，因为我们第一个主成分占了98.3%的方差比例，第二个主成分占了0.8%的方差比例，两者一起可以满足我们的阈值。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　最后我们看看让MLE算法自己选择降维维度的效果，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>pca = PCA(n_components=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">mle</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="line-height:1.5;">)
pca.fit(X)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_ratio_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> pca.n_components_</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出结果如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">[ 0.98318212]<br> [ 3.78483785]<br> 1</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可见由于我们的数据的第一个投影特征的方差占比高达98.3%，MLE算法只保留了我们的第一个特征。</p> 
   <p><span style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">&nbsp;</span><font><span style="font-size:12px;">本文转自刘建平Pinard博客园博客，原文链接：http://www.cnblogs.com/pinard/p/6243025.html，如需转载请自行联系原作者</span></font></p> 
   <div>
    <br>
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
