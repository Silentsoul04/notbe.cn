<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>scikit-learn决策树算法类库使用小结 « NotBeCN</title>
  <meta name="description" content="             1.&nbsp;scikit-learn决策树算法类库介绍    　　　　scikit-learn决策树算法类库内部实现是使用了调优过的CART树算法，既可以做分类，又可以做回归。分类决策树的类对应的是DecisionTreeClassifier，而回归决策树的类对应的是Decision...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/23/weixin_34185512_90120025.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">scikit-learn决策树算法类库使用小结</h1>
    <p class="post-meta">Nov 23, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">1.&nbsp;scikit-learn决策树算法类库介绍</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　scikit-learn决策树算法类库内部实现是使用了调优过的CART树算法，既可以做分类，又可以做回归。分类决策树的类对应的是<span class="n">DecisionTreeClassifier</span>，而回归决策树的类对应的是<span class="n">DecisionTreeRegressor。两者的参数定义几乎完全相同，但是意义不全相同。下面就对<span class="n">DecisionTreeClassifier和<span class="n">DecisionTreeRegressor的重要参数做一个总结，重点比较两者参数使用的不同点和调参的注意点。</span></span></span></p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">2.&nbsp;<span class="n">DecisionTreeClassifier</span>和<span class="n">DecisionTreeClassifier</span>&nbsp;重要参数调参注意点</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　为了便于比较，这里我们用表格的形式对<span class="n">DecisionTreeClassifier和<span class="n">DecisionTreeRegressor</span><span class="n">重要参数要点做一个比较。</span></span></p> 
   <table border="0" style="border-collapse:collapse;border-spacing:0px;border:1px solid #C0C0C0;font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">
    <tbody>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;">参数</td> 
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"><span class="n">DecisionTreeClassifier</span></td> 
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"><span class="n">DecisionTreeRegressor</span></td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>特征选择标准criterion</p> </td> 
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>可以使用"gini"或者"entropy"，前者代表基尼系数，后者代表信息增益。一般说使用默认的基尼系数"gini"就可以了，即CART算法。除非你更喜欢类似ID3, C4.5的最优特征选择方法。&nbsp;</p> </td> 
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;">&nbsp;可以使用"mse"或者"mae"，前者是均方差，后者是和均值之差的绝对值之和。推荐使用默认的"mse"。一般来说"mse"比"mae"更加精确。除非你想比较二个参数的效果的不同之处。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>特征划分点选择标准splitter</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>可以使用"best"或者"random"。前者在特征的所有划分点中找出最优的划分点。后者是随机的在部分划分点中找局部最优的划分点。</p> <p>默认的"best"适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐"random"&nbsp;</p> </td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>划分时考虑的最大特征数max_features</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑<span class="MathJax" style="line-height:normal;word-spacing:normal;border:0px;"><span class="math" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-size:15.12px;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mrow" style="border:0px;vertical-align:0px;line-height:normal;"><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">l</span><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">o</span><span class="msubsup" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">g<span style="border:0px;vertical-align:0px;line-height:normal;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mn" style="border:0px;vertical-align:0px;line-height:normal;font-size:12px;font-family:'MathJax_Main';">2</span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span></span><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">N<span style="border:0px;vertical-align:0px;line-height:normal;"></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span><span style="border-width:0px;border-left-style:solid;vertical-align:-.329em;line-height:normal;width:0px;"></span></span><span class="MJX_Assistive_MathML" style="border:0px;vertical-align:0px;line-height:normal;width:1px;">log2N</span></span>个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑<span class="MathJax" style="line-height:normal;word-spacing:normal;border:0px;"><span class="math" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-size:15.12px;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mrow" style="border:0px;vertical-align:0px;line-height:normal;"><span class="msqrt" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mrow" style="border:0px;vertical-align:0px;line-height:normal;"><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">N<span style="border:0px;vertical-align:0px;line-height:normal;"></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Main';">−<span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Main';">−<span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Main';">√</span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span><span style="border-width:0px;border-left-style:solid;vertical-align:-.246em;line-height:normal;width:0px;"></span></span><span class="MJX_Assistive_MathML" style="border:0px;vertical-align:0px;line-height:normal;width:1px;">N</span></span>个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。</p> <p>一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p> </td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>决策树最大深max_depth</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">&nbsp;决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>内部节点再划分所需最小样本数min_samples_split</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。&nbsp;默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>叶子节点最少样本数min_samples_leaf</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">&nbsp;这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。&nbsp;默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>叶子节点最小的样本权重和min_weight_fraction_leaf</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。&nbsp;默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>最大叶子节点数max_leaf_nodes</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">&nbsp;通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>类别权重class_weight</p> </td> 
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;">指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，或者用“balanced”，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的"None"</td> 
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;">&nbsp;不适用于回归树</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>节点划分最小不纯度min_impurity_split</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">&nbsp;这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点&nbsp;。</td> 
     </tr>
     <tr>
      <td style="border:1px solid #C0C0C0;border-collapse:collapse;"> <p>数据是否预排序presort</p> </td> 
      <td colspan="2" style="border:1px solid #C0C0C0;border-collapse:collapse;">这个值是布尔值，默认是False不排序。一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。</td> 
     </tr>
    </tbody>
   </table>
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　除了这些参数要注意以外，其他在调参时的注意点有：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　1）当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　2）如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　3）推荐多用决策树的可视化（下节会讲），同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　4）在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　5）决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　6）如果输入的样本矩阵是稀疏的，推荐在拟合前调用<code class="docutils literal"><span class="pre">csc_matrix</span></code>稀疏化，在预测前调用<code class="docutils literal"><span class="pre">csr_matrix稀疏化。</span></code></p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">3. scikit-learn决策树结果的可视化　</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　决策树可视化化可以方便我们直观的观察模型，以及发现模型中的问题。这里介绍下scikit-learn中决策树的可视化方法。</p> 
   <h2 style="font-size:21px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">3.1 决策树可视化环境搭建</h2> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　scikit-learn中决策树的可视化一般需要安装graphviz。主要包括graphviz的安装和python的graphviz插件的安装。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　第一步是安装graphviz。下载地址在：http://www.graphviz.org/。如果你是linux，可以用apt-get或者yum的方法安装。如果是windows，就在官网下载msi文件安装。无论是linux还是windows，装完后都要设置环境变量，将graphviz的bin目录加到PATH，比如我是windows，将C:/Program Files (x86)/Graphviz2.38/bin/加入了PATH</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><span class="nn">　　　　第二步是安装python插件graphviz：&nbsp;pip install graphviz</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　第三步是安装python插件<span class="nn">pydotplus</span>。这个没有什么好说的: pip install&nbsp;<span class="nn">pydotplus</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><span class="nn">　　　　这样环境就搭好了，有时候python会很笨，仍然找不到graphviz，这时，可以在代码里面加入这一行：</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><span class="nn">　　　　os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><span class="nn">　　　　注意后面的路径是你自己的graphviz的bin目录。</span></p> 
   <h2 style="font-size:21px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;"><span class="nn">3.2 决策树可视化的三种方法</span></h2> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><span class="nn">　　　　这里我们有一个例子讲解决策树可视化。</span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><span class="nn">　　　　首先载入类库：</span></p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.datasets <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> load_iris
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> tree
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> sys
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> os       
os.environ[</span><span style="color:rgb(128,0,0);line-height:1.5;">"</span><span style="color:rgb(128,0,0);line-height:1.5;">PATH</span><span style="color:rgb(128,0,0);line-height:1.5;">"</span>] += os.pathsep + <span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">C:/Program Files (x86)/Graphviz2.38/bin/</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span></pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　接着载入sciki-learn的自带数据，有决策树拟合，得到模型：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>iris =<span style="line-height:1.5;"> load_iris()
clf </span>=<span style="line-height:1.5;"> tree.DecisionTreeClassifier()
clf </span>= clf.fit(iris.data, iris.target)</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　现在可以将模型存入dot文件iris.dot。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>with open(<span style="color:rgb(128,0,0);line-height:1.5;">"</span><span style="color:rgb(128,0,0);line-height:1.5;">iris.dot</span><span style="color:rgb(128,0,0);line-height:1.5;">"</span>, <span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">w</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="line-height:1.5;">) as f:
    f </span>= tree.export_graphviz(clf, out_file=f)</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　这时候我们有3种可视化方法，第一种是用graphviz的dot命令生成决策树的可视化文件，敲完这个命令后当前目录就可以看到决策树的可视化文件iris.pdf.打开可以看到决策树的模型图。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre><span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;">注意，这个命令在命令行执行</span>
dot -Tpdf iris.dot -o iris.pdf</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　第二种方法是用<code class="docutils literal"><span class="pre">pydotplus</span></code>生成iris.pdf。这样就不用再命令行去专门生成pdf文件了。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> pydotplus 
dot_data </span>= tree.export_graphviz(clf, out_file=<span style="line-height:1.5;">None) 
graph </span>=<span style="line-height:1.5;"> pydotplus.graph_from_dot_data(dot_data) 
graph.write_pdf(</span><span style="color:rgb(128,0,0);line-height:1.5;">"</span><span style="color:rgb(128,0,0);line-height:1.5;">iris.pdf</span><span style="color:rgb(128,0,0);line-height:1.5;">"</span>) </pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　第三种办法是个人比较推荐的做法，因为这样可以直接把图产生在ipython的notebook。代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> IPython.display <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> Image  
dot_data </span>= tree.export_graphviz(clf, out_file=<span style="line-height:1.5;">None, 
                         feature_names</span>=<span style="line-height:1.5;">iris.feature_names,  
                         class_names</span>=<span style="line-height:1.5;">iris.target_names,  
                         filled</span>=True, rounded=<span style="line-height:1.5;">True,  
                         special_characters</span>=<span style="line-height:1.5;">True)  
graph </span>=<span style="line-height:1.5;"> pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png()) </span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　在ipython的notebook生成的图如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161112134751389-1951028811.png" alt="" style="border:0px;">　　</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">4.&nbsp;<span class="n">DecisionTreeClassifier</span>实例</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　这里给一个限制决策树层数为4的<span class="n">DecisionTreeClassifier</span>例子。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> itertools <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> product

</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> numpy as np
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> matplotlib.pyplot as plt

</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> datasets
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.tree <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> DecisionTreeClassifier


</span><span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;"> 仍然使用自带的iris数据</span>
iris =<span style="line-height:1.5;"> datasets.load_iris()
X </span>= iris.data[:, [0, 2<span style="line-height:1.5;">]]
y </span>=<span style="line-height:1.5;"> iris.target

</span><span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;"> 训练模型，限制树的最大深度4</span>
clf = DecisionTreeClassifier(max_depth=4<span style="line-height:1.5;">)
</span><span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;">拟合模型</span>
<span style="line-height:1.5;">clf.fit(X, y)


</span><span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;"> 画图</span>
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1<span style="line-height:1.5;">
y_min, y_max </span>= X[:, 1].min() - 1, X[:, 1].max() + 1<span style="line-height:1.5;">
xx, yy </span>= np.meshgrid(np.arange(x_min, x_max, 0.1<span style="line-height:1.5;">),
                     np.arange(y_min, y_max, </span>0.1<span style="line-height:1.5;">))

Z </span>=<span style="line-height:1.5;"> clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z </span>=<span style="line-height:1.5;"> Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha</span>=0.4<span style="line-height:1.5;">)
plt.scatter(X[:, 0], X[:, </span>1], c=y, alpha=0.8<span style="line-height:1.5;">)
plt.show()</span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　得到的图如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161112142033999-5911597.png" alt="" style="border:0px;"><br> 接着我们可视化我们的决策树，使用了推荐的第三种方法。代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> IPython.display <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> Image  
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> tree
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> pydotplus 
dot_data </span>= tree.export_graphviz(clf, out_file=<span style="line-height:1.5;">None, 
                         feature_names</span>=<span style="line-height:1.5;">iris.feature_names,  
                         class_names</span>=<span style="line-height:1.5;">iris.target_names,  
                         filled</span>=True, rounded=<span style="line-height:1.5;">True,  
                         special_characters</span>=<span style="line-height:1.5;">True)  
graph </span>=<span style="line-height:1.5;"> pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png()) </span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　生成的决策树图如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">&nbsp;<img src="https://images2015.cnblogs.com/blog/1042406/201611/1042406-20161112142549952-2076035538.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　以上就是scikit-learn决策树算法使用的一个总结，希望可以帮到大家。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><br></p> 
   <p><font><span style="font-size:12px;">本文转自刘建平Pinard博客园博客，原文链接：http://www.cnblogs.com/pinard/p/6056319.html，如需转载请自行联系原作者</span></font></p> 
   <div>
    <br>
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
