<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>用scikit-learn进行LDA降维 « NotBeCN</title>
  <meta name="description" content="             1.&nbsp;对scikit-learn中LDA类概述    　　　　在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDiscriminantAnalysis。那既可以用于分类又可以用于降维。当然，应用场景最多的还是降维...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/23/weixin_33713350_90128159.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">用scikit-learn进行LDA降维</h1>
    <p class="post-meta">Nov 23, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">1.&nbsp;对scikit-learn中LDA类概述</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDiscriminantAnalysis。那既可以用于分类又可以用于降维。当然，应用场景最多的还是降维。和PCA类似，LDA降维基本也不用调参，只需要指定降维到的维数即可。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">2.&nbsp;LinearDiscriminantAnalysis类概述</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　我们这里对LinearDiscriminantAnalysis类的参数做一个基本的总结。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　1）<strong>solver</strong>&nbsp;: 即求LDA超平面特征矩阵使用的方法。可以选择的方法有奇异值分解"svd"，最小二乘"lsqr"和特征分解"eigen"。一般来说特征数非常多的时候推荐使用svd，而特征数不多的时候推荐使用eigen。主要注意的是，如果使用svd，则不能指定正则化参数<strong>shrinkage</strong>进行正则化。默认值是svd</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　2）<strong>shrinkage</strong>：正则化参数，可以增强LDA分类的泛化能力。如果仅仅只是为了降维，则一般可以忽略这个参数。默认是None，即不进行正则化。可以选择"auto",让算法自己决定是否正则化。当然我们也可以选择不同的[0,1]之间的值进行交叉验证调参。注意shrinkage只在solver为最小二乘"lsqr"和特征分解"eigen"时有效。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　3）<strong>priors</strong>&nbsp;：类别权重，可以在做分类模型时指定不同类别的权重，进而影响分类模型建立。降维时一般不需要关注这个参数。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　4）<strong>n_components</strong>：即我们进行LDA降维时降到的维数。在降维时需要输入这个参数。注意只能为[1,类别数-1)范围之间的整数。如果我们不是用于降维，则这个值可以用默认的None。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　从上面的描述可以看出，如果我们只是为了降维，则只需要输入n_components,注意这个值必须小于“类别数-1”。PCA没有这个限制。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">3.&nbsp;LinearDiscriminantAnalysis降维实例</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　在LDA的原理篇我们讲到，PCA和LDA都可以用于降维。两者没有绝对的优劣之分，使用两者的原则实际取决于数据的分布。由于LDA可以利用类别信息，因此某些时候比完全无监督的PCA会更好。下面我们举一个LDA降维可能更优的例子。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　我们首先生成三类三维特征的数据，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> numpy as np
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> matplotlib.pyplot as plt
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> mpl_toolkits.mplot3d <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> Axes3D
</span>%<span style="line-height:1.5;">matplotlib inline
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.datasets.samples_generator <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> make_classification
X, y </span>= make_classification(n_samples=1000, n_features=3, n_redundant=0, n_classes=3, n_informative=2<span style="line-height:1.5;">,
                           n_clusters_per_class</span>=1,class_sep =0.5, random_state =10<span style="line-height:1.5;">)
fig </span>=<span style="line-height:1.5;"> plt.figure()
ax </span>= Axes3D(fig, rect=[0, 0, 1, 1], elev=30, azim=20<span style="line-height:1.5;">)
plt.scatter(X[:, 0], X[:, </span>1], X[:, 2],marker=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">o</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,c=y)</pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　我们看看最初的三维数据的分布情况：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170104165723722-2134679474.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　首先我们看看使用PCA降维到二维的情况，注意PCA无法使用类别信息来降维，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.decomposition <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> PCA
pca </span>= PCA(n_components=2<span style="line-height:1.5;">)
pca.fit(X)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_ratio_
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> pca.explained_variance_
X_new </span>=<span style="line-height:1.5;"> pca.transform(X)
plt.scatter(X_new[:, 0], X_new[:, </span>1],marker=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">o</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,c=<span style="line-height:1.5;">y)
plt.show()</span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　在输出中，PCA找到的两个主成分方差比和方差如下：</p> 
   <pre>[ 0.43377069  0.3716351 ]
[ 1.20962365  1.03635081]
</pre> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出的降维效果图如下:</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170104165958503-852104019.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　由于PCA没有利用类别信息，我们可以看到降维后，样本特征和类别的信息关联几乎完全丢失。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　现在我们再看看使用LDA的效果，代码如下：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.discriminant_analysis <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> LinearDiscriminantAnalysis
lda </span>= LinearDiscriminantAnalysis(n_components=2<span style="line-height:1.5;">)
lda.fit(X,y)
X_new </span>=<span style="line-height:1.5;"> lda.transform(X)
plt.scatter(X_new[:, 0], X_new[:, </span>1],marker=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">o</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,c=<span style="line-height:1.5;">y)
plt.show()</span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出的效果图如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170104170133503-1519821205.png" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可以看出降维后样本特征和类别信息之间的关系得以保留。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　一般来说，如果我们的数据是有类别标签的，那么优先选择LDA去尝试降维；当然也可以使用PCA做很小幅度的降维去消去噪声，然后再使用LDA降维。如果没有类别标签，那么肯定PCA是最先考虑的一个选择了。</p> 
   <p><span style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">&nbsp;</span><font><span style="font-size:12px;">本文转自刘建平Pinard博客园博客，原文链接：http://www.cnblogs.com/pinard/p/6249328.html，如需转载请自行联系原作者</span></font></p> 
   <div>
    <br>
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
