<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>scikit-learn随机森林调参小结 « NotBeCN</title>
  <meta name="description" content="             1. scikit-learn随机森林类库概述    　　　　在scikit-learn中，RF的分类类是RandomForestClassifier，回归类是RandomForestRegressor。当然RF的变种Extra Trees也有， 分类类ExtraTreesClassif...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/16/weixin_33958366_90124770.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">scikit-learn随机森林调参小结</h1>
    <p class="post-meta">Nov 16, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">1. scikit-learn随机森林类库概述</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　在scikit-learn中，RF的分类类是RandomForestClassifier，回归类是RandomForestRegressor。当然RF的变种Extra Trees也有， 分类类ExtraTreesClassifier，回归类ExtraTreesRegressor。由于RF和Extra Trees的区别较小，调参方法基本相同，本文只关注于RF的调参。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　和GBDT的调参类似，RF需要调参的参数也包括两部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数。下面我们就对这些参数做一个介绍。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">2.&nbsp;&nbsp;RF框架参数</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　首先我们关注于RF的Bagging框架的参数。这里可以和GBDT对比来学习。在<a id="homepage1_HomePageDays_DaysList_ctl01_DayList_TitleUrl_0" class="postTitle2" href="http://www.cnblogs.com/pinard/p/6143927.html" rel="nofollow" style="color:rgb(0,0,0);">scikit-learn 梯度提升树(GBDT)调参小结</a>中我们对GBDT的框架参数做了介绍。GBDT的框架参数比较多，重要的有最大迭代器个数，步长和子采样比例，调参起来比较费力。但是RF则比较简单，这是因为bagging框架里的各个弱学习器之间是没有依赖关系的，这减小的调参的难度。换句话说，达到同样的调参效果，RF调参时间要比GBDT少一些。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　下面我来看看RF重要的Bagging框架的参数，由于RandomForestClassifier和RandomForestRegressor参数绝大部分相同，这里会将它们一起讲，不同点会指出。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　1)&nbsp;<strong>n_estimators</strong>: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，计算量会太大，并且n_estimators到一定的数量后，再增大n_estimators获得的模型提升会很小，所以一般选择一个适中的数值。默认是100。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　2)&nbsp;<strong>oob_score</strong>&nbsp;:即是否采用袋外样本来评估模型的好坏。默认识False。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　3)&nbsp;<strong>criterion</strong><strong>:&nbsp;</strong>即CART树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益。回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　从上面可以看出， RF重要的框架参数比较少，主要需要关注的是&nbsp;n_estimators，即RF最大的决策树个数。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">3.&nbsp; RF决策树参数</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　下面我们再来看RF的决策树参数，它要调参的参数基本和GBDT相同，如下:</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　1)&nbsp;RF划分时考虑的最大特征数<strong>max_features</strong>:&nbsp;可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑<span class="MathJax" style="line-height:normal;word-spacing:normal;border:0px;"><span class="math" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-size:15.12px;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mrow" style="border:0px;vertical-align:0px;line-height:normal;"><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">l</span><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">o</span><span class="msubsup" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">g<span style="border:0px;vertical-align:0px;line-height:normal;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mn" style="border:0px;vertical-align:0px;line-height:normal;font-size:12px;font-family:'MathJax_Main';">2</span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span></span><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">N<span style="border:0px;vertical-align:0px;line-height:normal;"></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span><span style="border-width:0px;border-left-style:solid;vertical-align:-.329em;line-height:normal;width:0px;"></span></span><span class="MJX_Assistive_MathML" style="border:0px;vertical-align:0px;line-height:normal;width:1px;">log2N</span></span>个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑<span class="MathJax" style="line-height:normal;word-spacing:normal;border:0px;"><span class="math" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-size:15.12px;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mrow" style="border:0px;vertical-align:0px;line-height:normal;"><span class="msqrt" style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span class="mrow" style="border:0px;vertical-align:0px;line-height:normal;"><span class="mi" style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Math-italic';">N<span style="border:0px;vertical-align:0px;line-height:normal;"></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Main';">−<span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Main';">−<span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span><span style="border:0px;vertical-align:0px;line-height:normal;"><span style="border:0px;vertical-align:0px;line-height:normal;font-family:'MathJax_Main';">√</span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span></span></span><span style="border:0px;vertical-align:0px;line-height:normal;width:0px;"></span></span></span><span style="border-width:0px;border-left-style:solid;vertical-align:-.246em;line-height:normal;width:0px;"></span></span><span class="MJX_Assistive_MathML" style="border:0px;vertical-align:0px;line-height:normal;width:1px;">N</span></span>个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　2)&nbsp;决策树最大深度<strong>max_depth</strong>:&nbsp;默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　3)&nbsp;内部节点再划分所需最小样本数<strong>min_samples_split</strong>:&nbsp;这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。&nbsp;默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　4)&nbsp;叶子节点最少样本数<strong>min_samples_leaf</strong>:&nbsp;这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。&nbsp;默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　5）叶子节点最小的样本权重和<strong>min_weight_fraction_leaf</strong>：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。&nbsp;默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　6)&nbsp;最大叶子节点数<strong>max_leaf_nodes</strong>:&nbsp;通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　7)&nbsp;节点划分最小不纯度<strong>min_impurity_split:&nbsp;</strong>&nbsp;这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点&nbsp;。一般不推荐改动默认值1e-7。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　上面决策树参数中最重要的包括最大特征数max_features， 最大深度max_depth， 内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。</p> 
   <h1 style="font-size:28px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;">4.RF调参实例</h1> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　这里仍然使用GBDT调参时同样的数据集来做RF调参的实例，数据的<a title="下载地址在这" href="http://files.cnblogs.com/files/pinard/train_modified.zip" rel="nofollow" style="color:rgb(0,0,0);">下载地址在这</a>。本例我们采用袋外分数来评估我们模型的好坏。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　首先，我们载入需要的类库：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> pandas as pd
</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> numpy as np
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.ensemble <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> RandomForestClassifier
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn.grid_search <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> GridSearchCV
</span><span style="color:rgb(0,0,255);line-height:1.5;">from</span> sklearn <span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> cross_validation, metrics

</span><span style="color:rgb(0,0,255);line-height:1.5;">import</span><span style="line-height:1.5;"> matplotlib.pylab as plt
</span>%matplotlib inline</pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　接着，我们把解压的数据用下面的代码载入，顺便看看数据的类别分布。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>train = pd.read_csv(<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">train_modified.csv</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="line-height:1.5;">)
target</span>=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">Disbursed</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span> <span style="color:rgb(0,128,0);line-height:1.5;">#</span><span style="color:rgb(0,128,0);line-height:1.5;"> Disbursed的值就是二元分类的输出</span>
IDcol = <span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">ID</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="line-height:1.5;">
train[</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">Disbursed</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>].value_counts() </pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可以看到类别输出如下，也就是类别0的占大多数。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">0&nbsp;&nbsp;&nbsp; 19680<br> 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 320<br> Name: Disbursed, dtype: int64</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　接着我们选择好样本特征和类别输出。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>x_columns = [x <span style="color:rgb(0,0,255);line-height:1.5;">for</span> x <span style="color:rgb(0,0,255);line-height:1.5;">in</span> train.columns <span style="color:rgb(0,0,255);line-height:1.5;">if</span> x <span style="color:rgb(0,0,255);line-height:1.5;">not</span> <span style="color:rgb(0,0,255);line-height:1.5;">in</span><span style="line-height:1.5;"> [target, IDcol]]
X </span>=<span style="line-height:1.5;"> train[x_columns]
y </span>= train[<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">Disbursed</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>]</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　不管任何参数，都用默认的，我们拟合下数据看看：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>rf0 = RandomForestClassifier(oob_score=True, random_state=10<span style="line-height:1.5;">)
rf0.fit(X,y)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span><span style="line-height:1.5;"> rf0.oob_score_
y_predprob </span>= rf0.predict_proba(X)[:,1<span style="line-height:1.5;">]
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> <span style="color:rgb(128,0,0);line-height:1.5;">"</span><span style="color:rgb(128,0,0);line-height:1.5;">AUC Score (Train): %f</span><span style="color:rgb(128,0,0);line-height:1.5;">"</span> % metrics.roc_auc_score(y, y_predprob)</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出如下，可见袋外分数已经很高，而且AUC分数也很高。相对于GBDT的默认参数输出，RF的默认参数拟合效果对本例要好一些。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">0.98005<br> AUC Score (Train): 0.999833</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">&nbsp;　　　　我们首先对n_estimators进行网格搜索：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre>param_test1 = {<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">n_estimators</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>:range(10,71,10<span style="line-height:1.5;">)}
gsearch1 </span>= GridSearchCV(estimator = RandomForestClassifier(min_samples_split=100<span style="line-height:1.5;">,
                                  min_samples_leaf</span>=20,max_depth=8,max_features=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">sqrt</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span> ,random_state=10<span style="line-height:1.5;">), 
                       param_grid </span>= param_test1, scoring=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">roc_auc</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,cv=5<span style="line-height:1.5;">)
gsearch1.fit(X,y)
gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_</span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出结果如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">([mean: 0.80681, std: 0.02236, params: {'n_estimators': 10},<br> &nbsp; mean: 0.81600, std: 0.03275, params: {'n_estimators': 20},<br> &nbsp; mean: 0.81818, std: 0.03136, params: {'n_estimators': 30},<br> &nbsp; mean: 0.81838, std: 0.03118, params: {'n_estimators': 40},<br> &nbsp; mean: 0.82034, std: 0.03001, params: {'n_estimators': 50},<br> &nbsp; mean: 0.82113, std: 0.02966, params: {'n_estimators': 60},<br> &nbsp; mean: 0.81992, std: 0.02836, params: {'n_estimators': 70}],<br> {'n_estimators': 60},<br> 0.8211334476626017)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　这样我们得到了最佳的弱学习器迭代次数，接着我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';"> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
    <pre>param_test2 = {<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">max_depth</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>:range(3,14,2), <span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">min_samples_split</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>:range(50,201,20<span style="line-height:1.5;">)}
gsearch2 </span>= GridSearchCV(estimator = RandomForestClassifier(n_estimators= 60<span style="line-height:1.5;">, 
                                  min_samples_leaf</span>=20,max_features=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">sqrt</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span> ,oob_score=True, random_state=10<span style="line-height:1.5;">),
   param_grid </span>= param_test2, scoring=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">roc_auc</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,iid=False, cv=5<span style="line-height:1.5;">)
gsearch2.fit(X,y)
gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_</span></pre> 
    <div class="cnblogs_code_toolbar">
     <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
    </div> 
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出如下：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">([mean: 0.79379, std: 0.02347, params: {'min_samples_split': 50, 'max_depth': 3},<br> &nbsp; mean: 0.79339, std: 0.02410, params: {'min_samples_split': 70, 'max_depth': 3},<br> &nbsp; mean: 0.79350, std: 0.02462, params: {'min_samples_split': 90, 'max_depth': 3},<br> &nbsp; mean: 0.79367, std: 0.02493, params: {'min_samples_split': 110, 'max_depth': 3},<br> &nbsp; mean: 0.79387, std: 0.02521, params: {'min_samples_split': 130, 'max_depth': 3},<br> &nbsp; mean: 0.79373, std: 0.02524, params: {'min_samples_split': 150, 'max_depth': 3},<br> &nbsp; mean: 0.79378, std: 0.02532, params: {'min_samples_split': 170, 'max_depth': 3},<br> &nbsp; mean: 0.79349, std: 0.02542, params: {'min_samples_split': 190, 'max_depth': 3},<br> &nbsp; mean: 0.80960, std: 0.02602, params: {'min_samples_split': 50, 'max_depth': 5},<br> &nbsp; mean: 0.80920, std: 0.02629, params: {'min_samples_split': 70, 'max_depth': 5},<br> &nbsp; mean: 0.80888, std: 0.02522, params: {'min_samples_split': 90, 'max_depth': 5},<br> &nbsp; mean: 0.80923, std: 0.02777, params: {'min_samples_split': 110, 'max_depth': 5},<br> &nbsp; mean: 0.80823, std: 0.02634, params: {'min_samples_split': 130, 'max_depth': 5},<br> &nbsp; mean: 0.80801, std: 0.02637, params: {'min_samples_split': 150, 'max_depth': 5},<br> &nbsp; mean: 0.80792, std: 0.02685, params: {'min_samples_split': 170, 'max_depth': 5},<br> &nbsp; mean: 0.80771, std: 0.02587, params: {'min_samples_split': 190, 'max_depth': 5},<br> &nbsp; mean: 0.81688, std: 0.02996, params: {'min_samples_split': 50, 'max_depth': 7},<br> &nbsp; mean: 0.81872, std: 0.02584, params: {'min_samples_split': 70, 'max_depth': 7},<br> &nbsp; mean: 0.81501, std: 0.02857, params: {'min_samples_split': 90, 'max_depth': 7},<br> &nbsp; mean: 0.81476, std: 0.02552, params: {'min_samples_split': 110, 'max_depth': 7},<br> &nbsp; mean: 0.81557, std: 0.02791, params: {'min_samples_split': 130, 'max_depth': 7},<br> &nbsp; mean: 0.81459, std: 0.02905, params: {'min_samples_split': 150, 'max_depth': 7},<br> &nbsp; mean: 0.81601, std: 0.02808, params: {'min_samples_split': 170, 'max_depth': 7},<br> &nbsp; mean: 0.81704, std: 0.02757, params: {'min_samples_split': 190, 'max_depth': 7},<br> &nbsp; mean: 0.82090, std: 0.02665, params: {'min_samples_split': 50, 'max_depth': 9},<br> &nbsp; mean: 0.81908, std: 0.02527, params: {'min_samples_split': 70, 'max_depth': 9},<br> &nbsp; mean: 0.82036, std: 0.02422, params: {'min_samples_split': 90, 'max_depth': 9},<br> &nbsp; mean: 0.81889, std: 0.02927, params: {'min_samples_split': 110, 'max_depth': 9},<br> &nbsp; mean: 0.81991, std: 0.02868, params: {'min_samples_split': 130, 'max_depth': 9},<br> &nbsp; mean: 0.81788, std: 0.02436, params: {'min_samples_split': 150, 'max_depth': 9},<br> &nbsp; mean: 0.81898, std: 0.02588, params: {'min_samples_split': 170, 'max_depth': 9},<br> &nbsp; mean: 0.81746, std: 0.02716, params: {'min_samples_split': 190, 'max_depth': 9},<br> &nbsp; mean: 0.82395, std: 0.02454, params: {'min_samples_split': 50, 'max_depth': 11},<br> &nbsp; mean: 0.82380, std: 0.02258, params: {'min_samples_split': 70, 'max_depth': 11},<br> &nbsp; mean: 0.81953, std: 0.02552, params: {'min_samples_split': 90, 'max_depth': 11},<br> &nbsp; mean: 0.82254, std: 0.02366, params: {'min_samples_split': 110, 'max_depth': 11},<br> &nbsp; mean: 0.81950, std: 0.02768, params: {'min_samples_split': 130, 'max_depth': 11},<br> &nbsp; mean: 0.81887, std: 0.02636, params: {'min_samples_split': 150, 'max_depth': 11},<br> &nbsp; mean: 0.81910, std: 0.02734, params: {'min_samples_split': 170, 'max_depth': 11},<br> &nbsp; mean: 0.81564, std: 0.02622, params: {'min_samples_split': 190, 'max_depth': 11},<br> &nbsp; mean: 0.82291, std: 0.02092, params: {'min_samples_split': 50, 'max_depth': 13},<br> &nbsp; mean: 0.82177, std: 0.02513, params: {'min_samples_split': 70, 'max_depth': 13},<br> &nbsp; mean: 0.82415, std: 0.02480, params: {'min_samples_split': 90, 'max_depth': 13},<br> &nbsp; mean: 0.82420, std: 0.02417, params: {'min_samples_split': 110, 'max_depth': 13},<br> &nbsp; mean: 0.82209, std: 0.02481, params: {'min_samples_split': 130, 'max_depth': 13},<br> &nbsp; mean: 0.81852, std: 0.02227, params: {'min_samples_split': 150, 'max_depth': 13},<br> &nbsp; mean: 0.81955, std: 0.02885, params: {'min_samples_split': 170, 'max_depth': 13},<br> &nbsp; mean: 0.82092, std: 0.02600, params: {'min_samples_split': 190, 'max_depth': 13}],<br> {'max_depth': 13, 'min_samples_split': 110},<br> 0.8242016800050813)</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　我们看看我们现在模型的袋外分数：</p> 
   <div class="cnblogs_code" style="border:1px solid rgb(204,204,204);font-size:12px;font-family:'Courier New';">
    <pre>rf1 = RandomForestClassifier(n_estimators= 60, max_depth=13, min_samples_split=110<span style="line-height:1.5;">,
                                  min_samples_leaf</span>=20,max_features=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">sqrt</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span> ,oob_score=True, random_state=10<span style="line-height:1.5;">)
rf1.fit(X,y)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> rf1.oob_score_</pre>
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出结果为：</p> 
   <div> 
    <div> 
     <div> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">0.984</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可见此时我们的袋外分数有一定的提高。也就是时候模型的泛化能力增强了。</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　对于内部节点再划分所需最小样本数min_samples_split，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参。</p> 
      <div class="cnblogs_code" style="font-family:'Courier New';font-size:12px;border:1px solid rgb(204,204,204);"> 
       <div class="cnblogs_code_toolbar">
        <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
       </div> 
       <pre>param_test3 = {<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">min_samples_split</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>:range(80,150,20), <span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">min_samples_leaf</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>:range(10,60,10<span style="line-height:1.5;">)}
gsearch3 </span>= GridSearchCV(estimator = RandomForestClassifier(n_estimators= 60, max_depth=13<span style="line-height:1.5;">,
                                  max_features</span>=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">sqrt</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span> ,oob_score=True, random_state=10<span style="line-height:1.5;">),
   param_grid </span>= param_test3, scoring=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">roc_auc</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,iid=False, cv=5<span style="line-height:1.5;">)
gsearch3.fit(X,y)
gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_</span></pre> 
       <div class="cnblogs_code_toolbar">
        <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
       </div> 
      </div> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出如下：</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">([mean: 0.82093, std: 0.02287, params: {'min_samples_split': 80, 'min_samples_leaf': 10},<br> &nbsp; mean: 0.81913, std: 0.02141, params: {'min_samples_split': 100, 'min_samples_leaf': 10},<br> &nbsp; mean: 0.82048, std: 0.02328, params: {'min_samples_split': 120, 'min_samples_leaf': 10},<br> &nbsp; mean: 0.81798, std: 0.02099, params: {'min_samples_split': 140, 'min_samples_leaf': 10},<br> &nbsp; mean: 0.82094, std: 0.02535, params: {'min_samples_split': 80, 'min_samples_leaf': 20},<br> &nbsp; mean: 0.82097, std: 0.02327, params: {'min_samples_split': 100, 'min_samples_leaf': 20},<br> &nbsp; mean: 0.82487, std: 0.02110, params: {'min_samples_split': 120, 'min_samples_leaf': 20},<br> &nbsp; mean: 0.82169, std: 0.02406, params: {'min_samples_split': 140, 'min_samples_leaf': 20},<br> &nbsp; mean: 0.82352, std: 0.02271, params: {'min_samples_split': 80, 'min_samples_leaf': 30},<br> &nbsp; mean: 0.82164, std: 0.02381, params: {'min_samples_split': 100, 'min_samples_leaf': 30},<br> &nbsp; mean: 0.82070, std: 0.02528, params: {'min_samples_split': 120, 'min_samples_leaf': 30},<br> &nbsp; mean: 0.82141, std: 0.02508, params: {'min_samples_split': 140, 'min_samples_leaf': 30},<br> &nbsp; mean: 0.82278, std: 0.02294, params: {'min_samples_split': 80, 'min_samples_leaf': 40},<br> &nbsp; mean: 0.82141, std: 0.02547, params: {'min_samples_split': 100, 'min_samples_leaf': 40},<br> &nbsp; mean: 0.82043, std: 0.02724, params: {'min_samples_split': 120, 'min_samples_leaf': 40},<br> &nbsp; mean: 0.82162, std: 0.02348, params: {'min_samples_split': 140, 'min_samples_leaf': 40},<br> &nbsp; mean: 0.82225, std: 0.02431, params: {'min_samples_split': 80, 'min_samples_leaf': 50},<br> &nbsp; mean: 0.82225, std: 0.02431, params: {'min_samples_split': 100, 'min_samples_leaf': 50},<br> &nbsp; mean: 0.81890, std: 0.02458, params: {'min_samples_split': 120, 'min_samples_leaf': 50},<br> &nbsp; mean: 0.81917, std: 0.02528, params: {'min_samples_split': 140, 'min_samples_leaf': 50}],<br> {'min_samples_leaf': 20, 'min_samples_split': 120},<br> 0.8248650279471544)</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　最后我们再对最大特征数max_features做调参:</p> 
      <div class="cnblogs_code" style="font-family:'Courier New';font-size:12px;border:1px solid rgb(204,204,204);"> 
       <div class="cnblogs_code_toolbar">
        <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
       </div> 
       <pre>param_test4 = {<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">max_features</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>:range(3,11,2<span style="line-height:1.5;">)}
gsearch4 </span>= GridSearchCV(estimator = RandomForestClassifier(n_estimators= 60, max_depth=13, min_samples_split=120<span style="line-height:1.5;">,
                                  min_samples_leaf</span>=20 ,oob_score=True, random_state=10<span style="line-height:1.5;">),
   param_grid </span>= param_test4, scoring=<span style="color:rgb(128,0,0);line-height:1.5;">'</span><span style="color:rgb(128,0,0);line-height:1.5;">roc_auc</span><span style="color:rgb(128,0,0);line-height:1.5;">'</span>,iid=False, cv=5<span style="line-height:1.5;">)
gsearch4.fit(X,y)
gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_</span></pre> 
       <div class="cnblogs_code_toolbar">
        <span class="cnblogs_code_copy" style="line-height:1.5;"><a title="复制代码" style="text-decoration:underline;border:none;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码" style="border:none;"></a></span>
       </div> 
      </div> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　输出如下：</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">([mean: 0.81981, std: 0.02586, params: {'max_features': 3},<br> &nbsp; mean: 0.81639, std: 0.02533, params: {'max_features': 5},<br> &nbsp; mean: 0.82487, std: 0.02110, params: {'max_features': 7},<br> &nbsp; mean: 0.81704, std: 0.02209, params: {'max_features': 9}],<br> {'max_features': 7},<br> 0.8248650279471544)</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　用我们搜索到的最佳参数，我们再看看最终的模型拟合：</p> 
      <div class="cnblogs_code" style="font-family:'Courier New';font-size:12px;border:1px solid rgb(204,204,204);">
       <pre>rf2 = RandomForestClassifier(n_estimators= 60, max_depth=13, min_samples_split=120<span style="line-height:1.5;">,
                                  min_samples_leaf</span>=20,max_features=7 ,oob_score=True, random_state=10<span style="line-height:1.5;">)
rf2.fit(X,y)
</span><span style="color:rgb(0,0,255);line-height:1.5;">print</span> rf2.oob_score_</pre>
      </div> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　此时的输出为：</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">0.984</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">　　　　可见此时模型的袋外分数基本没有提高，主要原因是0.984已经是一个很高的袋外分数了，如果想进一步需要提高模型的泛化能力，我们需要更多的数据。</p> 
      <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;"><br></p> 
      <p><font><span style="font-size:12px;">本文转自刘建平Pinard博客园博客，原文链接：http://www.cnblogs.com/pinard/p/6160412.html，如需转载请自行联系原作者</span></font></p> 
      <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:12px;">
       <br>
      </div> 
     </div> 
    </div> 
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
