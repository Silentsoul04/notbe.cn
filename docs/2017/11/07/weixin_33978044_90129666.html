<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>深度学习笔记之神经网络、激活函数、目标函数和深度的初步认识 « NotBeCN</title>
  <meta name="description" content="             　AI技术研究的兴起，伴随着两种最直观的思维技巧，即遗传算法与神经网络，这是对生物学研究最直观的技术抽象。深度学习的前身就是神经网络，这个80年代灵光乍现的技术，在那一波人工智能的大潮驱使下，带着人们对于未来AI时代的憧憬，迅速蔓延，一时风头无两，和今天深度学习的火热几乎如出一辙。   ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/07/weixin_33978044_90129666.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">深度学习笔记之神经网络、激活函数、目标函数和深度的初步认识</h1>
    <p class="post-meta">Nov 7, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　AI技术研究的兴起，伴随着两种最直观的思维技巧，即遗传<a class="replace_word" title="算法与数据结构知识库" href="http://lib.csdn.net/base/datastructure" rel="nofollow" style="color:#000000;">算法</a>与神经网络，这是对生物学研究最直观的技术抽象。<a class="replace_word" title="深度学习知识库" href="http://lib.csdn.net/base/deeplearning" rel="nofollow" style="color:#000000;">深度学习</a>的前身就是神经网络，这个80年代灵光乍现的技术，在那一波人工<a class="replace_word" title="人工智能规划与决策知识库" href="http://lib.csdn.net/base/aiplanning" rel="nofollow" style="color:#000000;">智能</a>的大潮驱使下，带着人们对于未来AI时代的憧憬，迅速蔓延，一时风头无两，和今天深度学习的火热几乎如出一辙。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; 某乎友的观点给出了这两种技术的鲜明对比，如同麻汁与芝麻酱，换汤不换药，所谓深度在于把二两变成了三斤，给出的官方解释是深度学习只是依赖于<a class="replace_word" title="Hadoop知识库" href="http://lib.csdn.net/base/hadoop" rel="nofollow" style="color:#000000;">大数据</a>和更好的硬件，除此再无区别。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　当然也有不同观点，但不管如何，这并不是我们要讨论的话题，只要捋清来龙去脉，请读者自己站队。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;神经网络</span></strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;生物学家对人体大脑研究发现，人脑存在860亿个神经元，复杂的神经元通过生物电驱动，实现消息的加工和处理，神经元是怎么处理信息的呢？通过神经元结构来进行说明：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111145023984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p>1）神经元作为信息处理的基本结构，主要构件是<strong>树突，细胞体和轴突</strong>，<strong>树突及细胞体</strong>接收外部刺激信号（由其他神经元传递）；</p> 
    <p>2）<strong>轴突</strong>的作用过程是信号处理的过程，这个过程不完全是一种线性的处理过程，而是存在一种潜在的拟合方法，实现信号的放大或者抑制，也有可能出现信号消失，因此这种处理过程带有较大的随机性；</p> 
    <p>3）<strong>突触</strong>&nbsp;实现信号的向下传递，当前面的信号处理后仍然存在时，将信号传递到相连接的神经元；</p> 
    <p>4）由于每个神经元对其他神经元产生的刺激或者抑制，最初的信号得到加工，这个加工过程非常复杂，路径也很深，有人把这个过程称为一个“图灵机”。</p> 
   </blockquote> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;基于生物神经学的研究，神经网络的概念诞生，被抽象成数学符号，描述为以加权平均形式，而信号加工处理过程被简化为单向网络，如下图所示：</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111145330852?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 左图是1943年McCulloch和Pitts抽象出来的一种人工神经元模型，称为“M-P神经元模型”，分为信息归集（加权求和）和 随机处理（激活函数）两个部分，“加权求和”对接收信息进行触发响应，而“激活函数”旨在模拟信号衰减、抑制的随机过程。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;多个神经元按照层次划分，严格定义输入输出，这就是神经网络的雏形，虽然这与实际情况仍有较大差异。</p> 
   <span style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">目前你已经了解一个基本的神经网络模型，那这里面我们需要求解的是什么呢？对，是参数。直接给出公式：</span>
   <br style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111145127000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp; 第二层的输出 由第一层的输入 加权计算得到， 是加权后的一个非线性变换，我们将其描述为激活函数，这是我们接下来要讲的一个重要概念。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;激活函数</span></strong></span></p> 
   <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<a class="replace_word" title="机器学习知识库" href="http://lib.csdn.net/base/machinelearning" rel="nofollow" style="color:#000000;">机器学习</a>里面数据拟合有一个关键问题，模型越复杂，所能描述的信息量越丰富，其可分性也就越好。</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111145438713?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp; 从这个图上，我们能够看到一个很明显的结论，线性函数对于数据的实用性最差，想要描述问题，我们有两种方法：</p> 
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div> 
     <p align="left">1）原来线性的加权平均过程变为更高次幂函数；</p> 
    </div> 
    <div> 
     <p align="left">2）分段拟合。</p> 
    </div> 
   </blockquote> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;拟合函数的次数越高，越能描述数据的特征；分段越合理，结果越准确，这两种方法所描述的是同一种思路-<strong>非线性</strong>。</p> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;非线性解决了简单加权平均无法有效解决的分类描述问题，也是我们所说的激活函数的灵魂所在。</p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;激活函数</strong>&nbsp;通过添加一个额外的非线性过程来模拟神经元处理过程，避免了次数过高带来的计算复杂性，同时兼顾随机性因素。根据生物学特征研究，典型的激活函数应具备以下几个特征：</p> 
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div> 
     <p align="left"><strong>1）单侧抑制；</strong></p> 
    </div> 
    <div> 
     <p align="left"><strong>2）相对宽阔的兴奋边界；</strong></p> 
    </div> 
    <div> 
     <p align="left"><strong>3）稀疏激活性。</strong></p> 
    </div> 
   </blockquote> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong></p> 
    <p align="left"><strong>　　目前&nbsp;</strong>常用的几种激活函数有sigmoid、tanh、ReLU、Softplus。</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170112173319095?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp; &nbsp; &nbsp; &nbsp; Sigmoid函数也称S曲线，将输入映射到 &nbsp;之间，从数学上看，非线性的Sigmoid函数对中央区的信号增益较大，对两侧信号增益较小，公式可以表示为：
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　　　　　 　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111150512682?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp; tanh函数相比在于允许负值的出现，其表现与Sigmoid一致，公式描述为：</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111150526276?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;ReLU（RectifiedLinear Units）是最简单的激活函数（小于0时归0，大于0时不处理），代表特征是分段线性，因此其前向后向传播、求导都是分段线性，计算量相比前两种较小。</p> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;ReLU另一个关键特征在于，其有效模拟了输出为0的神经元丢弃特征，确保了网络稀疏性，能够有效减少参数的相互依存关系，缓解了过拟合问题的发生。ReLU解决了前面两种模型所未能有效描述的<strong>单侧抑制</strong>（右侧不抑制）问题，公式描述为：</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111150534349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Softplus同样是一类接近于生物学特征的激活函数，和ReLU的区别在于不具备<strong>稀疏性</strong>，公式描述为：</p> 
    <p align="left">　　　　　　　　　　　　　　　　　　　　　　　　　　　　 &nbsp; &nbsp;<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111150543177?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
    <p align="left">&nbsp;</p> 
    <p><span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;目标函数</span></strong></span></p> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp; 相信大家对目标函数并不陌生，借助于已有样本，我们能够衡量特定输入与期待的输出之间的差异，描述为：</p> 
    <p align="left">　　　　　　　　　　　　　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111150552880?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;我们的目的在于，通过大量的数据训练，得到上面所提到的一组参数集 ，使得对于特定样本输入，得到的输出结果最接近所期待的样本y值，也就是目标损失最小，一般也把这个损失定义为损失函数（LostFunction）。</p> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;目标函数优化的过程是个求极值的过程，经典的方法包括<strong>随机梯度下降法</strong>（RandomGradient Descent）、<strong>牛顿法</strong>（Newton’smethod）。</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111150559948?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp; 另外还有启发式方法、共轭梯度法等，不如前面两种常用，可以不必关注，对于梯度下降法和牛顿法建议大家系统学习一下PRML、MLAPP、ESL等机器学习的书籍，里面讲的比较透彻，喜欢看视频的也可以找下AndrewNg的Stanford公开课，这里我们不在概念上进行描述，仅给出这两种方法的优缺点，以供选择：</p> 
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div> 
     <p align="left">1）收敛方式不同，梯度下降法以梯度负方向为搜索方向，一阶收敛；而牛顿法考虑了梯度变化趋势，是二阶收敛，能更合适的确定全局搜索方向；</p> 
    </div> 
    <div> 
     <p align="left">2）在收敛速度上，梯度下降法在远离极小值的地方下降很快，而在靠近极小值的地方下降很慢；牛顿法收敛方向更为明确，过程更快；</p> 
    </div> 
    <div> 
     <p align="left">3）复杂性来看，牛顿法对目标函数有严格要求，必须有连续的一、二阶偏导数，海森矩阵必须正定，每一步的计算量更大；</p> 
    </div> 
   </blockquote> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div>
     &nbsp; &nbsp; &nbsp; &nbsp; 梯度下降法得到局部最优解，如果目标函数是一个凸优化问题，就是全局最优解；在非凸情况下，牛顿法不保证收敛到局部最小值，有些时候会收敛到鞍点。
    </div> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;前面提到了目标函数最小化，是求解网络参数的唯一手段，不管是梯度下降还是牛顿法，都是基于迭代过程的。</p> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;以梯度下降法为例，对于目标函数 cost(x)，如何求参数 (w,b) 呢？给定任意初始点，按照负梯度方向搜索，确定下一个迭代点，重复计算直到目标函数收敛，这里面最关键的一点就在于梯度方向的计算，而求负梯度方向的过程可以描述为求偏导的过程。</p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;链式求导法则</strong>，这个在微积分的基本概念是时候发挥用场了，对于函数 f(x)=(1+x)(1+x)，链式公式可以描述为：</p> 
    <div>
     　　　　　　　　　　　　　　　　　　　　　　　　　　　　
     <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111151402651?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
    </div> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>求导公式：</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111151408663?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">　　
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>代入可以得到：
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111151416897?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp; Backpropagationalgorithm，即BP算法，是神经网络典型的应用，其反向传播的思路在于，计算所构造的误差函数针对对应参数的偏导数，来更新对应参数的值。</p> 
    <div>
     &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;我们通过一个实际的例子来进行说明，in(x) 对应的 out(x)，对参数 a0,0&nbsp;迭代更新的过程。
    </div> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111151423897?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left">&nbsp; &nbsp; &nbsp; &nbsp; 我们假设一组样本及参数数据作为迭代输入，参数更新过程描述为：</p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Step0：初始化</strong></p> 
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <blockquote style="background:none;border:2px solid rgb(239,239,239);line-height:1.6;"> 
     <p align="left">1）样本 x=[0.1,0.3,0.2]，对应输出为 z = 0.6，</p> 
    </blockquote> 
    <blockquote style="background:none;border:2px solid rgb(239,239,239);line-height:1.6;"> 
     <p align="left">2）对应的初始化权值 a0&nbsp;= [0.5,0.5,0.5]<em>，</em>b0&nbsp;= 0.5，</p> 
    </blockquote> 
    <blockquote style="background:none;border:2px solid rgb(239,239,239);line-height:1.6;"> 
     <p align="left">3）对应的初始化权值 w&nbsp;= [0.5,0.5,0.5]<em>，</em>b1&nbsp;= 0.5，</p> 
    </blockquote> 
   </blockquote> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Step1：数据正向传播</strong></p> 
    <p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;Y0&nbsp;=0.5*0.1 + 0.5*0.3 + 0.5*0.2 + 0.5 = 0.8，</p> 
    <p>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;Y1&nbsp;Y2&nbsp;省略计算，假设结果为 [0.6, 0.8],</p> 
    <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;Z0&nbsp;=&nbsp;0.5*0.8 + 0.5*Y1&nbsp;+ 0.5*Y2&nbsp;+ 0.5 = 1.6，</p> 
    <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;当前样本误差值：</p> 
    <p>　　　　　　　　　　　　　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153459535?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;"></p> 
    <p>&nbsp; &nbsp; &nbsp; &nbsp; 平均样本误差值（可以作为终止判定依据）：</p> 
    <p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　&nbsp;&nbsp;<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153511175?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">&nbsp;</p> 
    <p>&nbsp; &nbsp; &nbsp; &nbsp; Step2：计算偏导</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153650598?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp; &nbsp; &nbsp; &nbsp; 通过链式求导公式分别计算：
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153659928?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153706194?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">&nbsp; &nbsp;
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153712958?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp; Step3：更新权值</strong>（负梯度方向）</p> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　　　　　　　　　　　　　　　　　
    <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170111153718848?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGlub2x6aGFuZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" style="border:0px;">
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>其中 L为Learn Rate学习率（设为0.6）。
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left"><span style="font-size:18pt;"><strong><span style="color:rgb(0,0,255);">•&nbsp;&nbsp;&nbsp;所谓深度</span></strong></span></p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>我想现在可以真正开启深度学习了。</p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>多层神经网络是不是深度学习？对于这个问题也许有一些争论，作者的给出的答案是肯定的。</p> 
    <p align="left"><strong><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>深度学习== 多层神经网络</strong></p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>Ok，工程师们可以不再有顾虑，开始干活了；学术健儿们先别慌，你字典里的深度学习必然要更高大上，否则让你的CVPR情何以堪呐？So在你眼中：</p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;深度学习==</strong>（带Trick的）<strong>多层神经网络</strong></p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>多层网络采用BP方法训练时，会遇到两个问题：</p> 
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div> 
     <p align="left">1）初始化参数选择不当，导致的&nbsp;<strong>局部极值问题</strong>。局部极值问题；</p> 
    </div> 
    <div> 
     <p align="left">2）误差传播到前面已经变得很小，无法起到参数矫正的意义，这就是所说的&nbsp;<strong>梯度发散</strong>。梯度发散导致最后面靠近输出的2-3层参数结果收敛，前面若干层震荡，而震荡效应又导致后面参数的重新收敛，因此，结果就是，大于3层的网络不光训练困难，准确度也会下降。</p> 
    </div> 
   </blockquote> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>对于这个实际存在的问题，大牛们通过两个阶段来克服：</p> 
    <p align="left"><strong><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>第一阶段：</strong></p> 
    <p align="left"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>2006年Hinton大神提出逐层贪心预训练（layer-wisepre-training）方法进行有效克服，方法概述为：</p> 
   </div> 
   <blockquote style="background:rgb(255,255,255) none;border:2px solid rgb(239,239,239);line-height:1.6;color:rgb(51,51,51);font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <div> 
     <p align="left">1）逐层构建单层神经元，保证每次训练一个单层网络；</p> 
    </div> 
    <div> 
     <p align="left">2）所有层训练完后，使用wake-sleep算法进行调优。</p> 
    </div> 
   </blockquote> 
   <div> 
    <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>第二阶段：</strong></p> 
    <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</strong>2012年开始，CNN大行其道，并且借助ReLU、Dropout等技巧，更有效的规避了训练问题（预训练方法基本被抛弃），至此，多层网络的“所谓深度”得以正名，“<strong>深度学习</strong>”的概念正式确立。</p> 
    <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br></p> 
    <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br></p> 
    <p align="left" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><br></p> 
    <p align="left"><font><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/6972989.html，如需转载请自行联系原作者</span></font><br></p> 
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
