<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>ReLu（修正线性单元）、sigmoid和tahh的比较 « NotBeCN</title>
  <meta name="description" content="             最近，在看论文，提及到这个修正线性单元（Rectified linear unit，ReLU）。    &nbsp;    Deep Sparse Rectifier Neural Networks&nbsp;ReLu(Rectified Linear Units)&nbsp;修正线性单...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2017/11/22/weixin_34396103_90126768.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">ReLu（修正线性单元）、sigmoid和tahh的比较</h1>
    <p class="post-meta">Nov 22, 2017</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">最近，在看论文，提及到这个<strong><span style="color:rgb(255,0,0);">修正线性单元（Rectified linear unit，ReLU）</span></strong>。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-size:18pt;"><a href="http://wenku.baidu.com/link?url=GtIz68_egY1kBAFgt3LY0f2v89H6doReu6pcNB34yHiG_PEiSekhZVtSszGB28dtRmEfAKrYOQtu5T0EVLeTzKwpZAeTJEfo00XOgbj9l4u" rel="nofollow" style="color:#000000;">Deep Sparse Rectifier Neural Networks</a>&nbsp;</span></strong><br><strong><span style="font-size:18pt;"><a href="http://www.cnblogs.com/neopenx/p/4453161.html" rel="nofollow" style="color:#000000;">ReLu(Rectified Linear Units)&nbsp;</a></span></strong><br><strong><span style="font-size:18pt;"><a href="http://www.tuicool.com/articles/RJzMZvi" rel="nofollow" style="color:#000000;">修正线性单元(Rectified linear unit，ReLU）</a>&nbsp;</span></strong><br><strong><span style="font-size:18pt;"><a href="http://blog.csdn.net/sheng_ai/article/details/38055209" rel="nofollow" style="color:#000000;">激活函数实现–4 Rectified linear函数实现&nbsp;<br></a><a href="http://www.douban.com/note/348196265/" rel="nofollow" style="color:#000000;">Rectified Linear Units</a></span></strong></p> 
   <h2 style="font-size:21px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;"><a id="cb_post_title_url" href="http://www.cnblogs.com/alexanderkun/p/5701694.html" rel="nofollow" style="color:#000000;">ReLU 和sigmoid 函数对比</a></h2> 
   <h2 style="font-size:21px;line-height:1.5;font-family:Verdana, Arial, Helvetica, sans-serif;"><a href="http://www.cnblogs.com/alexanderkun/p/6918029.html" rel="nofollow" style="color:#000000;">ReLU为什么比Sigmoid效果好</a></h2> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　在CNN卷积神经网络中，习惯用ReLU函数代替sigmoid, tahh等目标激活函数，这应该是因为，RELU函数在大于0的时候，导数为恒定值，而sigmoid和tahh函数则不是恒定值，相反，sigmoid和tahh的导数，类似于高斯函数的曲线形状，在两端接近目标时，导数变小。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　导数小的话，在训练神经网络的时候，会BP反向传播误差，导致收敛减慢；而ReLU函数则避免了这点，很好很强大。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　当然，事情不是一定的，还是得结合实际情况选择，或者涉及目标激活函数。</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">附：双曲函数类似于常见的(也叫圆函数的)三角函数。基本双曲函数是双曲<a href="http://baike.so.com/doc/5765956-5978724.html" rel="nofollow" style="color:#000000;">正弦</a>"sinh"，双曲余弦"cosh"，从它们导出双曲<a href="http://baike.so.com/doc/6760450-6975091.html" rel="nofollow" style="color:#000000;">正切</a>"tanh"</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160706084332495" alt="" width="350" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong><span style="font-size:18pt;">sigmod函数：</span></strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160706150220410" alt="" width="350" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><span style="font-size:18pt;"><strong>Relu函数：</strong></span></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160706150831634" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"><strong>综合：</strong></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">　　　　　　　　<img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160706150849807" alt="" style="border:0px;"></p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    <strong>@作者：<a class="author-link" href="https://www.zhihu.com/people/yue-han-yue-bu-yue" rel="nofollow" style="color:#000000;">约翰曰不约</a>&nbsp;<br></strong>
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
    <span style="font-size:18pt;"><strong>为什么通常Relu比sigmoid和tanh强，有什么不同？</strong></span>
    <br> 主要是因为它们gradient特性不同。sigmoid和tanh的gradient在饱和区域非常平缓，接近于0，很容易造成vanishing gradient的问题，减缓收敛速度。vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一。相反，Relu的gradient大多数情况下是常数，有助于解决深层网络的收敛问题。Relu的另一个优势是在生物上的合理性，它是单边的，相比sigmoid和tanh，更符合生物神经元的特征。
    <br> 而提出sigmoid和tanh，主要是因为它们全程可导。还有表达区间问题，sigmoid和tanh区间是0到1，或着-1到1，在表达上，尤其是输出层的表达上有优势。
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    &nbsp;
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    @作者：crackhopper
    <span class="bio">，<br></span> 
   </div> 
   <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
    　　ReLU更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息：
   </div> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <p style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">&nbsp;</p> 
   <div> 
    <br>
    <font><span style="font-size:14px;">@作者：Begin Again</span></font>
    <br>
    <div class="zm-editable-content clearfix" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
     　
     <strong><span style="font-size:18pt;">　第一个问题：为什么引入非线性激励函数？</span></strong>
     <br> 如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与
     <em>没有</em>隐藏层效果相当，这种情况就是
     <em>最原始的感知机</em>（Perceptron）了。
     <br> 正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释balabala）。
    </div> 
    <div class="zm-editable-content clearfix" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;"> 
     <br>
     <strong><span style="font-size:18pt;">　第二个问题：为什么引入Relu呢？</span></strong>
     <br> 第一，采用sigmoid等函数，
     <em>算激活函数时（指数运算），计算量大</em>，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。
     <br> 第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成
     <em>信息丢失，参见&nbsp;<a class="member_mention" href="http://blog.csdn.net/code_lr/article/details/51836153" rel="nofollow" style="color:#000000;">@Haofeng Li</a>&nbsp;答案的第三点</em>），从而无法完成深层网络的训练。
     <br> 第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。
     <br>
     <br> 当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。
     <br> 多加一句，现在主流的做法，会在做完relu之后，加一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。大家有兴趣可以看下。
    </div> 
    <div class="zm-editable-content clearfix" style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
     &nbsp;
    </div> 
    <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
     <br>
    </div> 
    <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
     <br>
    </div> 
    <div style="font-family:Verdana, Arial, Helvetica, sans-serif;font-size:14px;">
     <br>
    </div> 
    <div> 
     <font><span style="font-size:14px;">本文转自大数据躺过的坑博客园博客，原文链接：http://www.cnblogs.com/zlslch/p/6970538.html，如需转载请自行联系原作者</span></font>
     <br>
    </div> 
   </div> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
