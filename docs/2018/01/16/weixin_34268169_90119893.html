<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>LSTM « NotBeCN</title>
  <meta name="description" content="             Recurrent Neural Networks    人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。&nbsp...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2018/01/16/weixin_34268169_90119893.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">LSTM</h1>
    <p class="post-meta">Jan 16, 2018</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;">Recurrent Neural Networks</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。&nbsp;<br> 传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。&nbsp;<br> RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_204.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> RNN 包含循环</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。这些循环使得 RNN 看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN 可以被看做是同一神经网络的多次赋值，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_413.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 展开的 RNN</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">并且 RNN 也已经被人们应用了！在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 Andrej Karpathy 的博客文章——<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">The Unreasonable Effectiveness of Recurrent Neural Networks</a>&nbsp;来看看更丰富有趣的 RNN 的成功应用。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">而这些成功应用的关键之处就是 LSTM 的使用，这是一种特别的 RNN，比标准的 RNN 在很多的任务上都表现得更好。几乎所有的令人振奋的关于 RNN 的结果都是通过 LSTM 达到的。这篇博文也会就 LSTM 进行展开。</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t1" style="color:rgb(79,161,219);background:transparent;"></a>长期依赖（Long-Term Dependencies）问题</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个 语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_877.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 不太长的相关信息和位置间隔</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_96.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 相当长的相关信息和位置间隔</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。&nbsp;<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Bengio, et al. (1994)</a>&nbsp;等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">然而，幸运的是，LSTM 并没有这个问题！</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t2" style="color:rgb(79,161,219);background:transparent;"></a>LSTM 网络</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">Long Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Hochreiter &amp; Schmidhuber (1997)</a>&nbsp;提出，并在近期被&nbsp;<a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Alex Graves</a>&nbsp;进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_450.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 标准 RNN 中的重复模块包含单一的层</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_631.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> LSTM 中的重复模块包含四个交互的层</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181722_777.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> LSTM 中的图标</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t3" style="color:rgb(79,161,219);background:transparent;"></a>LSTM 的核心思想</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><strong>LSTM 的关键就是细胞状态</strong>，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。&nbsp;<br><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_640.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> Paste_Image.png</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_567.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> Paste_Image.png</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">LSTM 拥有三个门，来保护和控制细胞状态。</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t4" style="color:rgb(79,161,219);background:transparent;"></a>逐步理解 LSTM</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为&nbsp;<strong>忘记门层</strong>&nbsp;完成。该门会读取h_{t-1}和x_t，输出一个在 0 到 1 之间的数值给每个在细胞状态C_{t-1}中的数字。1 表示“完全保留”，0 表示“完全舍弃”。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前&nbsp;<strong>主语</strong>&nbsp;的类别，因此正确的&nbsp;<strong>代词</strong>&nbsp;可以被选择出来。当我们看到新的&nbsp;<strong>代词</strong>&nbsp;，我们希望忘记旧的&nbsp;<strong>代词</strong>&nbsp;。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_259.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 决定丢弃信息</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\tilde{C}_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在我们语言模型的例子中，我们希望增加新的代词的类别到细胞状态中，来替代旧的需要忘记的代词。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_897.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 确定更新的信息</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">现在是更新旧细胞状态的时间了，C_{t-1}更新为C_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">我们把旧状态与f_t相乘，丢弃掉我们确定需要丢弃的信息。接着加上i_t * \tilde{C}_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的类别信息并添加新的信息的地方。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_883.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 更新细胞状态</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">在语言模型的例子中，因为他就看到了一个&nbsp;<strong>代词</strong>&nbsp;，可能需要输出与一个&nbsp;<strong>动词</strong>&nbsp;相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_463.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> 输出信息</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t5" style="color:rgb(79,161,219);background:transparent;"></a>LSTM 的变体</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">其中一个流形的 LSTM 变体，就是由&nbsp;<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Gers &amp; Schmidhuber (2000)</a>&nbsp;提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_920.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> peephole 连接</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_682.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> coupled 忘记门和输入门</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由&nbsp;<a href="http://arxiv.org/pdf/1406.1078v3.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Cho, et al. (2014)</a>&nbsp;提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><img src="http://static.open-open.com/lib/uploadImg/20150829/20150829181723_697.png" alt="这里写图片描述" title="" style="border:0px;vertical-align:middle;">&nbsp;<br> GRU</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">这里只是部分流行的 LSTM 变体。当然还有很多其他的，如&nbsp;<a href="http://arxiv.org/pdf/1508.03790v2.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Yao, et al. (2015)</a>&nbsp;提出的 Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如&nbsp;<a href="http://arxiv.org/pdf/1402.3511v1.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Koutnik, et al. (2014)</a>&nbsp;提出的 Clockwork RNN。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">要问哪个变体是最好的？其中的差异性真的重要吗？&nbsp;<a href="http://arxiv.org/pdf/1503.04069.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Greff, et al. (2015)</a>&nbsp;给出了流行变体的比较，结论是他们基本上是一样的。&nbsp;<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Jozefowicz, et al. (2015)</a>&nbsp;则在超过 1 万中 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t6" style="color:rgb(79,161,219);background:transparent;"></a>结论</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">刚开始，我提到通过 RNN 得到重要的结果。本质上所有这些都可以使用 LSTM 完成。对于大多数任务确实展示了更好的性能！</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">由于 LSTM 一般是通过一系列的方程表示的，使得 LSTM 有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">LSTM 是我们在 RNN 中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是&nbsp;<strong>注意力</strong>&nbsp;！” 这个想法是让 RNN 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，&nbsp;<a href="http://arxiv.org/pdf/1502.03044v2.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Xu, et al. (2015)</a>&nbsp;已经这么做了——如果你希望深入探索&nbsp;<strong>注意力</strong>&nbsp;可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索……</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">注意力也不是 RNN 研究领域中唯一的发展方向。例如，&nbsp;<a href="http://arxiv.org/pdf/1507.01526v1.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Kalchbrenner, et al. (2015)</a>&nbsp;提出的 Grid LSTM 看起来也是很有钱途。使用生成模型的 RNN，诸如&nbsp;<a href="http://arxiv.org/pdf/1502.04623.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Gregor, et al. (2015)</a>&nbsp;<a href="http://arxiv.org/pdf/1506.02216v3.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Chung, et al. (2015)</a>&nbsp;和&nbsp;<a href="http://arxiv.org/pdf/1411.7610v3.pdf" rel="nofollow" style="color:rgb(202,12,22);text-decoration:none;background:transparent;">Bayer &amp; Osendorfer (2015)</a>&nbsp;提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！</p> 
   <hr style="border-right-width:0px;border-bottom-width:0px;border-left-width:0px;border-top-style:solid;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;line-height:24px;">
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;"><br></p> 
   <h2 style="font-family:'microsoft yahei';font-weight:100;line-height:1.1;color:rgb(63,63,63);font-size:2.15em;"> <a name="t7" style="color:rgb(79,161,219);background:transparent;"></a>致谢</h2> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">I’m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals , Greg Corrado , Jon Shlens , Luke Vilnis , and Ilya Sutskever . I’m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei , and Jacob Steinhardt . I’m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams.</p> 
   <p style="line-height:28px;color:rgb(63,63,63);font-family:'microsoft yahei';font-size:16px;">Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.</p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
