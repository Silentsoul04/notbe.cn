<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>tensorflow实现Word2Vec(找到目标英文单词的相近词) « NotBeCN</title>
  <meta name="description" content="                 根据自己的理解写的读书笔记。    import collections      import math      import os      import random      import zipfile      import urllib      import n...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2018/01/14/weixin_33828101_90123536.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">tensorflow实现Word2Vec(找到目标英文单词的相近词)</h1>
    <p class="post-meta">Jan 14, 2018</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <pre><br></pre> 
   <pre>根据自己的理解写的读书笔记。</pre> 
   <span style="color:#000080;font-weight:bold;">import </span>collections
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>math
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>os
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>random
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>zipfile
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>urllib
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>numpy 
   <span style="color:#000080;font-weight:bold;">as </span>np
   <br>
   <span style="color:#000080;font-weight:bold;">import </span>tensorflow 
   <span style="color:#000080;font-weight:bold;">as </span>tf
   <br>
   <br>
   <span style="color:#808080;font-style:italic;">#定义下载文本数据的函数<br></span>
   <span style="color:#808080;font-style:italic;"># url = 'http://mattmahoney.net/dc/'<br></span>
   <span style="color:#808080;font-style:italic;">#<br></span>
   <span style="color:#808080;font-style:italic;"># def maybe_download(filename,expected_bytes):<br></span>
   <span style="color:#808080;font-style:italic;"># if not os.path.exists(filename):<br></span>
   <span style="color:#808080;font-style:italic;"># filename,_ = urllib.request.urlretrieve(url + filename,filename)<br></span>
   <span style="color:#808080;font-style:italic;"># statinfo = os.stat(filename) #访问一个文件的详细信息。<br></span>
   <span style="color:#808080;font-style:italic;"># if statinfo.st_size == expected_bytes: #文件大小(以字节为单位)<br></span>
   <span style="color:#808080;font-style:italic;"># print('Found and verified(验证）',filename)<br></span>
   <span style="color:#808080;font-style:italic;"># else:<br></span>
   <span style="color:#808080;font-style:italic;"># print(statinfo.st_size)<br></span>
   <span style="color:#808080;font-style:italic;"># raise Exception('Failed to verify(验证）' + filename + 'Can you get to it with a browser(浏览器)?')<br></span>
   <span style="color:#808080;font-style:italic;"># return filename<br></span>
   <span style="color:#808080;font-style:italic;">#<br></span>
   <span style="color:#808080;font-style:italic;"># filename = maybe_download('text8.zip',31344016)<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>filename = 
   <span style="color:#008080;font-weight:bold;">'./text8.zip'<br></span>
   <span style="color:#008080;font-weight:bold;"><br></span>
   <span style="color:#808080;font-style:italic;">#解压文件，并将数据转化成单词的列表<br></span>
   <span style="color:#000080;font-weight:bold;">def </span>read_data(filename):
   <br>
   <span style="color:#000080;font-weight:bold;">with </span>zipfile.ZipFile(filename) 
   <span style="color:#000080;font-weight:bold;">as </span>f:
   <br>
   <span style="color:#808080;font-style:italic;">#获得名字列表，读取成字符串，编码成'utf-8'，最后进行分割<br></span>
   <span style="color:#808080;font-style:italic;"> </span>data = tf.compat.as_str(f.read(f.namelist()[
   <span style="color:#0000ff;">0</span>])).split()
   <br>
   <span style="color:#000080;font-weight:bold;">return </span>data
   <br>
   <br> words = read_data(filename)
   <br>
   <span style="color:#808080;font-style:italic;"># print('Data size',len(words))<br></span>
   <span style="color:#808080;font-style:italic;"># print(words)<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#808080;font-style:italic;">#创建词汇表，将出现最多的50000个单词作为词汇表，放入字典中。<br></span>vocabulary_size = 
   <span style="color:#0000ff;">50000<br></span>
   <span style="color:#0000ff;"><br></span>
   <span style="color:#000080;font-weight:bold;">def </span>build_dataset(words):
   <br> count = [[
   <span style="color:#008080;font-weight:bold;">'UNK'</span>,-
   <span style="color:#0000ff;">1</span>]]
   <br> count.extend(collections.Counter(words).most_common(vocabulary_size - 
   <span style="color:#0000ff;">1</span>))
   <br>
   <span style="color:#808080;font-style:italic;"># c=collections.Counter(words).most_common(10)<br></span>
   <span style="color:#808080;font-style:italic;"> # print(c)<br></span>
   <span style="color:#808080;font-style:italic;"> # count.extend(c)<br></span>
   <span style="color:#808080;font-style:italic;"> # print(count) #[['UNK', -1], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644)]<br></span>
   <span style="color:#808080;font-style:italic;"> </span>dictionary = 
   <span style="color:#000080;">dict</span>()
   <span style="color:#808080;font-style:italic;">#新建空字典<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">for </span>word,_ 
   <span style="color:#000080;font-weight:bold;">in </span>count:
   <br> dictionary[word] = 
   <span style="color:#000080;">len</span>(dictionary)
   <br>
   <span style="color:#808080;font-style:italic;"># print(dictionary) #{'UNK': 0, 'the': 1, 'of': 2, 'and': 3, 'one': 4, 'in': 5, 'a': 6, 'to': 7, 'zero': 8, 'nine': 9, 'two': 10}<br></span>
   <span style="color:#808080;font-style:italic;"> </span>data = 
   <span style="color:#000080;">list</span>()
   <br> unk_count = 
   <span style="color:#0000ff;">0</span>
   <span style="color:#808080;font-style:italic;">#未知单词数量<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">for </span>word 
   <span style="color:#000080;font-weight:bold;">in </span>words:
   <span style="color:#808080;font-style:italic;">#单词索引，不在字典中，则索引为0<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">if </span>word 
   <span style="color:#000080;font-weight:bold;">in </span>dictionary:
   <br> index = dictionary[word]
   <br>
   <span style="color:#000080;font-weight:bold;">else</span>:
   <br> index = 
   <span style="color:#0000ff;">0<br></span>
   <span style="color:#0000ff;"> </span>unk_count += 
   <span style="color:#0000ff;">1<br></span>
   <span style="color:#0000ff;"> </span>data.append(index)
   <br> count[
   <span style="color:#0000ff;">0</span>][
   <span style="color:#0000ff;">1</span>] = unk_count
   <br> reverse_dictionary = 
   <span style="color:#000080;">dict</span>(
   <span style="color:#000080;">zip</span>(dictionary.values(),dictionary.keys()))
   <br>
   <span style="color:#000080;font-weight:bold;">return </span>data,count,dictionary,reverse_dictionary
   <br>
   <br> data,count,dictionary,reverse_dictionary = build_dataset(words)
   <br>
   <br>
   <span style="color:#808080;font-style:italic;">#删除原始单词列表，节约内存。打印词汇表，了解词频<br></span>
   <span style="color:#000080;font-weight:bold;">del </span>words
   <br>
   <span style="color:#808080;font-style:italic;"># print('Most common words (+UNK)',count[:5])<br></span>
   <span style="color:#808080;font-style:italic;"># print('Sample data',data[:10],[reverse_dictionary[i] for i in data[:10]])<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#808080;font-style:italic;">#以上代码为数据处理，得到单词的词频和在字典中的索引<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#808080;font-style:italic;">#skip-gram模式：从目标单词反推语境<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>data_index = 
   <span style="color:#0000ff;">0<br></span>
   <span style="color:#0000ff;"><br></span>
   <span style="color:#808080;font-style:italic;">#生成训练用的batch数据<br></span>
   <span style="color:#808080;font-style:italic;">#batch_size为batch大小，num_skips为对每个单词生成样本数，skip_window为单词最远可以联系的距离<br></span>
   <span style="color:#000080;font-weight:bold;">def </span>generate_batch(batch_size,num_skips,skip_window):
   <br>
   <span style="color:#000080;font-weight:bold;">global </span>data_index 
   <span style="color:#808080;font-style:italic;">#声明全局变量<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">assert </span>batch_size % num_skips == 
   <span style="color:#0000ff;">0</span>
   <span style="color:#808080;font-style:italic;">#断言batch_size是num_skips的整倍数<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">assert </span>num_skips &lt;= 
   <span style="color:#0000ff;">2 </span>* skip_window
   <span style="color:#808080;font-style:italic;">#断言num_skips不大于skip_window的两倍<br></span>
   <span style="color:#808080;font-style:italic;"> </span>batch = np.ndarray(
   <span style="color:#660099;">shape</span>=(batch_size),
   <span style="color:#660099;">dtype</span>=np.int32)
   <span style="color:#808080;font-style:italic;">#初始化为数组<br></span>
   <span style="color:#808080;font-style:italic;"> </span>labels = np.ndarray(
   <span style="color:#660099;">shape</span>=(batch_size,
   <span style="color:#0000ff;">1</span>),
   <span style="color:#660099;">dtype</span>=np.int32)
   <br> span = 
   <span style="color:#0000ff;">2 </span>* skip_window + 
   <span style="color:#0000ff;">1 </span>
   <span style="color:#808080;font-style:italic;">#对某个单词创建相关样本时会使用到的单词数量<br></span>
   <span style="color:#808080;font-style:italic;"> </span>buffer = collections.deque(
   <span style="color:#660099;">maxlen</span>=span) 
   <span style="color:#808080;font-style:italic;">#创建最大容量为span的队列，即双向队列<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">for </span>_ 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(span):
   <br> buffer.append(data[data_index])
   <br> data_index = (data_index + 
   <span style="color:#0000ff;">1</span>) % 
   <span style="color:#000080;">len</span>(data)
   <br>
   <span style="color:#000080;font-weight:bold;">for </span>i 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(batch_size // num_skips):
   <span style="color:#808080;font-style:italic;">#'//'取商的整数部分<br></span>
   <span style="color:#808080;font-style:italic;"> </span>target = skip_window
   <br> targets_to_avoid = [skip_window]
   <span style="color:#808080;font-style:italic;">#因为要预测语境单词，不包括目标单词本身。所以需要一个避免列表<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">for </span>j 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(num_skips):
   <br>
   <span style="color:#000080;font-weight:bold;">while </span>target 
   <span style="color:#000080;font-weight:bold;">in </span>targets_to_avoid:
   <br> target = random.randint(
   <span style="color:#0000ff;">0</span>, span - 
   <span style="color:#0000ff;">1</span>)
   <br> targets_to_avoid.append(target)
   <br> batch[i * num_skips + j] = buffer[skip_window]
   <br> labels[i * num_skips + j, 
   <span style="color:#0000ff;">0</span>] = buffer[target]
   <br> buffer.append(data[data_index])
   <br> data_index = (data_index + 
   <span style="color:#0000ff;">1</span>) % 
   <span style="color:#000080;">len</span>(data)
   <br>
   <span style="color:#000080;font-weight:bold;">return </span>batch,labels
   <br>
   <br>
   <br>
   <span style="color:#808080;font-style:italic;"># batch,labels = generate_batch(batch_size=8,num_skips=2,skip_window=1)<br></span>
   <span style="color:#808080;font-style:italic;"># print(batch)#[3081 3081 12 12 6 6 195 195]<br></span>
   <span style="color:#808080;font-style:italic;"># print(labels)#[[5234]<br></span>
   <span style="color:#808080;font-style:italic;"># # [ 12]<br></span>
   <span style="color:#808080;font-style:italic;"># # [3081]<br></span>
   <span style="color:#808080;font-style:italic;"># # [ 6]<br></span>
   <span style="color:#808080;font-style:italic;"># # [ 12]<br></span>
   <span style="color:#808080;font-style:italic;"># # [ 195]<br></span>
   <span style="color:#808080;font-style:italic;"># # [ 6]<br></span>
   <span style="color:#808080;font-style:italic;"># # [ 2]]<br></span>
   <span style="color:#808080;font-style:italic;"># for i in range(8):<br></span>
   <span style="color:#808080;font-style:italic;"># print(batch[i],reverse_dictionary[batch[i]],'-&gt;',labels[i,0],reverse_dictionary[labels[i,0]])<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#808080;font-style:italic;"><br></span>batch_size = 
   <span style="color:#0000ff;">128<br></span>embedding_size = 
   <span style="color:#0000ff;">128</span>
   <span style="color:#808080;font-style:italic;">#将单词转为稠密向量的维度，一般在50~1000范围<br></span>skip_window = 
   <span style="color:#0000ff;">1<br></span>num_skips = 
   <span style="color:#0000ff;">2<br></span>
   <span style="color:#0000ff;"><br></span>valid_size = 
   <span style="color:#0000ff;">16<br></span>valid_window = 
   <span style="color:#0000ff;">100<br></span>valid_examples = np.random.choice(valid_window,valid_size,
   <span style="color:#660099;">replace</span>=
   <span style="color:#000080;font-weight:bold;">False</span>)
   <span style="color:#808080;font-style:italic;">#生成验证数据，随机抽取词频最高（前valid_window）的valid_size个单词<br></span>num_sampled = 
   <span style="color:#0000ff;">64</span>
   <span style="color:#808080;font-style:italic;">#做负样本的噪声单词数量<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#808080;font-style:italic;">#定义skip-gram网络结构<br></span>graph = tf.Graph()
   <br>
   <span style="color:#000080;font-weight:bold;">with </span>graph.as_default():
   <br>
   <br> train_inputs = tf.placeholder(tf.int32,
   <span style="color:#660099;">shape</span>=[batch_size])
   <br> train_labels = tf.placeholder(tf.int32,
   <span style="color:#660099;">shape</span>=[batch_size,
   <span style="color:#0000ff;">1</span>])
   <br> valid_dataset = tf.constant(valid_examples,
   <span style="color:#660099;">dtype</span>=tf.int32)
   <br>
   <br>
   <span style="color:#808080;font-style:italic;">#限定所有计算都在cpu上执行，因为接下来一些计算操作在GPU上可能还没有实现<br></span>
   <span style="color:#808080;font-style:italic;"> </span>
   <span style="color:#000080;font-weight:bold;">with </span>tf.device(
   <span style="color:#008080;font-weight:bold;">'/cpu:0'</span>):
   <br> embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-
   <span style="color:#0000ff;">1.0</span>,
   <span style="color:#0000ff;">1.0</span>))
   <span style="color:#808080;font-style:italic;">#随机生成所有单词的词向量，单词表大小50000，维度128<br></span>
   <span style="color:#808080;font-style:italic;"> </span>embed = tf.nn.embedding_lookup(embeddings,train_inputs)
   <span style="color:#808080;font-style:italic;">#查找输入train_inputs在embeddings里对应的向量<br></span>
   <span style="color:#808080;font-style:italic;"> #用截断正态分布truncated_normal初始化NCE Loss中的权重参数nce_weights，并将其初始化为0<br></span>
   <span style="color:#808080;font-style:italic;"> </span>nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size,embedding_size],
   <span style="color:#660099;">stddev</span>=
   <span style="color:#0000ff;">1.0</span>/math.sqrt(embedding_size)))
   <br> nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
   <br>
   <br> loss = tf.reduce_mean(tf.nn.nce_loss(
   <span style="color:#660099;">weights</span>=nce_weights,
   <span style="color:#660099;">biases</span>=nce_biases,
   <span style="color:#660099;">labels</span>=train_labels,
   <span style="color:#660099;">inputs</span>=embed,
   <span style="color:#660099;">num_sampled</span>=num_sampled,
   <span style="color:#660099;">num_classes</span>=vocabulary_size))
   <br>
   <br>
   <span style="color:#808080;font-style:italic;">#优化器SGD，学习率1.0<br></span>
   <span style="color:#808080;font-style:italic;"> </span>optimizer = tf.train.GradientDescentOptimizer(
   <span style="color:#0000ff;">1.0</span>).minimize(loss)
   <br>
   <span style="color:#808080;font-style:italic;">#先计算embeddings的平方，并按第二维降维到1，计算嵌入向量embeddings的L2范数<br></span>
   <span style="color:#808080;font-style:italic;"> </span>norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),
   <span style="color:#0000ff;">1</span>,
   <span style="color:#660099;">keep_dims</span>=
   <span style="color:#000080;font-weight:bold;">True</span>))
   <br>
   <span style="color:#808080;font-style:italic;">#标准化embeddings<br></span>
   <span style="color:#808080;font-style:italic;"> </span>normalized_embeddings = embeddings/norm
   <br>
   <span style="color:#808080;font-style:italic;">#查询单词的嵌入向量，并计算验证单词的嵌入向量与词汇表中所有单词的相似性<br></span>
   <span style="color:#808080;font-style:italic;"> </span>valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,valid_dataset)
   <br>
   <span style="color:#808080;font-style:italic;">#transpose_b=True 将b转置<br></span>
   <span style="color:#808080;font-style:italic;"> </span>similarity = tf.matmul(valid_embeddings,normalized_embeddings,
   <span style="color:#660099;">transpose_b</span>=
   <span style="color:#000080;font-weight:bold;">True</span>)
   <br>
   <span style="color:#808080;font-style:italic;">#初始化所有模型参数<br></span>
   <span style="color:#808080;font-style:italic;"> </span>init = tf.global_variables_initializer()
   <br>
   <br> num_steps = 
   <span style="color:#0000ff;">100001</span>
   <span style="color:#808080;font-style:italic;">#迭代10万次<br></span>
   <span style="color:#808080;font-style:italic;"><br></span>
   <span style="color:#000080;font-weight:bold;">with </span>tf.Session(
   <span style="color:#660099;">graph</span>=graph) 
   <span style="color:#000080;font-weight:bold;">as </span>session:
   <br> init.run()
   <br>
   <span style="color:#000080;">print</span>(
   <span style="color:#008080;font-weight:bold;">'Initialized'</span>)
   <br>
   <br> average_loss = 
   <span style="color:#0000ff;">0<br></span>
   <span style="color:#0000ff;"> </span>
   <span style="color:#000080;font-weight:bold;">for </span>step 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(num_steps):
   <br> batch_inputs,batch_labels = generate_batch(batch_size,num_skips,skip_window)
   <br> feed_dict = {train_inputs : batch_inputs,train_labels : batch_labels}
   <br>
   <br> _,loss_val = session.run([optimizer,loss],
   <span style="color:#660099;">feed_dict</span>=feed_dict)
   <br>
   <br> average_loss += loss_val
   <br>
   <br>
   <span style="color:#000080;font-weight:bold;">if </span>step % 
   <span style="color:#0000ff;">2000 </span>== 
   <span style="color:#0000ff;">0</span>:
   <br>
   <span style="color:#000080;font-weight:bold;">if </span>step &gt; 
   <span style="color:#0000ff;">0</span>:
   <br>
   <br> average_loss /= 
   <span style="color:#0000ff;">2000<br></span>
   <span style="color:#0000ff;"> </span>
   <span style="color:#000080;">print</span>(
   <span style="color:#008080;font-weight:bold;">'Average loss at step '</span>,step,
   <span style="color:#008080;font-weight:bold;">': '</span>,average_loss)
   <br> average_loss = 
   <span style="color:#0000ff;">0<br></span>
   <span style="color:#0000ff;"><br></span>
   <span style="color:#0000ff;"> </span>
   <span style="color:#000080;font-weight:bold;">if </span>step % 
   <span style="color:#0000ff;">10000 </span>== 
   <span style="color:#0000ff;">0</span>:
   <br> sim = similarity.eval()
   <br>
   <span style="color:#000080;font-weight:bold;">for </span>i 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(valid_size):
   <br> valid_word = reverse_dictionary[valid_examples[i]]
   <br> top_k = 
   <span style="color:#0000ff;">8<br></span>
   <span style="color:#0000ff;"> </span>nearest = (-sim[i, :]).argsort()[
   <span style="color:#0000ff;">1</span>:top_k+
   <span style="color:#0000ff;">1</span>]
   <span style="color:#808080;font-style:italic;">#argsort将数组从小到大排列，并返回索引<br></span>
   <span style="color:#808080;font-style:italic;"> </span>log_str = 
   <span style="color:#008080;font-weight:bold;">'Nearest to %s:' </span>% valid_word
   <br>
   <span style="color:#000080;font-weight:bold;">for </span>k 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(top_k):
   <br> close_word = reverse_dictionary[nearest[k]]
   <br> log_str = 
   <span style="color:#008080;font-weight:bold;">'%s %s,' </span>% (log_str,close_word)
   <br>
   <span style="color:#000080;">print</span>(log_str)
   <br> final_embeddings = normalized_embeddings.eval()
   <br>
   <br>
   <br>
   <span style="color:#000080;font-weight:bold;">from </span>sklearn.manifold 
   <span style="color:#000080;font-weight:bold;">import </span>TSNE
   <span style="color:#808080;font-style:italic;">#此降维算法比PCA更高级，可视化<br></span>
   <span style="color:#000080;font-weight:bold;">import </span>matplotlib.pyplot 
   <span style="color:#000080;font-weight:bold;">as </span>plt
   <br>
   <br>
   <span style="color:#000080;font-weight:bold;">def </span>plot_with_labels(low_dim_embs,labels,filename=
   <span style="color:#008080;font-weight:bold;">'tsne.png'</span>):
   <br>
   <span style="color:#000080;font-weight:bold;">assert </span>low_dim_embs.shape[
   <span style="color:#0000ff;">0</span>] &gt;= 
   <span style="color:#000080;">len</span>(labels),
   <span style="color:#008080;font-weight:bold;">'More labels than embeddings'<br></span>
   <span style="color:#008080;font-weight:bold;"> </span>plt.figure(
   <span style="color:#660099;">figsize</span>=(
   <span style="color:#0000ff;">18</span>,
   <span style="color:#0000ff;">18</span>))
   <br>
   <span style="color:#000080;font-weight:bold;">for </span>i,label 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">enumerate</span>(labels):
   <span style="color:#808080;font-style:italic;">#enumerate在字典上枚举<br></span>
   <span style="color:#808080;font-style:italic;"> </span>x,y = low_dim_embs[i,:]
   <br> plt.scatter(x,y)
   <span style="color:#808080;font-style:italic;">#显示散点图<br></span>
   <span style="color:#808080;font-style:italic;"> #（工具书p242）annotate在图上添加注释，xy设置箭头所指处的坐标，xytext注释内容坐标，textcoords注释内容坐标的坐标变换方式。<br></span>
   <span style="color:#808080;font-style:italic;"> #'offset points'以点为单位，相对于点xy的坐标<br></span>
   <span style="color:#808080;font-style:italic;"> # ha='right'点在注释右边（right,center,left），va='bottom'点在注释底部('top', 'bottom', 'center', 'baseline')<br></span>
   <span style="color:#808080;font-style:italic;"> </span>plt.annotate(label,
   <span style="color:#660099;">xy</span>=(x,y),
   <span style="color:#660099;">xytext</span>=(
   <span style="color:#0000ff;">5</span>,
   <span style="color:#0000ff;">2</span>),
   <span style="color:#660099;">textcoords</span>=
   <span style="color:#008080;font-weight:bold;">'offset points'</span>,
   <span style="color:#660099;">ha</span>=
   <span style="color:#008080;font-weight:bold;">'right'</span>,
   <span style="color:#660099;">va</span>=
   <span style="color:#008080;font-weight:bold;">'bottom'</span>)
   <br>
   <br> plt.savefig(filename)
   <br>
   <br>
   <span style="color:#808080;font-style:italic;">#perplexity（混乱，复杂）与最近邻数有关，一般在5~50，n_iter达到最优化所需的最大迭代次数，应当不少于250次<br></span>
   <span style="color:#808080;font-style:italic;">#init='pca'pca初始化比random稳定，n_components嵌入空间的维数（即降到2维，默认为2<br></span>tsne = TSNE(
   <span style="color:#660099;">perplexity</span>=
   <span style="color:#0000ff;">30</span>,
   <span style="color:#660099;">n_components</span>=
   <span style="color:#0000ff;">2</span>,
   <span style="color:#660099;">init</span>=
   <span style="color:#008080;font-weight:bold;">'pca'</span>,
   <span style="color:#660099;">n_iter</span>=
   <span style="color:#0000ff;">5000</span>)
   <br> plot_only = 
   <span style="color:#0000ff;">100</span>
   <span style="color:#808080;font-style:italic;">#显示词频最高的一百个<br></span>low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])
   <br> labels = [reverse_dictionary[i] 
   <span style="color:#000080;font-weight:bold;">for </span>i 
   <span style="color:#000080;font-weight:bold;">in </span>
   <span style="color:#000080;">range</span>(plot_only)]
   <br> plot_with_labels(low_dim_embs,labels)
   <br>
   <br>
   <span style="color:#808080;font-style:italic;"># plt.show()</span> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
