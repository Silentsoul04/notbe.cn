<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Millisecond Marketing with Trillions of User Tags Using PostgreSQL « NotBeCN</title>
  <meta name="description" content="                     Background    By taking advantage of just two small PostgreSQL functions, we are able to solve one of the biggest long-term problems pla...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2018/01/17/weixin_34167819_90119662.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Millisecond Marketing with Trillions of User Tags Using PostgreSQL</h1>
    <p class="post-meta">Jan 17, 2018</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="content-detail markdown-body"> 
   <p><img src="https://yqfile.alicdn.com/5a20bab0dd3b1ff3c26024b2f702c17cbeb430a0.jpeg" alt="Alibaba_Cloud_Whitepaper_Securing_the_Data_Center_in_a_Cloud_First_World" title="Alibaba_Cloud_Whitepaper_Securing_the_Data_Center_in_a_Cloud_First_World"></p> 
   <p></p> 
   <h2>Background</h2> 
   <p><b>By taking advantage of just two small PostgreSQL functions, we are able to solve one of the biggest long-term problems plaguing the industry.</b></p> 
   <p>The recommendation system is the backbone of any advertising platform. A proper recommendation system must be precise, real-time, and highly efficient.</p> 
   <p>But which advertisement platform is the best one? Which advertisement platform has the strongest core? To have an effective recommendation system, it needs to have three fundamental features: precise tagging, real-time tag updating, and efficient tagging.</p> 
   <p>Precision refers to the ability to draw distinct, precise user portraits based on massive amounts of user behavior data. This profiling system is also called a tag system. Effective recommendation often hinges on accurate tagging. For example, you likely would not market a pair of reading glasses to the average young man, unless his purchase intention tags demonstrate that he might wish to purchase a pair of reading glasses.</p> 
   <p>Tags must be updated in real time. Many tags are extremely time sensitive, for example marketing to certain customers may only be effective within a certain time window. Likewise, a certain product may be appropriate one minute and less so the next. If your tags are generated every other day or longer, you may miss the best recommendation opportunity. Therefore, it is critical for a recommendation system to operate in real time.</p> 
   <p>Efficiency refers to the system's ability to concurrently react and adapt to user behavior. After all, those of us in the advertisement industry need to obtain information as quickly as possible. If you have several clients purchasing advertisements, your system's capacity for concurrency will be tested before long.</p> 
   <p>An advertisement platform may only be competitive if it possesses the above three core capabilities. Of course, cost is also a concern, particularly hardware, development, and maintenance costs.</p> 
   <p>We will use an E-commerce recommendation system as an example to demonstrate database design and optimization techniques for recommendation systems.</p> 
   <p></p> 
   <h2>Requirements of E-commerce Recommendation Systems</h2> 
   <p>For example, how does a store discovers target customers?</p> 
   <p>To answer this question, we first need to collect some data. For example:</p> 
   <p>1.Users who browse and make purchases at this and other similar stores. Browsing online stores generates behavior records, for instance the stores and commodities we browse, the products we purchase, the stores where we make those purchases, and the times at which all of this happens. Then, for each store, a sample of users and browsed and purchased there can be extracted.</p> 
   <p>2.After obtaining these user groups, you can filter them for similar purchase intention or other attributes. You can analyze the attributes of these groups to form a larger target group and market to them.</p> 
   <p>These are just two simple deductions that can be made concerning E-commerce recommendation systems.</p> 
   <p></p> 
   <h3>Reach and Magnitude</h3> 
   <p>The number of e-shoppers may reach several billion worldwide, and stores could reach the order of tens of millions.</p> 
   <p>The number of available products (divided into subcategories like bar codes) might even reach several hundred million.</p> 
   <p>Store tags for a single user may include the stores the user visited, the time of visit, products viewed and number of times they were viewed, and finally the products purchased by the user. Hundreds of such tags can be generated for a person over time.</p> 
   <p>When the number of user tags reaches several hundred of thousand, we get a clearer picture of the user and his/her habits.</p> 
   <p>Estimates show that we could see trillions of user_tags in the near future (several billion users, each with hundreds of tags related to stores/products).</p> 
   <p></p> 
   <h2>Efficient Database Design</h2> 
   <p>First, we need to organize key data.</p> 
   <p>Data include user ID, browsed stores and times, browsed products and times, and purchased products and quantity (times/quantity can be set to represent a range. For example, 0 can represent any quantity less than 100, 1 can be a quantity between 100 and 500, etc.).</p> 
   <p>These data points can be easily generated from user behavior data, and in turn combined to obtain a target customer group for store or product specific advertisement. For example, we may use this data to define a group of users who browsed haircare related products ten times in one week.</p> 
   <p></p> 
   <h3>Designing table structure</h3> 
   <p>1.Generate IDs for stores and products.</p> 
   <p>2.Track browsing times and quantity of purchased products.</p> 
   <p>Each user may browse and purchase multiple products over a certain period of time. If a record is generated for each store and product, we will wind up with a massive number of records. This can be a waste of space and negatively impact query efficiency.</p> 
   <p>Arrays can be used in PostgreSQL to make these tasks possible while saving storage space. It also allows for array indexes, which improves query efficiency.</p> 
   <p>3.The table structure is as follows:</p> 
   <p>3.1Range table, including dozens or even hundreds of records</p> 
   <p>Field and description</p> 
   <pre><code>class int,    -- Dimension, corresponding to s1, s2, s3, s4 in a user tag table
id int,       -- Offset amount (or enumeration value)    
description   -- Description (such as 1-10,000, 10,001-100,000, etc.)    </code></pre> 
   <p>3.2User tag table</p> 
   <pre><code>uid int primary key,  -- User ID  
s1 int[],  -- Browsed stores and time ranges (store ID hash + range table ID)     
s2 int[],  -- Browsed products and time ranges (product ID hash + range table ID)  
s3 int[],  -- Purchased products and quantity ranges (product ID hash + range table ID)
s4 int[],   -- ... Other dimensions and the like 
Time interval 1, ... -- e.g. 1 day: count each day and write the data into the table</code></pre> 
   <p>3.3Step-wise times</p> 
   <p>Browsing times and purchase quantity are continuous values. It is suggested that they be step-wise for better mining.</p> 
   <p>According to the design in item 3.1, 1-10,000 constitutes a step, and 10,001-100,000 constitutes the next step.</p> 
   <p>Example</p> 
   <pre><code>Step corresponding to track s1
1 -&gt; 0
2 -&gt; 1-10
3 -&gt; 11-100
4 -&gt; 101-500
5 -&gt; 501-
...
9 -&gt; 10000+</code></pre> 
   <p>3.4Combine product and store IDs and the step into a new value - method 2</p> 
   <p>The value is stored in text[], for example store ID:OFFSET. A text array (text[]) is less efficient than an integer array (INT[]), and it requires more storage space.</p> 
   <p>However, if you can handle the downsides, text[] does require slightly less development effort.</p> 
   <p>Example</p> 
   <pre><code>userid|s1|s2|s3 1|{'1:2', '109:9'}|{'2:2', '88:19'}|{'2:1', '88:2'}
Meaning:
• User ID: 1,
• Browsed store 1 (step=2) and store 109 (step 9),
• Browsed product 2 (step=2) and product 88 (step 19),
• Purchased product 2 (step=1) and product 88 (step 2).</code></pre> 
   <p>3.5Combine product and store IDs and the step into a new value - method 2</p> 
   <p>The first method uses a text array, while the second method uses a more efficient int/int8 array.</p> 
   <p>A formula must be used to make the int/int8 value express two meanings (the original store, product ID, and step).</p> 
   <p>The formula design (formula and constant) is as follows:</p> 
   <p>Take the number of visits to the store (s1) field as an example:</p> 
   <pre><code>Starting ID of new value = new_start_val = 1                  -- This constant can be freely defined but cannot be changed once it has been set.
Corresponding step (such as the number of steps for the traffic to the store) = step = 9   -- This constant can be freely defined (the number of steps of each dimension can be different), and cannot be changed once it is set.
Store ID = dp_id                                  -- The original store ID
int/int8 new value = new_val                          -- The new generated value with two meanings (product ID and number of steps)

If the store ID is known, calculate new_val (a write and query process):

$new_val = new_start_val + (step+1)*(dp_id-1)

If new_val is known, calculate the store ID (a translation process):

$dp_id = 1 + (new_val-new_start_val)/(step+1)</code></pre> 
   <p>Example (step is 19, and new_start_val is 1)</p> 
   <pre><code>Browsed store ID=1, indicating 1 step
Browsed store ID=192, indicating 15 steps

Use the information above, the constant, and the formula to generate the new_val for the array:  

{1, 3821}

Translate the store ID based on the array above, constant, and formula:  

{1, 192}</code></pre> 
   <p>4.Shard</p> 
   <p>For example, it is suggested that each 5 million or 10 million users be organized into a partition. Query efficiency can be improved using parallel query.</p> 
   <p>You are recommended to use parallel query (with plproxy, a connection is given for each partition to carry out parallel query) to quickly retrieve all users.</p> 
   <p>If you want to obtain users quickly, and need stream return, you should use inheritance (if it is a multi-node, postgres_fdw+pg_pathman or postgres_fdw+inheritance is used) to return via a cursor.</p> 
   <p></p> 
   <h3>Performance indicators</h3> 
   <p>Browsed stores and products, purchased products, and the billions of users tracked over the specified time interval are all aggregated into a tag array to generate over a trillion user tags.</p> 
   <p>What kind of performance can we expect when parsing billions of user groups based on tags? Since we are using indexing, we can keep the processing time to around 10 ms as long as we also use stream return. Is it starting to feel like the analytics industry is going to be measured in milliseconds?</p> 
   <p>If you're not familiar with PostgreSQL, you may find these results surprising, but you'll quickly get used to it as you come into contact with PostgreSQL more frequently. It provides a whole host of functions to help you solve a wide variety of problems</p> 
   <p></p> 
   <h2>Real-time Design</h2> 
   <p>We've already explained how to efficiently obtain user data, now let's look at how we can update user tags in real time.</p> 
   <p></p> 
   <h3>Stream processing</h3> 
   <p>The goal of stream processing is to update user tags in real time. For example if a user generates hundreds of bytes of browsing records every second, then these records need to be merged into the appropriate tags.</p> 
   <p>With a hundred million or more active users, these update streams need to process up to a trillion updates a day, so how can we update the database in real time? A lot of users probably use the T+1 method and simply give up on real-time responsiveness.</p> 
   <p>However in reality it isn't an unreasonable thing to ask. For example we can use the stream processing function in PostgreSQL to update super high traffic streams.</p> 
   <p>You may be wondering whether the database is capable of processing such a stream. How is the data updated in real time in the database?</p> 
   <p>An open-source product called PipelineDB (based on PostgreSQL and compatible with PostgreSQL) in PostgreSQL is designed to do just this. It helps you merge data in real time (you can set the time interval or the number of accumulated updated rows), and carries out persistent actions after reaching the threshold; otherwise, it first updates continuously in the memory.</p> 
   <p>There are two articles available for reference</p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201512/20151215_01.md" rel="nofollow">Application of Stream Processing for the "Internet of Things (IoT)" - Use PostgreSQL for Real-time Processing (Trillions Each Day)</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161220_01.md" rel="nofollow">Ever-present StreamCompute - Combining PostgreSQL and PipelineDB for IoT</a></p> 
   <p>Of course, if it isn't necessary to process data in real time, and T+1 meets your requirements, then it's not necessary to use Pipeline DB.</p> 
   <p></p> 
   <h3>Why does stream processing need to be completed in the database?</h3> 
   <p>We know that target customer discovery can only be completed when the tag data has been submitted to the database, so if we don't use stream calculation, rather use frames like JSTROM, then a layer of updates will be ignored. For example, 100 billion updates could become only 100 million.</p> 
   <p>However, external stream processing does bring with it a number of challenges.</p> 
   <p>1.JSTROM requires additional computing resources without adding parallel efficiency compared to PipelineDB.</p> 
   <p>2.Furthermore, it does not provide faster queries than StreamCompute when directly inserting data into the database.</p> 
   <p>3.It also increases development costs.</p> 
   <p></p> 
   <h2>Stress Testing</h2> 
   <p>For the stress testing phase I chose a machine with 32 cores, 2 SSD cards, and 512GB memory.</p> 
   <p>It stores 3.2 hundred million users. Each user includes 4 array fields. Each field includes 1,000 elements, i.e., 4,000*3,200 million = 1,280,billion user_tags.</p> 
   <p></p> 
   <h3>Example 1</h3> 
   <p>There are 10 tables, each with 10 million users, and 4 tag fields stored with tsvector.</p> 
   <p>Use a rum index.</p> 
   <pre><code>postgres=# create tablespace tbs1 location '/u01/digoal/tbs1';  
CREATE TABLESPACE  
  
postgres=# create tablespace tbs2 location '/u02/digoal/tbs2';  
CREATE TABLESPACE  
  
do language plpgsql 
$$
  
declare  
  i int;  
  suffix text;  
  tbs text;  
begin  
  for i in 0..10 loop  
    if i=0 then  
      suffix := '';  
      tbs := 'tbs1';  
    elsif i &gt;=1 and i&lt;=5 then  
      suffix := i::text;  
      tbs := 'tbs1';  
    else  
      suffix := i::text;  
      tbs := 'tbs2';  
    end if;  
    if i=0 then  
      execute 'create unlogged table test'||suffix||'(uid int primary key USING INDEX TABLESPACE '||tbs||', s1 tsvector, s2 tsvector, s3 tsvector, s4 tsvector) with (autovacuum_enabled=off, toast.autovacuum_enabled=off) tablespace '||tbs;  
    else  
      execute 'create unlogged table test'||suffix||'(uid int primary key USING INDEX TABLESPACE '||tbs||', s1 tsvector, s2 tsvector, s3 tsvector, s4 tsvector) inherits(test) with (autovacuum_enabled=off, toast.autovacuum_enabled=off) tablespace '||tbs;  
    end if;  
    execute 'create index idx_test'||suffix||'_s1 on test'||suffix||' using rum(s1 rum_tsvector_ops) tablespace '||tbs;  
    execute 'create index idx_test'||suffix||'_s2 on test'||suffix||' using rum(s2 rum_tsvector_ops) tablespace '||tbs;  
    execute 'create index idx_test'||suffix||'_s3 on test'||suffix||' using rum(s3 rum_tsvector_ops) tablespace '||tbs;   
    execute 'create index idx_test'||suffix||'_s4 on test'||suffix||' using rum(s4 rum_tsvector_ops) tablespace '||tbs;  
  end loop;  
end;  

$$
;  
  
select relname,reltablespace from pg_class  where relname ~ 'test' order by 2,1;  </code></pre> 
   <p>Generate the test data script</p> 
   <pre><code>vi test.sql  
  
\set uid1 random(1,10000000)  
\set uid2 random(10000001,20000000)  
\set uid3 random(20000001,30000000)  
\set uid4 random(30000001,40000000)  
\set uid5 random(40000001,50000000)  
\set uid6 random(50000001,60000000)  
\set uid7 random(60000001,70000000)  
\set uid8 random(70000001,80000000)  
\set uid9 random(80000001,90000000)  
\set uid10 random(90000001,100000000)  
insert into test1 (uid,s1,s2,s3,s4) select :uid1+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test2 (uid,s1,s2,s3,s4) select :uid2+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test3 (uid,s1,s2,s3,s4) select :uid3+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test4 (uid,s1,s2,s3,s4) select :uid4+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test5 (uid,s1,s2,s3,s4) select :uid5+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test6 (uid,s1,s2,s3,s4) select :uid6+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test7 (uid,s1,s2,s3,s4) select :uid7+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test8 (uid,s1,s2,s3,s4) select :uid8+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test9 (uid,s1,s2,s3,s4) select :uid9+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
insert into test10 (uid,s1,s2,s3,s4) select :uid10+id, (select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)),(select array_to_tsvector(array_agg(trunc(5000000*random())||'_'||trunc(20*random()))) from generate_series(1,1000)) from generate_series(1,1000) t(id) on conflict do nothing;  
  
  
nohup pgbench -M prepared -n -r -P 1 -f ./test.sql -c 64 -j 64 -T 1000000 &gt;/dev/null 2&gt;&amp;1 &amp;  </code></pre> 
   <p>A tag is generated by the process of combining 5 million unique IDs and 20 unique IDs, and each tsvector stores 1,000 such combinations.</p> 
   <p></p> 
   <h3>Example 2</h3> 
   <p>There are 10 tables, each with 10 million users, and 4 tag fields stored in a text[].</p> 
   <p>Here we use a GIN index. Other factors are the same as in example 1.</p> 
   <pre><code>do language plpgsql 
$$
  
declare  
  i int;  
  suffix text;  
  tbs text;  
begin  
  for i in 0..10 loop  
    if i=0 then  
      suffix := '';  
      tbs := 'tbs1';  
    elsif i &gt;=1 and i&lt;=5 then  
      suffix := i::text;  
      tbs := 'tbs1';  
    else  
      suffix := i::text;  
      tbs := 'tbs2';  
    end if;  
    if i=0 then  
      execute 'create unlogged table test'||suffix||'(uid int primary key USING INDEX TABLESPACE '||tbs||', s1 text[], s2 text[], s3 text[], s4 text[]) with (autovacuum_enabled=off, toast.autovacuum_enabled=off) tablespace '||tbs;  
    else  
      execute 'create unlogged table test'||suffix||'(uid int primary key USING INDEX TABLESPACE '||tbs||', s1 text[], s2 text[], s3 text[], s4 text[]) inherits(test) with (autovacuum_enabled=off, toast.autovacuum_enabled=off) tablespace '||tbs;  
    end if;  
    execute 'create index idx_test'||suffix||'_s1 on test'||suffix||' using gin(s1 ) tablespace '||tbs;  
    execute 'create index idx_test'||suffix||'_s2 on test'||suffix||' using gin(s2 ) tablespace '||tbs;  
    execute 'create index idx_test'||suffix||'_s3 on test'||suffix||' using gin(s3 ) tablespace '||tbs;   
    execute 'create index idx_test'||suffix||'_s4 on test'||suffix||' using gin(s4 ) tablespace '||tbs;  
  end loop;  
end;  

$$
;  
  
select relname,reltablespace from pg_class  where relname ~ 'test' order by 2,1;  </code></pre> 
   <p></p> 
   <h3>Example 3 (pressure testing)</h3> 
   <p>There are 64 partition tables, each with 5 million records. An int array is used to store a total of 4 million tags, and each user has 4,000 random tags to ensure that there are enough target customers.</p> 
   <p>We also use a GIN index for target customer discovery.</p> 
   <pre><code>alter role postgres set gin_pending_list_limit='128MB';  
  
do language plpgsql 
$$
  
declare  
  i int;  
  suffix text;  
  tbs text;  
begin  
  for i in 0..64 loop  
    if i=0 then  
      suffix := '';  
      tbs := 'tbs1';  
    elsif i &gt;=1 and i&lt;=32 then  
      suffix := i::text;  
      tbs := 'tbs1';  
    else  
      suffix := i::text;  
      tbs := 'tbs2';  
    end if;  
    if i=0 then  
      execute 'create unlogged table test'||suffix||'(uid int primary key USING INDEX TABLESPACE '||tbs||', s1 int[], s2 int[], s3 int[], s4 int[]) with (autovacuum_enabled=off, toast.autovacuum_enabled=off) tablespace '||tbs;  
    else  
      execute 'create unlogged table test'||suffix||'(uid int primary key USING INDEX TABLESPACE '||tbs||', s1 int[], s2 int[], s3 int[], s4 int[]) inherits(test) with (autovacuum_enabled=off, toast.autovacuum_enabled=off) tablespace '||tbs;  
    end if;  
    execute 'create index idx_test'||suffix||'_s1 on test'||suffix||' using gin(s1 ) tablespace '||tbs;  
    execute 'create index idx_test'||suffix||'_s2 on test'||suffix||' using gin(s2 ) tablespace '||tbs;  
    execute 'create index idx_test'||suffix||'_s3 on test'||suffix||' using gin(s3 ) tablespace '||tbs;   
    execute 'create index idx_test'||suffix||'_s4 on test'||suffix||' using gin(s4 ) tablespace '||tbs;  
  end loop;  
end;  

$$
;  
  
select relname,reltablespace from pg_class  where relname ~ 'test' order by 2,1;  </code></pre> 
   <p>Script for generating test data</p> 
   <pre><code>vi test1.sh  
  
for ((i=1;i&lt;=64;i++))  
do  
echo "\set uid random($((($i-1)*5000000+1)),$(($i*5000000)))" &gt; test$i.sql  
  
echo "insert into test$i (uid,s1,s2,s3,s4) select :uid, (select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s1,(select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s2,(select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s3, (select array_agg(trunc(random()*4000000)) from generate_series(1,1000)) s4 on conflict do nothing;" &gt;&gt; test$i.sql  
  
done  
  
. ./test1.sh  </code></pre> 
   <p>Begin generating test data</p> 
   <pre><code>vi test2.sh  
  
for ((i=1;i&lt;=64;i++))  
do  
nohup pgbench -M prepared -n -r -P 1 -f ./test$i.sql -c 1 -j 1 -T 1000000 &gt;/dev/null 2&gt;&amp;1 &amp;  
done  
  
. ./test2.sh   </code></pre> 
   <p>Once the data has been inserted, merge the pending list.</p> 
   <p>Execute vacuum analyze or gin_clean_pending_list. Refer to</p> 
   <p><a href="https://www.postgresql.org/docs/9.6/static/functions-admin.html#FUNCTIONS-ADMIN-INDEX" rel="nofollow">https://www.postgresql.org/docs/9.6/static/functions-admin.html#FUNCTIONS-ADMIN-INDEX</a></p> 
   <p><a href="https://www.postgresql.org/docs/9.6/static/sql-vacuum.html" rel="nofollow">https://www.postgresql.org/docs/9.6/static/sql-vacuum.html</a></p> 
   <p><a href="https://www.postgresql.org/docs/9.6/static/gin-implementation.html#GIN-FAST-UPDATE" rel="nofollow">https://www.postgresql.org/docs/9.6/static/gin-implementation.html#GIN-FAST-UPDATE</a></p> 
   <p></p> 
   <h3>Target customer discovery requirements - performance testing</h3> 
   <p>Carry out pressure testing on example 3</p> 
   <p>1.Complete target customer discovery in 10 ms.</p> 
   <p>For example, search for groups in which s1 includes 3 and s2 includes 4.</p> 
   <pre><code>postgres=# begin;
BEGIN
Time: 0.030 ms
postgres=# declare a cursor for select uid from test where s1 @&gt; array[1] and s2 @&gt; array[4];
DECLARE CURSOR
Time: 6.679 ms
postgres=# fetch 100 in a;
    uid    
-----------
  19246842
 118611240
 148504032
 185844649
(4 rows)
Time: 101.041 ms</code></pre> 
   <p>We only found a few groups and they are not representative. We need to find more groups.</p> 
   <pre><code>postgres=# begin;
BEGIN
postgres=# declare a cursor for select uid from test where s1 @&gt; array[1] or s2 @&gt; array[4];
DECLARE CURSOR
Time: 3.484 ms
postgres=# fetch 100 in a;
   uid   
---------
 2911941
 2373506
 .....
   29713
 3353782
 2836804
 1602067
(100 rows)
Time: 3.892 ms

postgres=# fetch 100 in a;
   uid   
---------
  384170
 1332271
 4282941
 ......
 1190946
 4524861
 1110635
(100 rows)
Time: 4.005 ms</code></pre> 
   <p>2.Paging via a cursor, as mentioned above.</p> 
   <p>3.Stream return, as mentioned above.</p> 
   <p>4.Parallel batch return</p> 
   <p>In this step, the plug-in plproxy can be used to specify a parallel for each partition to implement parallel batch return. What are the best results we can expect?</p> 
   <p>For serial query, all shard tables are queried sequentially, so the operation can take quite a while, 113 ms for 15,221 target customer discoveries, for example.</p> 
   <pre><code>postgres=# explain (analyze,verbose,timing,costs,buffers) select uid from test where s1 @&gt; array[1];
                                                              QUERY PLAN                                                               
---------------------------------------------------------------------------------------------------------------------------------------
 Append  (cost=0.00..233541.24 rows=206876 width=4) (actual time=0.081..108.037 rows=15221 loops=1)
   Buffers: shared hit=60641
   -&gt;  Seq Scan on public.test  (cost=0.00..0.00 rows=1 width=4) (actual time=0.001..0.001 rows=0 loops=1)
         Output: test.uid
         Filter: (test.s1 @&gt; '{1}'::integer[])
   -&gt;  Bitmap Heap Scan on public.test1  (cost=33.71..2901.56 rows=3188 width=4) (actual time=0.078..0.381 rows=242 loops=1)
         Output: test1.uid
         Recheck Cond: (test1.s1 @&gt; '{1}'::integer[])
         Heap Blocks: exact=238
         Buffers: shared hit=243
         -&gt;  Bitmap Index Scan on idx_test1_s1  (cost=0.00..32.91 rows=3188 width=0) (actual time=0.049..0.049 rows=242 loops=1)
               Index Cond: (test1.s1 @&gt; '{1}'::integer[])
               Buffers: shared hit=5

... 62 tables in the middle are ignored.

   -&gt;  Bitmap Heap Scan on public.test64  (cost=34.00..2935.31 rows=3225 width=4) (actual time=0.068..0.327 rows=214 loops=1)
         Output: test64.uid
         Recheck Cond: (test64.s1 @&gt; '{1}'::integer[])
         Heap Blocks: exact=211
         Buffers: shared hit=216
         -&gt;  Bitmap Index Scan on idx_test64_s1  (cost=0.00..33.19 rows=3225 width=0) (actual time=0.041..0.041 rows=214 loops=1)
               Index Cond: (test64.s1 @&gt; '{1}'::integer[])
               Buffers: shared hit=5
 Planning time: 2.016 ms
 Execution time: 109.400 ms
(519 rows)
Time: 113.216 ms</code></pre> 
   <p>Parallel query performance is equivalent to the time required for a single partition, usually about 1ms.</p> 
   <pre><code>postgres=# explain (analyze,verbose,timing,costs,buffers) select uid from test1 where s1 @&gt; array[1];
                                                        QUERY PLAN                                                         
---------------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on public.test1  (cost=33.71..2901.56 rows=3188 width=4) (actual time=0.085..0.383 rows=242 loops=1)
   Output: uid
   Recheck Cond: (test1.s1 @&gt; '{1}'::integer[])
   Heap Blocks: exact=238
   Buffers: shared hit=243
   -&gt;  Bitmap Index Scan on idx_test1_s1  (cost=0.00..32.91 rows=3188 width=0) (actual time=0.051..0.051 rows=242 loops=1)
         Index Cond: (test1.s1 @&gt; '{1}'::integer[])
         Buffers: shared hit=5
 Planning time: 0.097 ms
 Execution time: 0.423 ms
(10 rows)
Time: 1.011 ms</code></pre> 
   <p>Therefore, parallel query can greatly improve overall performance.</p> 
   <p>References</p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201005/20100511_01.md" rel="nofollow">Use Plproxy to Design a PostgreSQL Distributed Database</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201110/20111025_01.md" rel="nofollow">A Smart PostgreSQL Extension plproxy 2.2 Practices</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201608/20160824_02.md" rel="nofollow">PostgreSQL Best Practices - Vertical Database Shard (Based on plproxy)</a></p> 
   <p>However parallel queries are unnecessary if you intend to use stream return.</p> 
   <p></p> 
   <h3>Sharding</h3> 
   <p>When we are dealing with several billions of users, we can shard them by user ID and then use multiple hosts.</p> 
   <p>Of course, there's no need to use multiple hosts if your machine has enough space and CPU cores to meet your needs.</p> 
   <p>What methods can we use to implement multiple hosts? Refer to the following articles for a few simple and quick solutions.</p> 
   <p>You can think of the node postgres_fdw as the TDDL or DRDS of MySQL. It supports cross-node JOIN, and condition, order, and aggregation push down, making it just as convenient as TDDL/DRDS.</p> 
   <p>postgres_fdw is stateless, and only stores a structure (a distribution rule), making it easy to horizontally expand the node postgres_fdw.</p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161004_01.md" rel="nofollow">PostgreSQL 9.6 Modularization, Sharding (Based on postgres_fdw) - Supporting Forward Pass on the Kernel Layer</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161005_01.md" rel="nofollow">PostgreSQL 9.6 Sharding + Modularization (Based on postgres_fdw) Best Practices - Design and Practices for General Database Sharding</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161027_01.md" rel="nofollow">PostgreSQL 9.6 Sharding Based on FDW &amp; pg_pathman</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161024_01.md" rel="nofollow">PostgreSQL 9.5+ Efficient Implementation of Table Partitions - pg_pathman</a></p> 
   <p></p> 
   <h2>Requirements and Performance for Target Customer Discovery</h2> 
   <p>This requirement falls into the realm of PostgreSQL. Actually, it is simply a location-based KNN query. PostgreSQL can easily meet this requirement via GiST index.</p> 
   <p>After data sharding, PostgreSQL can still obtain results quickly via Merge Sort.</p> 
   <p>For example,</p> 
   <pre><code>postgres=# explain (analyze,verbose,timing,costs,buffers) select * from t order by id limit 10;
                                                              QUERY PLAN                                                               
---------------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=0.72..1.13 rows=10 width=4) (actual time=0.158..0.165 rows=10 loops=1)
   Output: t.id
   Buffers: shared hit=3 read=4
   -&gt;  Merge Append  (cost=0.72..819.74 rows=20001 width=4) (actual time=0.157..0.162 rows=10 loops=1)
         Sort Key: t.id
         Buffers: shared hit=3 read=4
         -&gt;  Index Only Scan using idx on public.t  (cost=0.12..2.14 rows=1 width=4) (actual time=0.003..0.003 rows=0 loops=1)
               Output: t.id
               Heap Fetches: 0
               Buffers: shared hit=1
         -&gt;  Index Only Scan using idx1 on public.t1  (cost=0.29..225.28 rows=10000 width=4) (actual time=0.107..0.107 rows=6 loops=1)
               Output: t1.id
               Heap Fetches: 6
               Buffers: shared hit=1 read=2
         -&gt;  Index Only Scan using idx2 on public.t2  (cost=0.29..225.28 rows=10000 width=4) (actual time=0.043..0.044 rows=5 loops=1)
               Output: t2.id
               Heap Fetches: 5
               Buffers: shared hit=1 read=2
 Planning time: 0.181 ms
 Execution time: 0.219 ms
(20 rows)

postgres=# explain (analyze,verbose,timing,costs,buffers) select * from t order by id ;
                                                             QUERY PLAN                                                              
-------------------------------------------------------------------------------------------------------------------------------------
 Merge Append  (cost=0.72..819.74 rows=20001 width=4) (actual time=0.043..10.324 rows=20000 loops=1)
   Sort Key: t.id
   Buffers: shared hit=149
   -&gt;  Index Only Scan using idx on public.t  (cost=0.12..2.14 rows=1 width=4) (actual time=0.004..0.004 rows=0 loops=1)
         Output: t.id
         Heap Fetches: 0
         Buffers: shared hit=1
   -&gt;  Index Only Scan using idx1 on public.t1  (cost=0.29..225.28 rows=10000 width=4) (actual time=0.021..3.266 rows=10000 loops=1)
         Output: t1.id
         Heap Fetches: 10000
         Buffers: shared hit=74
   -&gt;  Index Only Scan using idx2 on public.t2  (cost=0.29..225.28 rows=10000 width=4) (actual time=0.017..3.309 rows=10000 loops=1)
         Output: t2.id
         Heap Fetches: 10000
         Buffers: shared hit=74
 Planning time: 0.175 ms
 Execution time: 11.791 ms
(17 rows)</code></pre> 
   <p><a href="https://yq.aliyun.com/articles/2999" rel="nofollow">Performance of PostgreSQL for Querying Neighboring Geographical Locations Among Tens of Billions of Data Points</a></p> 
   <p>A 12-core machine returns each request in about 0.8 ms. There are about 80,000 TPSs.</p> 
   <p></p> 
   <h2>Summary</h2> 
   <p>Let's get back to three core questions of the target customer system.</p> 
   <p>We did not go over precision in this article as it belongs more to the realm of the data mining system. We will address precision in a future article (Using the PostgreSQL, Greenplum MADlib Machine Learning Library).</p> 
   <p>The real-time requirement refers to the ability to update tags in real-time. The solution provided in this article is based on stream processing in the database, which, compared to external stream processing, takes up fewer resources, reduces development costs, improves development efficiency, and is more responsive.</p> 
   <p>As for efficiency, the solution provided here utilizes PostgreSQL along with GIN indexes of array data to achieve target customer discovery in a matter of milliseconds, even when dealing with trillions of user tags.</p> 
   <p></p> 
   <h2>Related Articles</h2> 
   <p><a href="https://github.com/digoal/blog/blob/master/201512/20151215_01.md" rel="nofollow">Application of Stream Processing for the "Internet of Things (IoT)" - Use PostgreSQL for Real-time Processing (Trillions Each Day)</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201606/20160621_01.md" rel="nofollow">For the Tribe - How to Leverage PostgreSQL to Pair Genes to Improve Future Generations</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161220_01.md" rel="nofollow">Ever-present StreamCompute - Combining PostgreSQL and PipelineDB for IoT</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161216_01.md" rel="nofollow">Secrets to Analysis Acceleration Engines - LLVM, Column-Store, Multi-core Parallel, Operator Multiplex Integrate - Open the Treasure Chest of PostgreSQL</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161213_01.md" rel="nofollow">Analysis on Requirements for Finance Risk Control, Police Criminal Detection, Social Relation, Networking Analysis, and Database Implementation - PostgreSQL Graph Database Scenario Application</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161205_02.md" rel="nofollow">Real-time Data Exchange Platform - BottledWater-pg with Confluent</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161126_01.md" rel="nofollow">Application of PostgreSQL in Video and Image De-duplication and Image Search Services</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161021_01.md" rel="nofollow">Create Real-time User Portrait Recommendation System Based on Alibaba Cloud RDS PostgreSQL</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161124_02.md" rel="nofollow">PostgreSQL and Thoughts on Winning Train Tickets at 12306</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161124_01.md" rel="nofollow">Access Control Advertisement Sales System Requirement Analysis and PostgreSQL Database Implementation</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161114_01.md" rel="nofollow">Technology used in Double 11 Shopping Events - Logistics and Dynamic Path Planning</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161115_01.md" rel="nofollow">Technology used in Double 11 Shopping Events - Word Division and Search</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161117_01.md" rel="nofollow">Technology used in Double 11 Shopping Events - Sniping Technology, Down to the Second</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201611/20161118_01.md" rel="nofollow">Technology used in Double 11 Shopping Events - Millisecond Word Division, Test Regularity and Similarity</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161001_01.md" rel="nofollow">PostgreSQL 9.6 Guides Open-source Database to Tackle Multi-core Parallel Computing Puzzles</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201609/20160929_02.md" rel="nofollow">PostgreSQL's Past and Present</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201609/20160906_01.md" rel="nofollow">How to Create a GIS Test Environment - Import OpenStreetMap Sample Data into PostgreSQL PostGIS Library</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161004_01.md" rel="nofollow">PostgreSQL 9.6 Modularization, Sharding (Based on postgres_fdw) - Supporting Forward Pass on the Kernel Layer</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161005_01.md" rel="nofollow">PostgreSQL 9.6 Sharding + Modularization (Based on postgres_fdw) Best Practices - Design and Practices for General Database Sharding</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161027_01.md" rel="nofollow">PostgreSQL 9.6 Sharding Based on FDW &amp; pg_pathman</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201610/20161024_01.md" rel="nofollow">PostgreSQL 9.5+ Efficient Implementation of Table Partitions - pg_pathman</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201506/20150601_01.md" rel="nofollow">PostgreSQL Database Security Guide</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201605/20160523_01.md" rel="nofollow">Secrets of PostgreSQL 9.6 – A Single Bloom Algorithm Index to Support Query for Any Column Combination</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201607/20160725_01.md" rel="nofollow">PostgreSQL Uses Recursive SQL to Find Dependency among Database Objects</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161203_01.md" rel="nofollow">Use PostgreSQL to Describe One's Life's Climax, Urine Point, and Low Point - Window/Frame or Slope/Derivative/Curvature/Calculus?</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201612/20161201_01.md" rel="nofollow">Use PostgreSQL to Make up 618 s of Lost Time - Convergence of Recursive Optimization</a></p> 
   <p><a href="https://yq.aliyun.com/articles/50922" rel="nofollow">Advantages of PostGIS in O2O Application</a></p> 
   <p><a href="https://yq.aliyun.com/articles/2999" rel="nofollow">Performance of PostgreSQL for Querying Neighboring Geographical Locations Among Tens of Billions of Data Points</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201005/20100511_01.md" rel="nofollow">Use Plproxy to Design a PostgreSQL Distributed Database</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201110/20111025_01.md" rel="nofollow">A Smart PostgreSQL Extension plproxy 2.2 Practices</a></p> 
   <p><a href="https://github.com/digoal/blog/blob/master/201608/20160824_02.md" rel="nofollow">PostgreSQL Best Practices - Vertical Database Shard (Based on plproxy)</a></p> 
  </div> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
