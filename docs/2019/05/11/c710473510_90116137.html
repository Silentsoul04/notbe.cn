<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Python中文分词工具大合集：安装、使用和测试 « NotBeCN</title>
  <meta name="description" content="         这篇文章事实上整合了前面两篇文章的相关介绍，同时添加一些其他的Python中文分词相关资源，甚至非Python的中文分词工具，仅供参考。   首先介绍之前测试过的8款中文分词工具，这几款工具可以直接在&nbsp;AINLP公众号后台在线测试&nbsp;，严格的说，它们不完全是纯粹的中文分词工具，...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/11/c710473510_90116137.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Python中文分词工具大合集：安装、使用和测试</h1>
    <p class="post-meta">May 11, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>这篇文章事实上整合了前面两篇文章的相关介绍，同时添加一些其他的Python中文分词相关资源，甚至非Python的中文分词工具，仅供参考。</p> 
  <p>首先介绍之前测试过的8款中文分词工具，这几款工具可以直接在&nbsp;<a href="http://www.52nlp.cn/%E4%BA%94%E6%AC%BE%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E7%BA%BF%E4%B8%8Apk-jieba-snownlp-pkuseg-thulac-hanlp" rel="nofollow">AINLP公众号后台在线测试</a>&nbsp;，严格的说，它们不完全是纯粹的中文分词工具，例如SnowNLP, Thulac, HanLP，LTP，CoreNLP都是很全面的(中文）自然语言处理工具。安装这些模块其实很简单，只要按官方文档的方法安装即可，以下做个简单介绍，在Python3.x的环境下测试，Ubuntu16.04 或 MacOS 测试成功。</p> 
  <p><img alt="" class="has" height="75" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019051122354422.png" width="299"></p> 
  <p>再附加介绍12款其他的中文分词工具或者中文分词模块，最后的两款fnlp和ansj是比较棒的java中文分词工具，貌似还没有python接口，记录一下。这些中文分词工具我没有测试，感兴趣的同学可以动手试试。</p> 
  <p>1）&nbsp;<a href="https://github.com/fxsjy/jieba" rel="nofollow">Jieba</a>&nbsp;:&nbsp;<a href="https://github.com/fxsjy/jieba" rel="nofollow">https://github.com/fxsjy/jieba</a></p> 
  <p>“结巴”中文分词：做最好的 Python 中文分词组件</p> 
  <p>"Jieba" (Chinese for "to stutter") Chinese text segmentation: built to be the best Python Chinese word segmentation module.</p> 
  <p>特点</p> 
  <p>支持三种分词模式：</p> 
  <p>精确模式，试图将句子最精确地切开，适合文本分析；</p> 
  <p>全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；</p> 
  <p>搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。</p> 
  <p>支持繁体分词</p> 
  <p>支持自定义词典</p> 
  <p>MIT 授权协议</p> 
  <p>安装：</p> 
  <p>代码对 Python 2/3 均兼容</p> 
  <p>全自动安装：easy_install jieba 或者 pip install jieba / pip3 install jieba</p> 
  <p>半自动安装：先下载 http://pypi.python.org/pypi/jieba/ ，解压后运行 python setup.py install</p> 
  <p>手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [6]: import jieba                                                            
 
In [7]: seg_list = jieba.cut("我爱自然语言处理", cut_all=True)                  
 
In [8]: print("Full Mode: " + " ".join(seg_list)) # 全模式                      
Full Mode: 我 爱 自然 自然语言 语言 处理
 
In [9]: seg_list = jieba.cut("我爱自然语言处理", cut_all=False)                 
 
In [10]: print("Default Mode: " + " ".join(seg_list)) # 默认模式/精确模式       
Default Mode: 我 爱 自然语言 处理
 
In [11]: seg_list = jieba.cut("我爱自然语言处理")                               
 
In [12]: print("Default Mode: " + " ".join(seg_list)) # 默认精确模式            
Default Mode: 我 爱 自然语言 处理
 
In [13]: seg_list = jieba.cut_for_search("我爱自然语言处理") # 搜索引擎模式     
 
In [14]: print("Search Mode: " + " ".join(seg_list)) # 搜索引擎模式              
Search Mode: 我 爱 自然 语言 自然语言 处理</code></pre> 
  <p>2)&nbsp;<a href="https://github.com/isnowfy/snownlp" rel="nofollow">SnowNLP</a>&nbsp;:&nbsp;<a href="https://github.com/isnowfy/snownlp" rel="nofollow">https://github.com/isnowfy/snownlp</a></p> 
  <pre>
<code class="language-html hljs">SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。
Features  中文分词（Character-Based Generative Model）  词性标注（TnT 3-gram 隐马）  情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）  文本分类（Naive Bayes）  转换成拼音（Trie树实现的最大匹配）  繁体转简体（Trie树实现的最大匹配）  提取文本关键词（TextRank算法）  提取文本摘要（TextRank算法）  tf，idf  Tokenization（分割成句子）  文本相似（BM25）  支持python3（感谢erning）
</code></pre> 
  <p>安装：</p> 
  <p>$ pip install snownlp</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [18]: from snownlp import SnowNLP                                            
 
In [19]: s = SnowNLP("我爱自然语言处理")                                        
 
In [20]: print(' '.join(s.words))                                               
我 爱 自然 语言 处理</code></pre> 
  <p>3)&nbsp;<a href="https://github.com/lancopku/pkuseg-python" rel="nofollow">PkuSeg</a>&nbsp;:&nbsp;<a href="https://github.com/lancopku/pkuseg-python" rel="nofollow">https://github.com/lancopku/pkuseg-python</a></p> 
  <p>pkuseg多领域中文分词工具; The pkuseg toolkit for multi-domain Chinese word segmentation</p> 
  <p>主要亮点</p> 
  <p>pkuseg具有如下几个特点：</p> 
  <p>多领域分词。不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了新闻领域，网络领域，医药领域，旅游领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。各领域分词样例可参考 example.txt。</p> 
  <p>更高的分词准确率。相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。</p> 
  <p>支持用户自训练模型。支持用户使用全新的标注数据进行训练。</p> 
  <p>支持词性标注。</p> 
  <p>编译和安装</p> 
  <p>目前仅支持python3</p> 
  <p>为了获得好的效果和速度，强烈建议大家通过pip install更新到目前的最新版本</p> 
  <p>通过PyPI安装(自带模型文件)：</p> 
  <p>pip3 install pkuseg</p> 
  <p>之后通过import pkuseg来引用</p> 
  <p>建议更新到最新版本以获得更好的开箱体验：</p> 
  <p>pip3 install -U pkuseg</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [23]: import pkuseg                                                               
 
In [24]: pku_seg = pkuseg.pkuseg()                                                   
 
In [25]: print(' '.join(pku_seg.cut('我爱自然语言处理')))                            
我 爱 自然 语言 处理</code></pre> 
  <p>4)&nbsp;<a href="https://github.com/thunlp/THULAC-Python" rel="nofollow">THULAC</a>&nbsp;:&nbsp;<a href="https://github.com/thunlp/THULAC-Python" rel="nofollow">https://github.com/thunlp/THULAC-Python</a></p> 
  <p>THULAC：一个高效的中文词法分析工具包</p> 
  <p>THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：</p> 
  <p>能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。</p> 
  <p>准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。</p> 
  <p>速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。</p> 
  <p>编译和安装</p> 
  <p>python版(兼容python2.x版和python3.x版)</p> 
  <p>从github下载(需下载模型文件，见获取模型)</p> 
  <p>将thulac文件放到目录下，通过 import thulac 来引用</p> 
  <p>thulac需要模型的支持，需要将下载的模型放到thulac目录下。</p> 
  <p>pip下载(自带模型文件)</p> 
  <p>pip install thulac</p> 
  <p>通过 import thulac 来引用</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [31]: import thulac                                                               
 
In [32]: thu_lac = thulac.thulac(seg_only=True)                                      
Model loaded succeed
 
In [33]: thu_result = thu_lac.cut("我爱自然语言处理", text=True)                     
 
In [34]: print(thu_result)                                                           
我 爱 自然 语言 处理</code></pre> 
  <p>5)&nbsp;<a href="https://github.com/hankcs/pyhanlp" rel="nofollow">pyhanlp</a>&nbsp;:&nbsp;<a href="https://github.com/hankcs/pyhanlp" rel="nofollow">https://github.com/hankcs/pyhanlp</a></p> 
  <p>pyhanlp: Python interfaces for HanLP</p> 
  <p>自然语言处理工具包HanLP的Python接口, 支持自动下载与升级HanLP，兼容py2、py3。</p> 
  <p>安装</p> 
  <p>pip install pyhanlp</p> 
  <p>注意pyhanlp安装之后使用的时候还会自动下载相关的数据文件，zip压缩文件600多M，速度有点慢，时间有点长</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [36]: from pyhanlp import HanLP                                                   
 
In [37]: han_word_seg = HanLP.segment('我爱自然语言处理')                            
 
In [38]: print(' '.join([term.word for term in han_word_seg]))                       
我 爱 自然语言处理</code></pre> 
  <p>6）&nbsp;<a href="https://github.com/rockyzhengwu/FoolNLTK" rel="nofollow">FoolNLTK</a>&nbsp;：&nbsp;<a href="https://github.com/rockyzhengwu/FoolNLTK" rel="nofollow">https://github.com/rockyzhengwu/FoolNLTK</a></p> 
  <p>特点</p> 
  <p>可能不是最快的开源中文分词，但很可能是最准的开源中文分词</p> 
  <p>基于BiLSTM模型训练而成</p> 
  <p>包含分词，词性标注，实体识别,都有比较高的准确率</p> 
  <p>用户自定义词典</p> 
  <p>可训练自己的模型</p> 
  <p>批量处理</p> 
  <p>定制自己的模型</p> 
  <p>get clone https://github.com/rockyzhengwu/FoolNLTK.git</p> 
  <p>cd FoolNLTK/train</p> 
  <p>详细训练步骤可参考文档</p> 
  <p>仅在linux Python3环境测试通过</p> 
  <p>安装，依赖TensorFlow, 会自动安装：</p> 
  <p>pip install foolnltk</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [1]: import fool                                                              
 
In [2]: text = "我爱自然语言处理"                                                
 
In [3]: print(fool.cut(text))                                                    
[['我', '爱', '自然', '语言', '处理']]
 
In [4]: print(' '.join(fool.cut(text)[0]))                                       
我 爱 自然 语言 处理</code></pre> 
  <p>7) LTP:&nbsp;<a href="https://github.com/HIT-SCIR/ltp" rel="nofollow">https://github.com/HIT-SCIR/ltp</a></p> 
  <p><a href="https://github.com/HIT-SCIR/pyltp" rel="nofollow">pyltp</a>&nbsp;:&nbsp;<a href="https://github.com/HIT-SCIR/pyltp" rel="nofollow">https://github.com/HIT-SCIR/pyltp</a></p> 
  <p>pyltp 是 语言技术平台（Language Technology Platform, LTP） 的 Python 封装。</p> 
  <p>安装 pyltp</p> 
  <p>注：由于新版本增加了新的第三方依赖如dynet等，不再支持 windows 下 python2 环境。</p> 
  <p>使用 pip 安装</p> 
  <p>使用 pip 安装前，请确保您已安装了 pip</p> 
  <p>$ pip install pyltp</p> 
  <p>接下来，需要下载 LTP 模型文件。</p> 
  <p>下载地址 - `模型下载 http://ltp.ai/download.html`_</p> 
  <p>当前模型版本 - 3.4.0</p> 
  <p>注意在windows下 3.4.0 版本的 语义角色标注模块 模型需要单独下载，具体查看下载地址链接中的说明。</p> 
  <p>请确保下载的模型版本与当前版本的 pyltp 对应，否则会导致程序无法正确加载模型。</p> 
  <p>从源码安装</p> 
  <p>您也可以选择从源代码编译安装</p> 
  <p>$ git clone https://github.com/HIT-SCIR/pyltp</p> 
  <p>$ git submodule init</p> 
  <p>$ git submodule update</p> 
  <p>$ python setup.py install</p> 
  <p>安装完毕后，也需要下载相应版本的 LTP 模型文件。</p> 
  <p>这里使用"pip install pyltp"安装，安装完毕后在LTP模型页面下载模型数据：&nbsp;<a href="http://ltp.ai/download.html" rel="nofollow">http://ltp.ai/download.html</a>&nbsp;，我下载的是 ltp_data_v3.4.0.zip ，压缩文件有600多M，解压后1.2G，里面有不同NLP任务的模型。</p> 
  <p>中文分词示例：</p> 
  <pre>
<code class="language-html hljs">In [5]: from pyltp import Segmentor                                              
 
In [6]: segmentor = Segmentor()                                                  
 
# 分词模型路径，依据你下载后放得位置而定
In [7]: segmentor.load('./data/ltp/ltp_data_v3.4.0/cws.model')                   
 
In [8]: print(' '.join(segmentor.segment('我爱自然语言处理')))                   
我 爱 自然 语言 处理</code></pre> 
  <p>8)&nbsp;<a href="https://stanfordnlp.github.io/CoreNLP/" rel="nofollow">Stanford CoreNLP</a>&nbsp;:&nbsp;<a href="https://stanfordnlp.github.io/CoreNLP/" rel="nofollow">https://stanfordnlp.github.io/CoreNLP/</a></p> 
  <p><a href="https://github.com/Lynten/stanford-corenlp" rel="nofollow">stanfordcorenlp</a>&nbsp;:&nbsp;<a href="https://github.com/Lynten/stanford-corenlp" rel="nofollow">https://github.com/Lynten/stanford-corenlp</a></p> 
  <p>这里用的是斯坦福大学CoreNLP的python封装：stanfordcorenlp</p> 
  <pre>
<code class="language-html hljs">stanfordcorenlp is a Python wrapper for Stanford CoreNLP. It provides a simple API for text processing tasks such as Tokenization, Part of Speech Tagging, Named Entity Reconigtion, Constituency Parsing, Dependency Parsing, and more.
</code></pre> 
  <p>安装很简单，pip即可：</p> 
  <p>pip install stanfordcorenlp</p> 
  <p>但是要使用中文NLP模块需要下载两个包，在CoreNLP的下载页面下载模型数据及jar文件，目前官方是3.9.1版本：</p> 
  <p><a href="https://nlp.stanford.edu/software/corenlp-backup-download.html" rel="nofollow">https://nlp.stanford.edu/software/corenlp-backup-download.html</a></p> 
  <p>第一个是：stanford-corenlp-full-2018-02-27.zip</p> 
  <p>第二个是：stanford-chinese-corenlp-2018-02-27-models.jar</p> 
  <p>前者解压后把后者也要放进去，否则指定中文的时候会报错。</p> 
  <p>中文分词使用示例：</p> 
  <pre>
<code class="language-html hljs">In [11]: from stanfordcorenlp import StanfordCoreNLP                             
 
In [12]: stanford_nlp = StanfordCoreNLP('./data/corenlp/stanford-corenlp-full-201
    ...: 8-02-27', lang='zh')                                                    
 
In [13]: seg_results = stanford_nlp.word_tokenize('我爱自然语言处理')            
 
In [14]: print(' '.join(seg_results))                                            
我爱 自然 语言 处理</code></pre> 
  <p>9) NLPIR: NLPIR大数据语义智能分析平台</p> 
  <p><a href="https://github.com/NLPIR-team/NLPIR" rel="nofollow">https://github.com/NLPIR-team/NLPIR</a></p> 
  <p>Python接口：&nbsp;<a href="https://github.com/tsroten/pynlpir" rel="nofollow">https://github.com/tsroten/pynlpir</a></p> 
  <p>10）DeepNLP: Deep Learning NLP Pipeline implemented on Tensorflow</p> 
  <p>深度学习中文（分词）NLP工具</p> 
  <p><a href="https://github.com/rockingdingo/deepnlp" rel="nofollow">https://github.com/rockingdingo/deepnlp</a></p> 
  <p>11) kcws: Deep Learning Chinese Word Segment</p> 
  <p>深度学习中文分词</p> 
  <p><a href="https://github.com/koth/kcws" rel="nofollow">https://github.com/koth/kcws</a></p> 
  <p>12) ID-CNN-CWS: Source codes and corpora of paper "Iterated Dilated Convolutions for Chinese Word Segmentation"</p> 
  <p>基于迭代卷积神经网络的中文分词</p> 
  <p><a href="https://github.com/hankcs/ID-CNN-CWS" rel="nofollow">https://github.com/hankcs/ID-CNN-CWS</a></p> 
  <p>13）Genius: a chinese segment base on crf</p> 
  <p>中文分词 (Python) Genius是一个开源的python中文分词组件，采用 CRF(Conditional Random Field)条件随机场算法。</p> 
  <p><a href="https://github.com/duanhongyi/genius" rel="nofollow">https://github.com/duanhongyi/genius</a></p> 
  <p>14）YaYaNLP：Pure python NLP toolkit</p> 
  <p>纯python编写的中文自然语言处理包</p> 
  <p><a href="https://github.com/Tony-Wang/YaYaNLP" rel="nofollow">https://github.com/Tony-Wang/YaYaNLP</a></p> 
  <p>15）小明NLP：提供中文分词, 词性标注, 拼写检查，文本转拼音，情感分析，文本摘要，偏旁部首</p> 
  <p><a href="https://github.com/SeanLee97/xmnlp" rel="nofollow">https://github.com/SeanLee97/xmnlp</a></p> 
  <p>16）loso: Chinese segmentation library</p> 
  <p><a href="https://github.com/fangpenlin/loso" rel="nofollow">https://github.com/fangpenlin/loso</a></p> 
  <p>17) yaha:"哑哈"中文分词</p> 
  <p>更快或更准确，由你来定义。通过简单定制，让分词模块更适用于你的需求。 "Yaha" You can custom your Chinese Word Segmentation efficiently by using Yaha</p> 
  <p><a href="https://github.com/jannson/yaha" rel="nofollow">https://github.com/jannson/yaha</a></p> 
  <p>18) ChineseWordSegmentation：无需语料库的中文分词</p> 
  <p><a href="https://github.com/Moonshile/ChineseWordSegmentation" rel="nofollow">https://github.com/Moonshile/ChineseWordSegmentation</a></p> 
  <p>19) fnlp: 中文自然语言处理工具包 Toolkit for Chinese natural language processing</p> 
  <p>https://github.com/FudanNLP/fnlp</p> 
  <p>这一款出自复旦NLP组，Java实现，貌似还没有Python接口。</p> 
  <p>20）ansj分词</p> 
  <p>ict的真正java实现.分词效果速度都超过开源版的ict. 中文分词,人名识别,词性标注,用户自定义词典</p> 
  <p>这一款也是一个很棒的中文分词工具，不过貌似也没有很好的Python接口。</p> 
  <p>文章出自：“我爱自然语言处理”</p> 
  <p>原文地址：&nbsp;<a href="http://www.52nlp.cn/" rel="nofollow">http://www.52nlp.cn</a></p> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
