<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Python爬虫入门教程第二十九讲：掘金网全站用户爬虫 scrapy « NotBeCN</title>
  <meta name="description" content="         爬前叨叨   已经编写了33篇爬虫文章了，如果你按着一个个的实现，你的爬虫技术已经入门，从今天开始慢慢的就要写一些有分析价值的数据了，今天我选了一个《掘金网》，我们去爬取一下他的全站用户数据。   爬取思路   获取全站用户，理论来说从1个用户作为切入点就可以，我们需要爬取用户的关注列表，从关注...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/1557726508836.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Python爬虫入门教程第二十九讲：掘金网全站用户爬虫 scrapy</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h2 id="爬前叨叨">爬前叨叨</h2> 
  <p>已经编写了33篇爬虫文章了，如果你按着一个个的实现，你的爬虫技术已经入门，从今天开始慢慢的就要写一些有分析价值的数据了，今天我选了一个《掘金网》，我们去爬取一下他的<strong>全站用户</strong>数据。</p> 
  <h3 id="爬取思路">爬取思路</h3> 
  <p>获取全站用户，理论来说从1个用户作为切入点就可以，我们需要爬取用户的关注列表，从关注列表不断的叠加下去。</p> 
  <p>随便打开一个用户的个人中心</p> 
  <p><img alt="在这里插入图片描述" class="has" src="https://user-gold-cdn.xitu.io/2019/2/15/168ee7a76e5fff53?w=1012&amp;h=391&amp;f=png&amp;s=149614"></p> 
  <p>绿色圆圈里面的都是我们想要采集到的信息。这个用户关注0人？那么你还需要继续找一个入口，这个用户一定要关注了别人。选择关注列表，是为了让数据有价值，因为关注者里面可能大量的小号或者不活跃的账号，价值不大。</p> 
  <p>我选了这样一个入口页面，它关注了3个人，你也可以选择多一些的，这个没有太大影响！<br><code>https://juejin.im/user/55fa7cd460b2e36621f07dde/following</code><br> 我们要通过这个页面，去抓取用户的ID<br><img alt="在这里插入图片描述" class="has" src="https://user-gold-cdn.xitu.io/2019/2/15/168ee7a76e4d82a5?w=1538&amp;h=479&amp;f=png&amp;s=173364"></p> 
  <p>得到ID之后，你才可以拼接出来下面的链接</p> 
  <pre class="has">
<code>https://juejin.im/user/用户ID/following</code></pre> 
  <h2 id="爬虫编写">爬虫编写</h2> 
  <p>分析好了之后，就可以创建一个<code>scrapy</code>项目了</p> 
  <p><code>items.py</code>&nbsp;文件，用来限定我们需要的所有数据，注意到下面有个<code>_id = scrapy.Field()</code>&nbsp;这个先预留好，是为了<code>mongdb</code>准备的，其他的字段解释请参照注释即可。</p> 
  <pre class="has">
<code>class JuejinItem(scrapy.Item):
   
    _id = scrapy.Field()
    username = scrapy.Field()
    job = scrapy.Field()
    company =scrapy.Field()
    intro = scrapy.Field()
    # 专栏
    columns = scrapy.Field()
    # 沸点
    boiling = scrapy.Field()
    # 分享
    shares = scrapy.Field()
    # 赞
    praises = scrapy.Field()
    #
    books = scrapy.Field()
    # 关注了
    follow = scrapy.Field()
    # 关注者
    followers = scrapy.Field()
    goods = scrapy.Field()
    editer = scrapy.Field()
    reads = scrapy.Field()
    collections = scrapy.Field()
    tags = scrapy.Field()
</code></pre> 
  <p>编写爬虫主入口文件&nbsp;<code>JuejinspiderSpider.py</code></p> 
  <pre class="has">
<code>import scrapy
from scrapy.selector import Selector
from Juejin.items import JuejinItem

class JuejinspiderSpider(scrapy.Spider):
    name = 'JuejinSpider'
    allowed_domains = ['juejin.im']
    # 起始URL    5c0f372b5188255301746103
    start_urls = ['https://juejin.im/user/55fa7cd460b2e36621f07dde/following']</code></pre> 
  <p><code>def parse</code>&nbsp;函数，逻辑不复杂，处理两个业务即可</p> 
  <ol>
   <li>返回item</li> 
   <li>返回关注列表的Request</li> 
  </ol>
  <p>item的获取，我们需要使用xpath匹配即可，为了简化代码量，我编写了一个提取方法，叫做<code>get_default</code>函数。</p> 
  <pre class="has">
<code>    def get_default(self,exts):
        if len(exts)&gt;0:
            ret = exts[0]
        else:
            ret = 0
        return ret

    def parse(self, response):
        #base_data = response.body_as_unicode()
        select = Selector(response)
        item = JuejinItem()
        # 这个地方获取一下数据
        item["username"] = select.xpath("//h1[@class='username']/text()").extract()[0]
        position = select.xpath("//div[@class='position']/span/span/text()").extract()
        if position:
            job = position[0]
            if len(position)&gt;1:
                company = position[1]
            else:
                company = ""
        else:
            job = company = ""
        item["job"] = job
        item["company"] = company
        item["intro"] = self.get_default(select.xpath("//div[@class='intro']/span/text()").extract())
        # 专栏
        item["columns"] = self.get_default(select.xpath("//div[@class='header-content']/a[2]/div[2]/text()").extract())
        # 沸点
        item["boiling"] = self.get_default(select.xpath("//div[@class='header-content']/a[3]/div[2]/text()").extract())
        # 分享
        item["shares"] = self.get_default(select.xpath("//div[@class='header-content']/a[4]/div[2]/text()").extract())
        # 赞
        item["praises"] = self.get_default(select.xpath("//div[@class='header-content']/a[5]/div[2]/text()").extract())
        #
        item["books"] = self.get_default(select.xpath("//div[@class='header-content']/a[6]/div[2]/text()").extract())

        # 关注了
        item["follow"] = self.get_default(select.xpath("//div[@class='follow-block block shadow']/a[1]/div[2]/text()").extract())
        # 关注者
        item["followers"] = self.get_default(select.xpath("//div[@class='follow-block block shadow']/a[2]/div[2]/text()").extract())


        right = select.xpath("//div[@class='stat-block block shadow']/div[2]/div").extract()
        if len(right) == 3:
            item["editer"] = self.get_default(select.xpath("//div[@class='stat-block block shadow']/div[2]/div[1]/span/text()").extract())
            item["goods"] = self.get_default(select.xpath("//div[@class='stat-block block shadow']/div[2]/div[2]/span/span/text()").extract())
            item["reads"] = self.get_default(select.xpath("//div[@class='stat-block block shadow']/div[2]/div[3]/span/span/text()").extract())

        else:
            item["editer"] = ""
            item["goods"] = self.get_default(select.xpath("//div[@class='stat-block block shadow']/div[2]/div[1]/span/span/text()").extract())
            item["reads"] = self.get_default(select.xpath("//div[@class='stat-block block shadow']/div[2]/div[2]/span/span/text()").extract())


        item["collections"] = self.get_default(select.xpath("//div[@class='more-block block']/a[1]/div[2]/text()").extract())
        item["tags"] = self.get_default(select.xpath("//div[@class='more-block block']/a[2]/div[2]/text()").extract())
        yield item  # 返回item
 </code></pre> 
  <p>上述代码，已经成功返回了item，打开<code>setting.py</code>文件中的<code>pipelines</code>设置，测试一下是否可以存储数据,顺便在<br><code>DEFAULT_REQUEST_HEADERS</code>&nbsp;配置一下request的请求参数。</p> 
  <p><code>setting.py</code></p> 
  <pre class="has">
<code>DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en',
    "Host": "juejin.im",
    "Referer": "https://juejin.im/timeline?sort=weeklyHottest",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 浏览器UA"
}

ITEM_PIPELINES = {
   'Juejin.pipelines.JuejinPipeline': 20,
}</code></pre> 
  <p>本爬虫数据存储到<code>mongodb</code>里面，所以需要你在<code>pipelines.py</code>文件编写存储代码。</p> 
  <pre class="has">
<code>
import time
import pymongo

DATABASE_IP = '127.0.0.1'
DATABASE_PORT = 27017
DATABASE_NAME = 'sun'
client = pymongo.MongoClient(DATABASE_IP,DATABASE_PORT)
db = client.sun
db.authenticate("dba", "dba")
collection = db.jujin  # 准备插入数据


class JuejinPipeline(object):

    def process_item(self, item, spider):
        try:
            collection.insert(item)
        except Exception as e:
            print(e.args)
</code></pre> 
  <p>运行代码之后，如果没有报错，完善最后一步即可，在Spider里面将爬虫的循环操作完成</p> 
  <pre class="has">
<code>      list_li = select.xpath("//ul[@class='tag-list']/li")  # 获取所有的关注
      for li in list_li:
           a_link = li.xpath(".//meta[@itemprop='url']/@content").extract()[0] # 获取URL
             # 返回拼接好的数据请求
           yield scrapy.Request(a_link+"/following",callback=self.parse)</code></pre> 
  <p>所有的代码都已经写完啦<br><img alt="在这里插入图片描述" class="has" src="https://user-gold-cdn.xitu.io/2019/2/15/168ee7a76e882d96?w=1280&amp;h=302&amp;f=png&amp;s=76247"></p> 
  <p>全站用户爬虫编写完毕，厉害吧。小编整理一套Python资料和PDF，有需要Python学习资料可以加学习群：1004391443，反正闲着也是闲着呢，不如学点东西啦~~</p> 
  <p>扩展方向</p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" src="https://user-gold-cdn.xitu.io/2019/2/15/168ee7a76ef5ede0?w=120&amp;h=120&amp;f=jpeg&amp;s=1597">&nbsp;</p> 
  <ol>
   <li>爬虫每次只爬取关注列表的第一页，也可以循环下去，这个不麻烦</li> 
   <li>在<code>setting.py</code>中开启多线程操作</li> 
   <li>添加redis速度更快，后面会陆续的写几篇分布式爬虫，提高爬取速度</li> 
   <li>思路可以扩展，N多网站的用户爬虫，咱后面也写几个</li> 
  </ol>
  <p>&nbsp;</p> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
