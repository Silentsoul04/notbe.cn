<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Task1-PyTorch的基础概念 « NotBeCN</title>
  <meta name="description" content="                   Task 1-PyTorch的基本概念   1. 什么是PyTorch？为什么选择PyTorch？       PyTorch是一个深度学习框架，同时它也是一个科学计算库。**这是PyTorch的核心团队对PyTorch的描述。PyTorch的科学计算方面主要是PyTorch...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/MainTain__90118363.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Task1-PyTorch的基础概念</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="Task_1PyTorch_0"></a>Task 1-PyTorch的基本概念</h1> 
  <h4><a id="1_PyTorchPyTorch_2"></a>1. 什么是PyTorch？为什么选择PyTorch？</h4> 
  <ul> 
   <li>PyTorch是一个深度学习框架，同时它也是一个科学计算库。**这是PyTorch的核心团队对PyTorch的描述。PyTorch的科学计算方面主要是PyTorch的张量库和相关张量运算的结果。PyTorch的Tensor（张量）可以在GPU上运行。PyTorch的最初版本是在2017年1月发布，其前身是机器学习框架Torch。Soumith Chintala因为引导了PyTorch项目而广受赞誉，而他创建PyTorch的原因也很简单：Lua版本的Torch正在老化，因此需要用Python编写更新版本。因此，PyTorch诞生了。我们可能听说过PyTorch是由Facebook创造和维护的，这是因为PyTorch被创建时Soumith Chintala正在Facebook的AI研究中心工作。</li> 
   <li>对于深度学习和神经网络的初学者来说，学习PyTorch的最主要的原因就是它是一个简洁且高效快速的框架。使用PyTorch进行编程之后，我们将对神经网络和深度学习有一个更深的理解。PyTorch的顶级哲学之一就是不干涉，这使得我们可以更加专注于神经网络而不是实际框架。当我们在使用PyTorch写代码时，我们就只是在扩展标准Python类，当我们需要调试时，也是使用标准的Python调试器。PyTorch的设计是现代化的，其源码对于Python开发者来说非常容易阅读，因为其主要是由Python实现的，只对一些有性能瓶颈的操作使用了C++和CUDA的代码。总而言之，PyTorch是一个伟大的工具，它将加深我们对深度学习和神经网络的理解。</li> 
  </ul> 
  <h4><a id="2_PyTorch_7"></a>2. PyTorch的安装</h4> 
  <p>首先介绍一下Anaconda，它是一个开源的包、环境管理器，可以用于在同一个机器上安装不同版本的软件包及其依赖，并能够在不同的环境之间切换。</p> 
  <p>安装分为以下几步：</p> 
  <ol> 
   <li> <p><a href="https://www.anaconda.com/distribution/" rel="nofollow">下载并安装Anaconda</a></p> </li> 
   <li> <p><a href="https://pytorch.org/" rel="nofollow">访问PyTorch官网</a></p> </li> 
   <li> <p>为特定环境指定适当的配置选项，例：</p> 
    <ul> 
     <li>OS： Windows</li> 
     <li>Package Manager：conda</li> 
     <li>Language：Python3.7</li> 
     <li>CUDA：10.0</li> 
    </ul> </li> 
   <li> <p>然后运行生成的命令：</p> <pre><code class="prism language-shell"><span class="token operator">&gt;</span> conda <span class="token function">install</span> pytorch torchvision cudatoolkit<span class="token operator">=</span>10.0 -c pytorch
</code></pre> </li> 
  </ol> 
  <p>验证PyTorch的安装：</p> 
  <ol> 
   <li>To use PyTorch we <code>import torch</code>.</li> 
   <li>To check the vision, we use <code>torch.__version__</code></li> 
  </ol> 
  <p>验证GPU的功能：</p> 
  <pre><code class="prism language-shell"><span class="token operator">&gt;</span> torch.cuda.is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
True

<span class="token operator">&gt;</span> torch.version.cuda
<span class="token string">'10.0'</span>
</code></pre> 
  <h4><a id="3_PyTorch_45"></a>3. PyTorch的基础概念</h4> 
  <p>分点陈述：</p> 
  <ul> 
   <li>Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。Tensor和numpy的数组间的互相操作非常容易且快速（他们共享内存）。就算numpy中有些操作在tensor中无法实现，也可以通过先转为numpy数组处理，之后再转回tensor。</li> 
   <li>PyTorch是一个灵活的深度学习框架，它允许通过动态神经网络（即if条件语句和while循环语句那样利用动态控制流的网络）自动分化。它支持GPU加速、分布式训练、多种优化以及更多的、更简洁的特性。</li> 
  </ul> 
  <h4><a id="4__52"></a>4. 通过代码实现流程（实现一个深度学习的代码流程）</h4> 
  <blockquote> 
   <p><a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html" rel="nofollow">本例取自PyTorch官网</a></p> 
  </blockquote> 
  <p>在这里，我们使用PyTorch的Tensors将双层神经网络与随机数据进行匹配，我们需要手动实现网络中的前向和后向传播。</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> torch

dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span>
device <span class="token operator">=</span> touch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token comment"># device = touch.device("cuda:0") #当运行在GPU时，要加上这行代码</span>

<span class="token comment"># N是batch size；D_in是输入的维度</span>
<span class="token comment"># H是隐藏层的维度；D_out是输出的维度</span>
N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> D_out <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span>

<span class="token comment"># 构造随机生成的输入和输出数据</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_out<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>

<span class="token comment"># 网络权重随机初始化</span>
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>

<span class="token comment"># 学习率</span>
learing_rate <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 前向传播：计算出预测的y</span>
    h <span class="token operator">=</span> x<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w1<span class="token punctuation">)</span>
    h_relu <span class="token operator">=</span> h<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    y_pred <span class="token operator">=</span> h_relu<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">)</span>
    
    <span class="token comment"># 计算并打印损失</span>
    loss <span class="token operator">=</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> loss<span class="token punctuation">)</span>
    
    <span class="token comment"># 后向传播，计算出w1和w2相对于损耗的梯度</span>
    grad_y_pred <span class="token operator">=</span> <span class="token number">2.0</span><span class="token operator">*</span><span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span>
    grad_w2 <span class="token operator">=</span> h_relu<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>grad_y_pred<span class="token punctuation">)</span>
    grad_h_relu <span class="token operator">=</span> grad_y_pred<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    grad_h <span class="token operator">=</span> grad_h_relu<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
    grad_h<span class="token punctuation">[</span>h <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
    grad_w1 <span class="token operator">=</span> x<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>grad_h<span class="token punctuation">)</span>
    
    <span class="token comment"># 使用梯度下降更新权重</span>
    w1 <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad_w1
    w2 <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad_w2
</code></pre> 
  <p>以上示例只是一个小的两层的网络，因此手动实现后向传播并不是一个困难的事情，但是对于一个大型且复杂的网络，这将不再是一个轻松的工作。PyTorch可以借用Autograd轻松解决这个问题。当使用autograd时，我们将定义一个计算图，图中的结点是tensors，图中的边是产生从输入张量到输出张量的函数。通过此图进行反向传播，我们可以轻松计算梯度。如果x是一个张量，并且x.requires_grad=True，那么x.grad就是另一个张量（ <code>x.grad</code> is another Tensor holding the gradient of <code>x</code> with respect to some scalar value ）。</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> torch

dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token comment"># device = touch.device("cuda:0") #当运行在GPU时，要加上这行代码</span>

<span class="token comment"># N是batch size；D_in是输入的维度</span>
<span class="token comment"># H是隐藏层的维度；D_out是输出的维度</span>
N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> D_out <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span>

<span class="token comment"># 构造随机生成的输入和输出数据</span>
<span class="token comment"># 设置requires_grad=False，表明在后向传播时，我们不需要计算关于这些张量的梯度</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>

<span class="token comment"># 网络权重随机初始化</span>
<span class="token comment"># 设置requires_grad=True，表明在后向传播时，我们需要计算关于这些张量的梯度</span>
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 学习率</span>
learing_rate <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 前向传播：这些与我们使用tensors计算正向传播的操作完全相同，但是我们不需要</span>
    <span class="token comment"># 保留对中间值的引用，因为我们没有手动实现反向传播。</span>
    y_pred <span class="token operator">=</span> x<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">)</span>
    
    <span class="token comment"># 使用tensor操作计算并打印损失</span>
    <span class="token comment"># 现在loss是一个shape(1,)类型的张量</span>
    <span class="token comment"># loss.item()可以获取loss中保存的标量值</span>
    loss <span class="token operator">=</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 使用autograd来计算后向传播；这个调用将计算相对于所有requires_grad=True的</span>
    <span class="token comment"># 张量的损失的梯度。</span>
    <span class="token comment"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span>
    <span class="token comment"># of the loss with respect to w1 and w2 respectively.</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 使用梯度下降手动更新权重。之所以封装于torch.no_grad()，是因为权重的</span>
    <span class="token comment"># requires_grad=True，而我们在自动微分中没有必要跟踪这些；</span>
    <span class="token comment"># 另一种可选择的操作是operate on weight.data and weight.grad.data.</span>
    <span class="token comment"># You can also use torch.optim.SGD to achieve this.</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        w1 <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w1<span class="token punctuation">.</span>grad
        w2 <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w2<span class="token punctuation">.</span>grad
        
        <span class="token comment"># 更新权重之后，手动将梯度设置为0</span>
        w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
</code></pre> 
  <p>PyTorch: 定义新的autograd函数</p> 
  <p>Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The <strong>forward</strong> function computes output Tensors from input Tensors. The <strong>backward</strong> function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.</p> 
  <p>In PyTorch we can easily define our own autograd operator by defining a subclass of <code>torch.autograd.Function</code> and implementing the <code>forward</code> and <code>backward</code> functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.</p> 
  <p>In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" 我们可以实现自定义（定制的）autograd函数，通过继承torch.autograd.Function并实现运行 Tensors上的前向和后向传播。 """</span>
    
    @<span class="token builtin">staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" 前向传播里，我们接收到一个包含输入的张量并输出一个包含输出的张量。ctx是一个上下文 对象，可用于存储信息以进行反向计算。You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. """</span>
        ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    
    @<span class="token builtin">staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""" 后向传播里，我们接收到一个包含关于输出的损失的梯度的张量，并且我们需要计算关于输入 的损失的梯度 """</span>"
        <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors
        grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
        grad_input<span class="token punctuation">[</span><span class="token builtin">input</span><span class="token operator">&lt;</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">return</span> grad_input

dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
<span class="token comment"># device = touch.device("cuda:0") #当运行在GPU时，要加上这行代码</span>

<span class="token comment"># N是batch size；D_in是输入的维度</span>
<span class="token comment"># H是隐藏层的维度；D_out是输出的维度</span>
N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> D_out <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span>

<span class="token comment"># 构造随机生成的输入和输出数据</span>
<span class="token comment"># 设置requires_grad=False，表明在后向传播时，我们不需要计算关于这些张量的梯度</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">)</span>

<span class="token comment"># 网络权重随机初始化</span>
<span class="token comment"># 设置requires_grad=True，表明在后向传播时，我们需要计算关于这些张量的梯度</span>
w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 学习率</span>
learing_rate <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 为了使用我们定义的函数，我们使用Function.apply函数。</span>
    relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span><span class="token builtin">apply</span>
    
    <span class="token comment"># 前向传播：compute predicted y using operations; we compute</span>
    <span class="token comment"># ReLU using our custom autograd operation.</span>
    y_pred <span class="token operator">=</span> relu<span class="token punctuation">(</span>x<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">)</span>
    
    <span class="token comment"># 计算并打印损失</span>
    loss <span class="token operator">=</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 使用autograd计算后向传播</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 使用梯度下降更新权值</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       w1 <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w1<span class="token punctuation">.</span>grad
       w2 <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w2<span class="token punctuation">.</span>grad
        
       <span class="token comment"># 更新权重之后，手动将梯度设置为0</span>
       w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
       w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
