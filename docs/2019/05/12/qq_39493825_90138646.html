<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>文献总结《Multimodal Gesture Recognition Using 3-D Convolution and Convolutional LSTM》 « NotBeCN</title>
  <meta name="description" content="                  《Learning Spatiotemporal Features with 3D Convolutional Networks》   更多内容可访问我的主页。(https://wangpei.ink/)   原文链接在这里 pdf   这是G. Zhu等人于2017年发表于I...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_39493825_90138646.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">文献总结《Multimodal Gesture Recognition Using 3-D Convolution and Convolutional LSTM》</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h2><a id="Learning_Spatiotemporal_Features_with_3D_Convolutional_Networks_0"></a>《Learning Spatiotemporal Features with 3D Convolutional Networks》</h2> 
  <p>更多内容可访问我的<a href="https://wangpei.ink/" rel="nofollow">主页</a>。(<a href="https://wangpei.ink/" rel="nofollow">https://wangpei.ink/</a>)</p> 
  <p><a href="https://ieeexplore.ieee.org/abstract/document/7880648" rel="nofollow">原文链接</a>在这里 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7880648" rel="nofollow">pdf</a></p> 
  <p>这是G. Zhu等人于2017年发表于IEEE ACCESS的一篇关于深度学习用于手势识别的文献，提出了3D卷积神经网络与卷积LSTM的结合使用，进行时空特征提取。</p> 
  <h3><a id="1	_8"></a>1. 主要内容：</h3> 
  <ul> 
   <li>提出了一个多模手势识别，基于3D卷积以及conv LSTM；</li> 
   <li>首先用3D卷积学习短期时空特征；</li> 
   <li>然后时空卷积LSTM学习长期时空特征，基于先前提取出的短期特征；</li> 
   <li>分别在IsoGD和SKIG大规模数据集上取得了最优的成绩</li> 
  </ul> 
  <h3><a id="2	_14"></a>2. 介绍：</h3> 
  <p>手势识别旨在识别和理解人体的有意义的运动，有效的手势识别仍然是一个非常具有挑战性的问题，部分原因是文化差异，各种观察环境，噪声，图像中手指相对较小的尺寸，词汇外动作等。</p> 
  <p><strong>传统方法</strong>：隐马尔可夫模型，粒子滤波，有限状态机和连接模型。<br> 由于上述具有挑战性的因素，手工制作的特征不能完全满足实际手势识别系统的要求。</p> 
  <p><strong>思路</strong>：手势识别通常基于视频或图像序列–&gt;时间信息在手势识别过程中起着关键作用–&gt;复杂的背景会给手势识别带来更多挑战–&gt;同时同步学习时空特征将为手势识别提供更多信息。</p> 
  <ul> 
   <li>双流卷积网络分别从RGB和叠加的光流图像中提取空间和时间特征。</li> 
   <li>LRCN首先从每帧中学习空间特征，然后使用RNN基于空间特征序列学习时间特征。</li> 
   <li>VideoLSTM使用卷积LSTM网络从先前提取的二维空间特征中学习时空特征。</li> 
  </ul> 
  <p>这三种代表性的方法<strong>分别</strong>或<strong>不同阶段</strong>地学习时空特征。而本文提出的方法则是<strong>同步</strong>的。</p> 
  <ol> 
   <li>首先，利用3D CNN从输入视频中提取短时空特征。</li> 
   <li>然后利用conv LSTM进一步学习长时空特征。</li> 
   <li>最后，用SPP来规范最终分类的时空特征。</li> 
  </ol> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512113652496.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NDkzODI1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>本文<strong>主要贡献</strong>：</p> 
  <ul> 
   <li>针对手势识别，提出基于3D CNN和卷积LSTM的方法；</li> 
   <li>对多模式数据之间的微调进行评估，并将其视为可选技巧，以防止在没有预先训练的模在时出现过拟合；</li> 
   <li>在IsoGD和SKIG数据集中展现较好的性能；</li> 
  </ul> 
  <h3><a id="3__41"></a>3. 相关研究：</h3> 
  <p>  A. 基于人工提取特征的方法<br>   B. 基于神经网络的方法</p> 
  <h3><a id="4__46"></a>4. 创新点：</h3> 
  <ul> 
   <li> <p>3D-CNN</p> 
    <ol> 
     <li>参照C3D网络设计（具体看论文《learning spatiotemporal features with 3D convolutional networks》）；</li> 
     <li>使用Batch normalization 加速训练；</li> 
     <li>3D-CNN组件只能学习短时空特征；</li> 
    </ol> <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512113711207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NDkzODI1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> </li> 
   <li> <p>B. Convolutional LSTM</p> 
    <ol> 
     <li> <p>传统的全连接LSTM不需要考虑空间相关性；</p> </li> 
     <li> <p>卷积LSTM（ConvLSTM）同时具有输入状态和状态转换的卷积结构，能够针对时空关系很好地建模；</p> </li> 
     <li> <p>ConvLSTM的输入X1,X2…Xt，神经元状态C1,C2,C3…Ct和隐藏层状态H1,H2,H3…Ht以及各个门(gates)都是三维张量，并且最后两维是空间维度；</p> <p><strong>公式</strong>：</p> <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512113730691.png" alt="在这里插入图片描述"><br> 这里的*表示卷积。</p> </li> 
    </ol> </li> 
   <li> <p>Spatial Pyramid Pooling</p> 
    <ol> 
     <li>3-D CNN仅在空间域上以小的4的比例缩小图像，并且ConvLSTM组件不改变特征图的空间大小</li> 
     <li>空间金字塔池SPP插入ConvLSTM和全连接（FC）层之间以降低维度</li> 
    </ol> </li> 
   <li> <p>Multimodal Fusion(多模式融合)</p> 
    <ol> 
     <li>采用后期多模态融合，并通过平均值融合不同网络的预测得到最终的预测分数。</li> 
    </ol> </li> 
  </ul> 
  <h3><a id="5__73"></a>5. 网络结构：</h3> 
  <p>先进行输入预处理：方法一：将每个手势序列分成具有固定长度的剪辑，但是一个剪辑不能表示整个手势。方法二：将每个手势序列下采样到固定长度L中。</p> 
  <p>使用方法二，同时，采用时间抖动策 略的均匀采样来增强数据集。</p> 
  <p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          I
         </mi>
         <mi>
          d
         </mi>
         <msub>
          <mi>
           x
          </mi>
          <mi>
           i
          </mi>
         </msub>
         <mo>
          =
         </mo>
         <mfrac>
          <mi>
           S
          </mi>
          <mi>
           L
          </mi>
         </mfrac>
         <mo>
          ∗
         </mo>
         <mo>
          (
         </mo>
         <mi>
          i
         </mi>
         <mo>
          +
         </mo>
         <mi>
          j
         </mi>
         <mi>
          i
         </mi>
         <mi>
          t
         </mi>
         <mi mathvariant="normal">
          /
         </mi>
         <mn>
          2
         </mn>
         <mo>
          )
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         Idx_i= \frac{S}{L}*(i+jit/2)
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathit" style="margin-right: 0.07847em;">I</span><span class="mord mathit">d</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.21733em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.872331em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">L</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.05764em;">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathit">i</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.05724em;">j</span><span class="mord mathit">i</span><span class="mord mathit">t</span><span class="mord">/</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></span></p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512113913208.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NDkzODI1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="6__85"></a>6. 实验设置：</h3> 
  <ul> 
   <li>无预训练，从零开始；</li> 
   <li>Batch normalization加速训练，使用更高学习率且需要更少的时间；</li> 
   <li>初始学习率设置为0.1，并且每15,000次迭代降至1/10；</li> 
   <li>权重衰减初始化为0.004，并在40,000次迭代后减小至0.00004;</li> 
   <li>使用IsoGD，需要60,000次迭代；</li> 
   <li>基于IsoGD的预训练模型，针对SKIG进行了微调；</li> 
   <li>Batch Size大小为13；</li> 
   <li>每个剪辑的时间长度为32帧，每个图像的裁剪大小为112；</li> 
  </ul> 
  <h3><a id="7__98"></a>7. 结果：</h3> 
  <p>在IsoGD上的实验结果：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512113947268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NDkzODI1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>在SKIG的结果：</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512113953527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5NDkzODI1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h3><a id="8__112"></a>8. 总结：</h3> 
  <p>  本文提出了一种基于三维卷积神经网络和卷积长短期记忆（LSTM）网络的多模式手势识别方法。结果表明，同时学习时空特征比连续或单独学习手势识别的空间和时间特征更合适。<br>   三维卷积神经网络适合学习短时空特征，而卷积LSTM网络适合长时空学习。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
