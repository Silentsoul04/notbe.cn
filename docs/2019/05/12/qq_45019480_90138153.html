<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>机器学习中的损失函数 « NotBeCN</title>
  <meta name="description" content="                     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_45019480_90138153.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">机器学习中的损失函数</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是<strong>经验风险函数</strong>的核心部分，也是<strong>结构风险函数</strong>重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：<br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24%5Ctheta%5E*%20%3D%20%5Carg%20%5Cmin_%5Ctheta%20%5Cfrac%7B1%7D%7BN%7D%7B%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20L%28y_i%2C%20f%28x_i%3B%20%5Ctheta%29%20+%20%5Clambda%5C%20%5CPhi%28%5Ctheta%29%24%24" rel="nofollow" target="_blank"><img alt="$$\theta^* = \arg \min_\theta \frac{1}{N}{}\sum_{i=1}^{N} L(y_i, f(x_i; \theta) + \lambda\&nbsp; \Phi(\theta)$$" src="http://latex.codecogs.com/gif.latex?%24%24%5Ctheta%5E*%20%3D%20%5Carg%20%5Cmin_%5Ctheta%20%5Cfrac%7B1%7D%7BN%7D%7B%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20L%28y_i%2C%20f%28x_i%3B%20%5Ctheta%29%20+%20%5Clambda%5C%20%5CPhi%28%5Ctheta%29%24%24"></a></p>
    <a id="more" target="_blank"></a>
    <span></span>
    <p class="has-jax">其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的<span class="MathJax_Preview"></span>是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是<span><span class="has-jax">找到使目标函数最小时的<span class="MathJax_Preview"></span>值</span></span>。下面主要列出几种常见的损失函数。</p>
   </div>
   <h2 id="一、log对数损失函数（逻辑回归）"><a></a><a target="_blank"></a>一、log对数损失函数（逻辑回归）</h2>
   <p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从<span><strong>伯努利分布（0-1分布）</strong></span>，然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：<span><strong>最小化负的似然函数（即max F(y, f(x)) —-&gt; min -F(y, f(x)))</strong></span>。从损失函数的视角来看，它就成了log损失函数了。</p>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p class="has-jax"><strong>log损失函数的标准形式</strong>：<br><span class="MathJax_Preview"></span></p>
   <div class="MathJax_Display"></div>
   <br>刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，
   <span><strong>就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大</strong></span>）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。
   <br>逻辑回归的P(Y=y|x)表达式如下：
   <br>
   <span class="MathJax_Preview"></span>
   <div class="MathJax_Display"></div>
   <br>将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：
   <br>
   <span class="MathJax_Preview"></span>
   <div class="MathJax_Display"></div>
   <br>逻辑回归最后得到的目标式子如下：
   <p></p>
   <p><a class="fancybox" href="http://latex.codecogs.com/gif.latex?J%28%5Ctheta%29%20%3D%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Cleft%20%5B%20%5Csum_%7Bi%3D1%7D%5Em%20y%5E%7B%28i%29%7D%20%5Clog%20h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29%20+%20%281-y%5E%7B%28i%29%7D%29%20%5Clog%281-h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29%29%20%5Cright%20%5D" rel="nofollow" target="_blank"><img alt="$$J(\theta) = - \frac{1}{m}\left [ \sum_{i=1}^m y^{(i)} \log h_{\theta}(x^{(i)}) + (1-y^{(i)}) \log(1-h_{\theta}(x^{(i)}))&nbsp; \right ]$$" src="http://latex.codecogs.com/gif.latex?J%28%5Ctheta%29%20%3D%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Cleft%20%5B%20%5Csum_%7Bi%3D1%7D%5Em%20y%5E%7B%28i%29%7D%20%5Clog%20h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29%20+%20%281-y%5E%7B%28i%29%7D%29%20%5Clog%281-h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29%29%20%5Cright%20%5D"></a></p>
   <p>如果是二分类的话，则m值等于2，如果是多分类，m就是相应的类别总个数。这里需要解释一下：<span><strong>之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉</strong></span>。</p>
   <p>这里有个PDF可以参考一下：<a href="https://www.cs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf" rel="nofollow" target="_blank">Lecture 6: logistic regression.pdf</a>.</p>
   <h2 id="二、平方损失函数（最小二乘法,_Ordinary_Least_Squares_）"><a></a><a target="_blank"></a>二、平方损失函数（最小二乘法, Ordinary Least Squares ）</h2>
   <p>最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是<strong>中心极限定理</strong>，可以参考<a href="https://en.wikipedia.org/wiki/Central_limit_theorem" rel="nofollow" target="_blank">【central limit theorem】</a>），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：<span><strong>最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小</strong></span>。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
   <ul>
    <li>简单，计算方便；</li>
    <li>欧氏距离是一种很好的相似性度量标准；</li>
    <li>在不同的表示域变换后特征性质不变。</li>
   </ul>
   <p class="has-jax"><strong>平方损失（Square loss）的标准形式如下：</strong><br><span class="MathJax_Preview"></span></p>
   <div class="MathJax_Display"></div>
   <br>当样本个数为n时，此时的损失函数变为：
   <br>
   <a class="fancybox" href="http://latex.codecogs.com/gif.latex?L%28Y%2C%20f%28X%29%29%20%3D%20%5Csum%20_%7Bi%3D1%7D%5E%7Bn%7D%28Y%20-%20f%28X%29%29%5E2" rel="nofollow" target="_blank"><img alt="$$L(Y, f(X)) = \sum _{i=1}^{n}(Y - f(X))^2$$" src="http://latex.codecogs.com/gif.latex?L%28Y%2C%20f%28X%29%29%20%3D%20%5Csum%20_%7Bi%3D1%7D%5E%7Bn%7D%28Y%20-%20f%28X%29%29%5E2"></a>
   <br>
   <code>Y-f(X)</code>表示的是残差，整个式子表示的是
   <span><strong>残差的平方和</strong></span>，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。
   <p></p>
   <p class="has-jax">而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：<br><span class="MathJax_Preview"></span></p>
   <div class="MathJax_Display"></div>
   <br>上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数
   <span class="MathJax_Preview"></span>的线性函数。在机器学习中，通常指的都是后一种情况。
   <p></p>
   <h2 id="三、指数损失函数（Adaboost）"><a></a><a target="_blank"></a>三、指数损失函数（Adaboost）</h2>
   <p class="has-jax">学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到<span class="MathJax_Preview"></span>:</p>
   <p><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24f_m%20%28x%29%20%3D%20f_%7Bm-1%7D%28x%29%20+%20%5Calpha_m%20G_m%28x%29%24%24" rel="nofollow" target="_blank"><img alt="$$f_m (x) = f_{m-1}(x) + \alpha_m G_m(x)$$" src="http://latex.codecogs.com/gif.latex?%24%24f_m%20%28x%29%20%3D%20f_%7Bm-1%7D%28x%29%20+%20%5Calpha_m%20G_m%28x%29%24%24"></a></p>
   <p class="has-jax">Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数<span class="MathJax_Preview"></span>&nbsp;和G：</p>
   <p><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24%5Carg%20%5Cmin_%7B%5Calpha%2C%20G%7D%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20exp%5B-y_%7Bi%7D%20%28f_%7Bm-1%7D%28x_i%29%20+%20%5Calpha%20G%28x_%7Bi%7D%29%29%5D%24%24" rel="nofollow" target="_blank"><img alt="$$\arg \min_{\alpha, G} = \sum_{i=1}^{N} exp[-y_{i} (f_{m-1}(x_i) + \alpha G(x_{i}))]$$" src="http://latex.codecogs.com/gif.latex?%24%24%5Carg%20%5Cmin_%7B%5Calpha%2C%20G%7D%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20exp%5B-y_%7Bi%7D%20%28f_%7Bm-1%7D%28x_i%29%20+%20%5Calpha%20G%28x_%7Bi%7D%29%29%5D%24%24"></a></p>
   <p><strong>而指数损失函数(exp-loss）的标准形式如下</strong></p>
   <p><a class="fancybox" href="http://latex.codecogs.com/gif.latex?L%28y%2C%20f%28x%29%29%20%3D%20%5Cexp%5B-yf%28x%29%5D" rel="nofollow" target="_blank"><img alt="$$L(y, f(x)) = \exp[-yf(x)]$$" src="http://latex.codecogs.com/gif.latex?L%28y%2C%20f%28x%29%29%20%3D%20%5Cexp%5B-yf%28x%29%5D"></a></p>
   <p>可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：</p>
   <p><a class="fancybox" href="http://latex.codecogs.com/gif.latex?L%28y%2C%20f%28x%29%29%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cexp%5B-y_if%28x_i%29%5D" rel="nofollow" target="_blank"><img alt="L(y, f(x)) = \frac{1}{n}\sum_{i=1}^{n}\exp[-y_if(x_i)]" src="http://latex.codecogs.com/gif.latex?L%28y%2C%20f%28x%29%29%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cexp%5B-y_if%28x_i%29%5D"></a></p>
   <p>关于Adaboost的推导，可以参考Wikipedia：<a href="https://en.wikipedia.org/wiki/AdaBoost" rel="nofollow" target="_blank">AdaBoost</a>或者《统计学习方法》P145.</p>
   <h2 id="四、Hinge损失函数（SVM）"><a></a><a target="_blank"></a>四、Hinge损失函数（SVM）</h2>
   <p class="has-jax">在机器学习算法中，hinge损失函数和SVM是息息相关的。在<strong>线性支持向量机</strong>中，最优化问题可以等价于下列式子：<br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24%5Cmin_%7Bw%2Cb%7D%20%5C%20%5Csum_%7Bi%7D%5E%7BN%7D%20%5B1%20-%20y_i%28w%5Ccdot%20x_i%20+%20b%29%5D_%7B+%7D%20+%20%5Clambda%7C%7Cw%7C%7C%5E2%20%24%24" rel="nofollow" target="_blank"><img alt="$$\min_{w,b}&nbsp; \ \sum_{i}^{N} [1 - y_i(w\cdot x_i + b)]_{+} + \lambda||w||^2 $$" src="http://latex.codecogs.com/gif.latex?%24%24%5Cmin_%7Bw%2Cb%7D%20%5C%20%5Csum_%7Bi%7D%5E%7BN%7D%20%5B1%20-%20y_i%28w%5Ccdot%20x_i%20+%20b%29%5D_%7B+%7D%20+%20%5Clambda%7C%7Cw%7C%7C%5E2%20%24%24"></a><br>下面来对式子做个变形，令：<br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24%5B1%20-%20y_i%28w%5Ccdot%20x_i%20+%20b%29%5D_%7B+%7D%20%3D%20%5Cxi_%7Bi%7D%24%24" rel="nofollow" target="_blank"><img alt="$$[1 - y_i(w\cdot x_i + b)]_{+} = \xi_{i}$$" src="http://latex.codecogs.com/gif.latex?%24%24%5B1%20-%20y_i%28w%5Ccdot%20x_i%20+%20b%29%5D_%7B+%7D%20%3D%20%5Cxi_%7Bi%7D%24%24"></a><br>于是，原式就变成了：<br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24%5Cmin_%7Bw%2Cb%7D%20%5C%20%5Csum_%7Bi%7D%5E%7BN%7D%20%5Cxi_i%20+%20%5Clambda%7C%7Cw%7C%7C%5E2%20%24%24" rel="nofollow" target="_blank"><img alt="$$\min_{w,b}&nbsp; \ \sum_{i}^{N} \xi_i + \lambda||w||^2 $$" src="http://latex.codecogs.com/gif.latex?%24%24%5Cmin_%7Bw%2Cb%7D%20%5C%20%5Csum_%7Bi%7D%5E%7BN%7D%20%5Cxi_i%20+%20%5Clambda%7C%7Cw%7C%7C%5E2%20%24%24"></a><br>如若取<span class="MathJax_Preview"></span>，式子就可以表示成：<br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%24%24%5Cmin_%7Bw%2Cb%7D%20%5Cfrac%7B1%7D%7BC%7D%5Cleft%20%28%20%5Cfrac%7B1%7D%7B2%7D%5C%20%7C%7Cw%7C%7C%5E2%20%24%24%20+%20C%20%5Csum_%7Bi%7D%5E%7BN%7D%20%5Cxi_i%5Cright%20%29%24%24" rel="nofollow" target="_blank"><img alt="$$\min_{w,b}&nbsp; \frac{1}{C}\left ( \frac{1}{2}\ ||w||^2 $$ + C \sum_{i}^{N} \xi_i\right )$$" src="http://latex.codecogs.com/gif.latex?%24%24%5Cmin_%7Bw%2Cb%7D%20%5Cfrac%7B1%7D%7BC%7D%5Cleft%20%28%20%5Cfrac%7B1%7D%7B2%7D%5C%20%7C%7Cw%7C%7C%5E2%20%24%24%20+%20C%20%5Csum_%7Bi%7D%5E%7BN%7D%20%5Cxi_i%5Cright%20%29%24%24"></a><br>可以看出，该式子与下式非常相似：<br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20l%28w%20%5Ccdot%20x_i%20+%20b%2C%20y_i%29%20+%20%7C%7Cw%7C%7C%5E2" rel="nofollow" target="_blank"><img alt="$$\frac{1}{m} \sum_{i=1}^{m} l(w \cdot&nbsp; x_i + b, y_i) + ||w||^2$$" src="http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%20l%28w%20%5Ccdot%20x_i%20+%20b%2C%20y_i%29%20+%20%7C%7Cw%7C%7C%5E2"></a></p>
   <p class="has-jax">前半部分中的<span class="MathJax_Preview"></span>就是hinge损失函数，而后面相当于L2正则项。</p>
   <p class="has-jax"><strong>Hinge 损失函数的标准形式</strong><br><span class="MathJax_Preview"></span></p>
   <div class="MathJax_Display"></div>
   <br>可以看出，当|y|&gt;=1时，L(y)=0。
   <p></p>
   <p>更多内容，参考<a href="https://en.wikipedia.org/wiki/Hinge_loss" rel="nofollow" target="_blank">Hinge-loss</a>。</p>
   <p>补充一下：在libsvm中一共有4中核函数可以选择，对应的是<code>-t</code>参数分别是：</p>
   <ul>
    <li>0-线性核；</li>
    <li>1-多项式核；</li>
    <li>2-RBF核；</li>
    <li>3-sigmoid核。</li>
   </ul>
   <h2 id="五、其它损失函数"><a></a><a target="_blank"></a>五、其它损失函数</h2>
   <p>除了以上这几种损失函数，常用的还有：</p>
   <p><strong>0-1损失函数</strong><br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?L%28Y%2C%20f%28X%29%29%20%3D%20%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D1%20%2C%26%20Y%20%5Cneq%20f%28X%29%5C%5C%200%20%2C%26%20y%20%3D%20f%28X%29%20%5Cend%7Bmatrix%7D%5Cright." rel="nofollow" target="_blank"><img alt="L(Y, f(X)) = \left\{\begin{matrix}1 ,&amp; Y \neq f(X)\\ 0 ,&amp; y = f(X)&nbsp;&nbsp;&nbsp; \end{matrix}\right." src="http://latex.codecogs.com/gif.latex?L%28Y%2C%20f%28X%29%29%20%3D%20%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D1%20%2C%26%20Y%20%5Cneq%20f%28X%29%5C%5C%200%20%2C%26%20y%20%3D%20f%28X%29%20%5Cend%7Bmatrix%7D%5Cright."></a><br><strong>绝对值损失函数</strong><br><a class="fancybox" href="http://latex.codecogs.com/gif.latex?L%28Y%2C%20f%28X%29%29%20%3D%20%7CY-f%28X%29%7C" rel="nofollow" target="_blank"><img alt="$$L(Y, f(X)) = |Y-f(X)|$$" src="http://latex.codecogs.com/gif.latex?L%28Y%2C%20f%28X%29%29%20%3D%20%7CY-f%28X%29%7C"></a><br>下面来看看几种损失函数的可视化图像，对着图看看横坐标，看看纵坐标，再看看每条线都表示什么损失函数，多看几次好好消化消化。<br><a class="fancybox" href="http://www.csuldw.com/assets/articleImg/4DFDU.png" rel="nofollow" target="_blank"><img alt="" src="http://www.csuldw.com/assets/articleImg/4DFDU.png"></a><br>OK，暂时先写到这里，休息下。最后，需要记住的是：<span><strong>参数越多，模型越复杂，而越复杂的模型越容易过拟合</strong></span>。过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。</p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
