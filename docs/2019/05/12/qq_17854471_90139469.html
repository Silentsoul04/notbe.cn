<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>文本主题模型之LDA在搜狐新闻数据集上的实践 « NotBeCN</title>
  <meta name="description" content="                  LDA用于文本的主题提取，关于它的理论知识看了很多，现在想在python环境下做一个实践。实践的数据集，英文的主要是希拉里的邮件数据集： 准备工作需要： 1、搭建python 环境 2、pip install gensim 3、安装nltk语言包 4、下载希拉里邮件数据集文件：...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_17854471_90139469.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">文本主题模型之LDA在搜狐新闻数据集上的实践</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p>LDA用于文本的主题提取，关于它的理论知识看了很多，现在想在python环境下做一个实践。实践的数据集，英文的主要是希拉里的邮件数据集：<br> 准备工作需要：<br> 1、搭建python 环境<br> 2、pip install gensim<br> 3、安装nltk语言包<br> 4、下载希拉里邮件数据集文件：HillaryEmails.csv<br> 有币的同学可以在csdn里面找到。</p> 
  <pre><code>#coding=utf8
import numpy as np
import pandas as pd
import re
from gensim import corpora, models, similarities
import gensim
from nltk.corpus import stopwords

df = pd.read_csv("./input/HillaryEmails.csv")
df = df[[‘Id‘, ‘ExtractedBodyText‘]].dropna()

def clean_email_text(text):
    text = text.replace(‘\n‘," ") #新行，我们是不需要的
    text = re.sub(r"-", " ", text) #把 "-" 的两个单词，分开。（比如：july-edu ==&gt; july edu）
    text = re.sub(r"\d+/\d+/\d+", "", text) #日期，对主体模型没什么意义
    text = re.sub(r"[0-2]?[0-9]:[0-6][0-9]", "", text) #时间，没意义
    text = re.sub(r"[\w]+@[\.\w]+", "", text) #邮件地址，没意义
    text = re.sub(r"/[a-zA-Z]*[:\//\]*[A-Za-z0-9\-_]+\.+[A-Za-z0-9\.\/%&amp;=\?\-_]+/i", "", text) #网址，没意义
    pure_text = ‘‘
    # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉
    for letter in text:
        # 只留下字母和空格
        if letter.isalpha() or letter==‘ ‘:
            pure_text += letter
    # 再把那些去除特殊字符后落单的单词，直接排除。
    # 我们就只剩下有意义的单词了。
    text = ‘ ‘.join(word for word in pure_text.split() if len(word)&gt;1)
    return text

docs = df[‘ExtractedBodyText‘]
docs = docs.apply(lambda s: clean_email_text(s))
doclist = docs.values
stopwords = set(stopwords.words(‘english‘))

texts = [[word for word in doc.lower().split() if word not in stopwords] for doc in doclist]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)


print lda.print_topics(num_topics=20, num_words=5)
</code></pre> 
  <p>但是对于国内的应用来说，还是处理中文文本的情况占大多数，所以我刚开始在网易新闻、和讯网上自己做了一个文件集合，但是手工做还是非常费劲。无意中发现了前人整理的搜狐新闻数据集，数据量挺大的，分成了9个大类，每个大类2000个文档。<br> 下载地址：<a href="https://pan.baidu.com/s/1gg2y3Gf" rel="nofollow">https://pan.baidu.com/s/1gg2y3Gf</a> 密码：hk3y</p> 
  <p>那就用这个文档好啦。<br> 参考文章：<a href="https://www.mlln.cn/2018/02/07/gensim-lda-%E6%96%87%E6%A1%A3%E4%B8%BB%E9%A2%98%E6%8F%90%E5%8F%96%E5%AE%9E%E7%8E%B0/" rel="nofollow">https://www.mlln.cn/2018/02/07/gensim-lda-文档主题提取实现/</a></p> 
  <p>这个文章分词工具用的是 pyltp这个工具，百度了一下这个工具的安装似乎问题很多。于是我采用了熟悉的工具jieba。只要一个pip install jieba命令就安装成功了。</p> 
  <p>首先获得文件夹下的所有文档：</p> 
  <pre><code>import os
def list_all_files(rootdir):
     _files = []
    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件
    for i in range(0,len(list)):
           path = os.path.join(rootdir,list[i])
           if os.path.isdir(path):
              _files.extend(list_all_files(path))
           if os.path.isfile(path):
              _files.append(path)
    return _files
    
files=list_all_files('C:\\Users\\jack\\Desktop\\sohu_news\\SogouC.reduced\\Reduced\\C000013')
file_list=[]
</code></pre> 
  <p>定义停用词库，网上下载一个中文停用词库即可：</p> 
  <pre><code>def stop(filepath):  
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8', errors='ignore').readlines()]  
    return stopwords  

stopWords=stop("stop_words.txt")  
</code></pre> 
  <p>定义去除标点符号的正则表达式pattern：</p> 
  <pre><code>import re
pattern=re.compile(r'[-,$()#+&amp;*《》&lt;&gt;。，、：“”（）]')
</code></pre> 
  <p>下面就可以处理文档集合了，生成lda的语料集合：</p> 
  <pre><code>import numpy as np
import pandas as pd
import re
import os
import jieba
import jieba.analyse
    for f in files:
        with open(f,'rb') as fr:
            print(f)
            f_read_decode=fr.read().decode('gb2312',errors='ignore')
            content=re.sub(pattern,"",f_read_decode)
            words=jieba.cut(content,cut_all='False')
            stayed_line=""
            for word in words:
    #            if word.encode("utf-8") not in stopWords:
                if word not in stopWords:
    #                print(word)
                    stayed_line+=word+' '
            con=list(stayed_line.split(' '))

            file_list.append(con)
            file_name.append(f.split('\\')[-1])
</code></pre> 
  <p>这里文档的格式gb2312，先用rb二进制格式读取，然后用gb2312格式进行decode。<br> 直接用utf-8会读取成乱码。如何查看文档的编码格式呢。<br> 参考文章：<a href="https://www.cnblogs.com/fanyuchen/p/7151584.html" rel="nofollow">https://www.cnblogs.com/fanyuchen/p/7151584.html</a></p> 
  <pre><code>import chardet
with open("C:\\Users\\fyc\\Desktop\\json.txt", "r") as f:
        text = f.read()
        type = chardet.detect(text)  #获取编码格式
        text1 = text.decode(type["encoding"])
</code></pre> 
  <p>一上完成之后，就可以往lda模型喂数据了，数据的格式是列表，每个列表的元素的是一个文档的词语列表，相当于一个一层嵌套。</p> 
  <pre><code>from gensim import corpora,models,similarities
import gensim 
dic = corpora.Dictionary(file_list)
clean_data = [dic.doc2bow(words) for words in file_list]

lda = models.ldamodel.LdaModel(clean_data, id2word=dic,  num_topics=5)
# 查看主题
for topic in lda.print_topics(num_words=10):
    print(topic[1])
</code></pre> 
  <p>结果如下：</p> 
  <pre><code>0.023*"
" + 0.003*"治疗" + 0.003*"中" + 0.003*"做" + 0.003*"医院" + 0.003*"nbsp" + 0.003*"企业" + 0.003*"工作" + 0.003*"说" + 0.002*"健康"
0.027*"
" + 0.006*"医院" + 0.004*"中" + 0.004*"治疗" + 0.004*"nbsp" + 0.003*"工作" + 0.003*"说" + 0.003*"做" + 0.003*"中国" + 0.002*"检查"
0.039*"
" + 0.006*"nbsp" + 0.004*"中" + 0.004*"医院" + 0.003*"药品" + 0.003*"gt" + 0.003*"时" + 0.002*"治疗" + 0.002*"企业" + 0.002*"做"
0.036*"
" + 0.004*"中" + 0.004*"医院" + 0.004*"说" + 0.003*"治疗" + 0.003*"卫生" + 0.003*"时" + 0.003*"手术" + 0.003*"企业" + 0.002*"医疗"
0.037*"
" + 0.015*"nbsp" + 0.003*"gt" + 0.003*"治疗" + 0.003*"医院" + 0.003*"
nbsp" + 0.003*"工作" + 0.003*"时" + 0.003*"说" + 0.002*"中"
</code></pre> 
  <p>改进意见：<br> 因为我只用了其中一个健康的目录做实验，主题看起来差不多，而且有一些没有意义的词在分词之前没有处理，可以在停用词字典里面做加工。<br> 另外在分词时可以加上词性标注，在结果中只取名词 动词等词性的词。</p> 
  <p>完整代码如下：</p> 
  <pre><code># -*- coding: utf-8 -*-
"""
Created on Sat May 11 20:40:22 2019

@author: jack
"""

import numpy as np
import pandas as pd
import re
import os
import jieba
import jieba.analyse
from gensim import corpora,models,similarities
import gensim 

def stop(filepath):  
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8', errors='ignore').readlines()]  
    return stopwords  
 


jieba.analyse.set_stop_words("stop_words.txt")
jieba.load_userdict("C:\\Users\\jack\\user_dict.txt")

def list_all_files(rootdir):

    _files = []
    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件
    for i in range(0,len(list)):
           path = os.path.join(rootdir,list[i])
           if os.path.isdir(path):
              _files.extend(list_all_files(path))
           if os.path.isfile(path):
              _files.append(path)
    return _files
 
files=list_all_files('C:\\Users\\jack\\Desktop\\sohu_news\\SogouC.reduced\\Reduced')
file_list=[]
file_name=[]
pattern=re.compile(r'[-,$()#+&amp;*《》&lt;&gt;。，、：“”（）]')
#stopWords=[]
#for word in open('stop_words.txt','r'):
#    stopWords.append(word.strip())
stopWords=stop("stop_words.txt")  
print(stopWords)
 
#print(files)
for f in files:
    with open(f,'rb') as fr:
        print(f)
        f_read_decode=fr.read().decode('gb2312',errors='ignore')
        content=re.sub(pattern,"",f_read_decode)
        words=jieba.cut(content,cut_all='False')
        stayed_line=""
        for word in words:
#            if word.encode("utf-8") not in stopWords:
            if word not in stopWords:
#                print(word)
                stayed_line+=word+' '
        con=list(stayed_line.split(' '))
#        print(con)
#        print(con)
        file_list.append(con)
        file_name.append(f.split('\\')[-1])
#        print('\n')
#print(file_name)
#print(file_list[1])



dic = corpora.Dictionary(file_list)
clean_data = [dic.doc2bow(words) for words in file_list]

lda = models.ldamodel.LdaModel(clean_data, id2word=dic,  num_topics=5)
# 查看主题
for topic in lda.print_topics(num_words=10):
    print(topic[1])
</code></pre> 
  <p>全部集合的数据lda的结果如下：</p> 
  <pre><code>0.020*"
" + 0.008*"网络" + 0.008*"系统" + 0.008*"中国" + 0.007*"信息" + 0.006*"日" + 0.006*"志愿" + 0.004*"技术" + 0.004*"第一" + 0.003*"美国"
0.054*"
" + 0.006*"说" + 0.005*"中" + 0.003*"中国" + 0.003*"时" + 0.003*"nbsp" + 0.003*"训练" + 0.002*"做" + 0.002*"工作" + 0.002*""
0.024*"
" + 0.008*"大学" + 0.007*"学生" + 0.006*"工作" + 0.006*"教育" + 0.005*"专业" + 0.005*"社会" + 0.005*"学校" + 0.004*"研究" + 0.004*"中"
0.023*"
" + 0.010*"中国" + 0.009*"nbsp" + 0.007*"美国" + 0.004*"军事" + 0.004*"日本" + 0.004*"发展" + 0.004*"导弹" + 0.004*"国家" + 0.003*"日"
0.015*"
" + 0.008*"工资" + 0.006*"会计" + 0.005*"投资" + 0.005*"元" + 0.004*"日" + 0.004*"银行" + 0.004*"航空" + 0.004*"贷款" + 0.003*"劳动"
</code></pre> 
  <p>感觉还是有提升的空间。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
