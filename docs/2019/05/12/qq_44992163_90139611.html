<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>神经网络从原理到实现 « NotBeCN</title>
  <meta name="description" content="                     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_44992163_90139611.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">神经网络从原理到实现</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <h2 id="1简单介绍"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">1.简单介绍</font></strong></h2>
    <blockquote>
     &nbsp; 
     <p><font size="3">在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，<font color="#009900">用于对函数进行估计或近似</font>。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具。典型的神经网络具有以下三个部分：</font></p>
    </blockquote>
    <ol>
     <li><strong>结构 （Architecture）</strong> 结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。</li>
     <li><strong>激励函数（Activity Rule）</strong> 大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。</li>
     <li><strong>学习规则（Learning Rule）</strong>学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。</li>
    </ol>
   </div>
   <h2 id="2初识神经网络"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">2.初识神经网络</font></strong></h2>
   <p><font size="3">如上文所说，神经网络主要包括三个部分：结构、激励函数、学习规则。图1是一个三层的神经网络，输入层有d个节点，隐层有q个节点，输出层有l个节点。<font color="#009900" size="3">除了输入层，每一层的节点都包含一个非线性变换。</font></font></p>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129160017514"> 
    <br>
    <center>
     <strong>图1</strong>
    </center>
   </center>
   <p></p>
   <p><font color="#ff0000" size="3">那么为什么要进行非线性变换呢？</font></p>
   <p><font size="3">（1）如果只进行线性变换，那么即使是多层的神经网络，依然只有一层的效果。类似于0.6*(0.2x1+0.3x2)=0.12x1+0.18x2。 <br>（2）进行非线性变化，可以使得神经网络可以拟合任意一个函数,图2是一个四层网络的图。 <br></font></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129171019389"> 
    <br>
    <center>
     <strong>图2</strong>
    </center>
   </center>
   <p></p>
   <p><font color="#ff0000" size="3">下面使用数学公式描述每一个神经元工作的方式</font></p>
   <p><font size="3">（1）输出x <br>（2）计算z=w*x <br>（3）输出new_x = f(z)，这里的f是一个函数，可以是sigmoid、tanh、relu等，f就是上文所说到的<font color="#009900" size="3">激励函数</font>。</font></p>
   <h2 id="3反向传播bp算法"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">3.反向传播(bp)算法</font></strong></h2>
   <p><font size="3">有了上面的<font color="#009900" size="3">网络结构</font>和<font color="#009900" size="3">激励函数</font>之后，这个网络是如何学习参数（<font color="#009900" size="3">学习规则</font>）的呢？</font></p>
   <p><font color="#ff0000" size="3">首先我们先定义下本文使用的激活函数、目标函数</font></p>
   <p><font size="3">（1）<strong>激活函数（sigmoid）：</strong><img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129162102837"></font></p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span>+np.exp(-z))</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p><font size="3">sigmoid函数有一个十分重要的性质：<img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129162402837">，即计算导数十分方便。</font></p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_prime</span><span class="hljs-params">(z)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> sigmoid(z)*(<span class="hljs-number">1</span>-sigmoid(z))</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p><font size="3">下面给出一个简单的证明：<img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129162503823"></font></p>
   <p><font size="3">（2）<strong>目标函数（差的平方和）</strong><img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129162143396">，公式中的1/2是为了计算导数方便。</font></p>
   <p><font color="#ff0000" size="3">然后，这个网络是如何运作的</font></p>
   <p><font color="#009900" size="3">（1）数据从输入层到输出层，经过各种非线性变换的过程即前向传播。</font></p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">feedforward</span><span class="hljs-params">(self, a)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a = sigmoid(np.dot(w, a)+b)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> a</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>其中，初始的权重（w）和偏置（b）是随机赋值的</p>
   <pre class="prettyprint"><code class="hljs livecodeserver has-numbering">biases = [np.<span class="hljs-built_in">random</span>.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-operator">in</span> sizes[<span class="hljs-number">1</span>:]]weights = [np.<span class="hljs-built_in">random</span>.randn(y, x) <span class="hljs-keyword">for</span> x, y <span class="hljs-operator">in</span> zip(sizes[:-<span class="hljs-number">1</span>], sizes[<span class="hljs-number">1</span>:])]</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p><font color="#009900" size="3">（2）参数更新，即反向传播</font></p>
   <p><font size="3">在写代码之前，先进行推导，即利用梯度下降更新参数，以上面的网络结构（图1）为例</font></p>
   <p><font size="3"><strong>（1）输出层与隐层之间的参数更新</strong></font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129163937156">
   </center>
   <p></p>
   <p><font size="3"><strong>（2）隐层与输入层之间的参数更新</strong></font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161129164036876">
   </center>
   <p></p>
   <p><font size="3"><strong>有两点需要强调下：</strong></font></p>
   <ol>
    <li><font size="3">（2）中的结果比（1）中的结果多了一个求和公式，这是因为计算隐层与输入层之间的参数时，输出层与隐层的每一个节点都有影响。</font></li>
    <li><font size="3">（2）中参数更新的结果可以复用（1）中的参数更新结果，从某种程度上，与反向传播这个算法名称不谋而合，不得不惊叹。</font></li>
   </ol>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backprop</span><span class="hljs-params">(self, x, y)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""返回一个元组(nabla_b, nabla_w)代表目标函数的梯度."""</span>&nbsp;&nbsp;&nbsp; nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]&nbsp;&nbsp;&nbsp; nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># feedforward</span>&nbsp;&nbsp;&nbsp; activation = x&nbsp;&nbsp;&nbsp; activations = [x] <span class="hljs-comment"># list to store all the activations, layer by layer</span>&nbsp;&nbsp;&nbsp; zs = [] <span class="hljs-comment"># list to store all the z vectors, layer by layer</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; z = np.dot(w, activation)+b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zs.append(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; activation = sigmoid(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; activations.append(activation)&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># backward pass</span>&nbsp;&nbsp;&nbsp; delta = self.cost_derivative(activations[-<span class="hljs-number">1</span>], y) * \&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sigmoid_prime(zs[-<span class="hljs-number">1</span>])&nbsp;&nbsp;&nbsp; nabla_b[-<span class="hljs-number">1</span>] = delta&nbsp;&nbsp;&nbsp; nabla_w[-<span class="hljs-number">1</span>] = np.dot(delta, activations[-<span class="hljs-number">2</span>].transpose())&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""l = 1 表示最后一层神经元，l = 2 是倒数第二层神经元, 依此类推."""</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">2</span>, self.num_layers):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; z = zs[-l]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sp = sigmoid_prime(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delta = np.dot(self.weights[-l+<span class="hljs-number">1</span>].transpose(), delta) * sp&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_b[-l] = delta&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_w[-l] = np.dot(delta, activations[-l-<span class="hljs-number">1</span>].transpose())&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> (nabla_b, nabla_w)</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h2 id="4完整代码实现"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">4.完整代码实现</font></strong></h2>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-keyword">import</span> random<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Network</span><span class="hljs-params">(object)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, sizes)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""参数sizes表示每一层神经元的个数，如[2,3,1],表示第一层有2个神经元，第二层有3个神经元，第三层有1个神经元."""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.num_layers = len(sizes)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.sizes = sizes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.biases = [np.random.randn(y, <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> sizes[<span class="hljs-number">1</span>:]]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.weights = [np.random.randn(y, x)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> zip(sizes[:-<span class="hljs-number">1</span>], sizes[<span class="hljs-number">1</span>:])]&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">feedforward</span><span class="hljs-params">(self, a)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""前向传播"""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a = sigmoid(np.dot(w, a)+b)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> a&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">SGD</span><span class="hljs-params">(self, training_data, epochs, mini_batch_size, eta,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test_data=None)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""随机梯度下降"""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> test_data:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n_test = len(test_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n = len(training_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(epochs):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(training_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mini_batches = [&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; training_data[k:k+mini_batch_size]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">0</span>, n, mini_batch_size)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> mini_batch <span class="hljs-keyword">in</span> mini_batches:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.update_mini_batch(mini_batch, eta)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> test_data:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">"Epoch {0}: {1} / {2}"</span>.format(j, self.evaluate(test_data), n_test)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">else</span>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">"Epoch {0} complete"</span>.format(j)&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_mini_batch</span><span class="hljs-params">(self, mini_batch, eta)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""使用后向传播算法进行参数更新.mini_batch是一个元组(x, y)的列表、eta是学习速率"""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> mini_batch:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delta_nabla_b, delta_nabla_w = self.backprop(x, y)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_b = [nb+dnb <span class="hljs-keyword">for</span> nb, dnb <span class="hljs-keyword">in</span> zip(nabla_b, delta_nabla_b)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_w = [nw+dnw <span class="hljs-keyword">for</span> nw, dnw <span class="hljs-keyword">in</span> zip(nabla_w, delta_nabla_w)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.weights = [w-(eta/len(mini_batch))*nw&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> w, nw <span class="hljs-keyword">in</span> zip(self.weights, nabla_w)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.biases = [b-(eta/len(mini_batch))*nb&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b, nb <span class="hljs-keyword">in</span> zip(self.biases, nabla_b)]&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backprop</span><span class="hljs-params">(self, x, y)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""返回一个元组(nabla_b, nabla_w)代表目标函数的梯度."""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_b = [np.zeros(b.shape) <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> self.biases]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_w = [np.zeros(w.shape) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> self.weights]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># 前向传播</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; activation = x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; activations = [x] <span class="hljs-comment"># list to store all the activations, layer by layer</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zs = [] <span class="hljs-comment"># list to store all the z vectors, layer by layer</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b, w <span class="hljs-keyword">in</span> zip(self.biases, self.weights):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; z = np.dot(w, activation)+b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; zs.append(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; activation = sigmoid(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; activations.append(activation)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># backward pass</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delta = self.cost_derivative(activations[-<span class="hljs-number">1</span>], y) * sigmoid_prime(zs[-<span class="hljs-number">1</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_b[-<span class="hljs-number">1</span>] = delta&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_w[-<span class="hljs-number">1</span>] = np.dot(delta, activations[-<span class="hljs-number">2</span>].transpose())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""l = 1 表示最后一层神经元，l = 2 是倒数第二层神经元, 依此类推."""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">2</span>, self.num_layers):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; z = zs[-l]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sp = sigmoid_prime(z)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; delta = np.dot(self.weights[-l+<span class="hljs-number">1</span>].transpose(), delta) * sp&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_b[-l] = delta&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nabla_w[-l] = np.dot(delta, activations[-l-<span class="hljs-number">1</span>].transpose())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> (nabla_b, nabla_w)&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(self, test_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""返回分类正确的个数"""</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test_results = [(np.argmax(self.feedforward(x)), y) <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> test_data]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> sum(int(x == y) <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> test_results)&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost_derivative</span><span class="hljs-params">(self, output_activations, y)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> (output_activations-y)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span>/(<span class="hljs-number">1.0</span>+np.exp(-z))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid_prime</span><span class="hljs-params">(z)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""sigmoid函数的导数"""</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> sigmoid(z)*(<span class="hljs-number">1</span>-sigmoid(z))</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
     <li>38</li>
     <li>39</li>
     <li>40</li>
     <li>41</li>
     <li>42</li>
     <li>43</li>
     <li>44</li>
     <li>45</li>
     <li>46</li>
     <li>47</li>
     <li>48</li>
     <li>49</li>
     <li>50</li>
     <li>51</li>
     <li>52</li>
     <li>53</li>
     <li>54</li>
     <li>55</li>
     <li>56</li>
     <li>57</li>
     <li>58</li>
     <li>59</li>
     <li>60</li>
     <li>61</li>
     <li>62</li>
     <li>63</li>
     <li>64</li>
     <li>65</li>
     <li>66</li>
     <li>67</li>
     <li>68</li>
     <li>69</li>
     <li>70</li>
     <li>71</li>
     <li>72</li>
     <li>73</li>
     <li>74</li>
     <li>75</li>
     <li>76</li>
     <li>77</li>
     <li>78</li>
     <li>79</li>
     <li>80</li>
     <li>81</li>
     <li>82</li>
     <li>83</li>
     <li>84</li>
     <li>85</li>
     <li>86</li>
     <li>87</li>
     <li>88</li>
     <li>89</li>
     <li>90</li>
     <li>91</li>
     <li>92</li>
     <li>93</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h2 id="5简单应用"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">5.简单应用</font></strong></h2>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><span class="hljs-keyword">from</span> network <span class="hljs-keyword">import</span> *<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">vectorized_result</span><span class="hljs-params">(j,nclass)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-string">"""离散数据进行one-hot"""</span>&nbsp;&nbsp;&nbsp; e = np.zeros((nclass, <span class="hljs-number">1</span>))&nbsp;&nbsp;&nbsp; e[j] = <span class="hljs-number">1.0</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> e<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_format_data</span><span class="hljs-params">(X,y,isTest)</span>:</span>&nbsp;&nbsp;&nbsp; ndim = X.shape[<span class="hljs-number">1</span>]&nbsp;&nbsp;&nbsp; nclass = len(np.unique(y))&nbsp;&nbsp;&nbsp; inputs = [np.reshape(x, (ndim, <span class="hljs-number">1</span>)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isTest:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; results = [vectorized_result(y,nclass) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> y]&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">else</span>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; results = y&nbsp;&nbsp;&nbsp; data = zip(inputs, results)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> data<span class="hljs-comment">#随机生成数据</span><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> *np.random.seed(<span class="hljs-number">0</span>)X, y = make_moons(<span class="hljs-number">200</span>, noise=<span class="hljs-number">0.20</span>)ndim = X.shape[<span class="hljs-number">1</span>]nclass = len(np.unique(y))<span class="hljs-comment">#划分训练、测试集</span><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_splittrain_x,test_x,train_y,test_y = train_test_split(X,y,test_size=<span class="hljs-number">0.2</span>,random_state=<span class="hljs-number">0</span>)training_data = get_format_data(train_x,train_y,<span class="hljs-keyword">False</span>)test_data = get_format_data(test_x,test_y,<span class="hljs-keyword">True</span>)net = Network(sizes=[ndim,<span class="hljs-number">10</span>,nclass])net.SGD(training_data=training_data,epochs=<span class="hljs-number">5</span>,mini_batch_size=<span class="hljs-number">10</span>,eta=<span class="hljs-number">0.1</span>,test_data=test_data)</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
