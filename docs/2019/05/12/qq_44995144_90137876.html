<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>反向传播算法的直观理解 « NotBeCN</title>
  <meta name="description" content="                     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_44995144_90137876.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">反向传播算法的直观理解</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <h2><a></a><a target="_blank"></a>一、反向传播的由来</h2>在我们开始DL的研究之前，需要把ANN—人工神经元网络以及bp算法做一个简单解释。
    <br>关于ANN的结构，我不再多说，网上有大量的学习资料，主要就是搞清一些名词：
    <br>输入层/输入神经元，输出层/输出神经元，隐层/隐层神经元，权值，偏置，激活函数
    <br>
    <br>接下来我们需要知道ANN是怎么训练的，假设ANN网络已经搭建好了，在所有应用问题中（不管是网络结构，训练手段如何变化）我们的目标是不会变的，那就是网络的权值和偏置最终都变成一个最好的值，这个值可以让我们由输入可以得到理想的输出，于是问题就变成了y=f(x，w，b)（x是输入，w是权值，b为偏置，所有这些量都可以有多个，比如多个x1，x2，x3……最后f()就好比我们的网络它一定可以用一个函数来表示，我们不需要知道f(x)具体是怎样的函数，从小我们就认为只要是函数就一定要是可表示的，像f(x)=sin(x)一样，但是请摈弃这样的错误观念，我们只需要知道一系列的w和b决定了一个函数f（x），这个函数让我们由输入可以计算出合理的y）
    <br>
   </div>
   <p><br>最后的目标就变成了尝试不同的w，b值，使得最后的y=f（x）无限接近我们希望得到的值t<br></p>
   <p><br>但是这个问题依然很复杂，我们把它简化一下，让（y-t）的值尽可能的小。于是原先的问题化为了C（w，b）=（f（x，w，b）-t）取到一个尽可能小的值。这个问题不是一个困难的问题，不论函数如何复杂，如果C降低到了一个无法再降低的值，那么就取到了最小值（假设我们不考虑局部最小的情况）<br></p>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p>如何下降？数学告诉我们对于一个多变量的函数f(a,b,c,d,……)而言，我们可以求得一个向量，它称作该函数的梯度，要注意的是，梯度是一个方向向量，它表示这个函数在该点变化率最大的方向（这个定理不详细解释了，可以在高等数学教材上找到）于是C（w，b）的变化量ΔC就可以表示成<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105213621530"></p>
   <p>其中</p>
   <p><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105213727954">是该点上的微小变化，我们可以随意指定这些微小变化，只需要保证ΔC&lt;0就可以了，但是为了更快的下降，我们为何不选在梯度方向上做变化呢？</p>
   <p>事实上，梯度下降的思想就是这样考虑的，我们使得<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105213941569">从而保证C一直递减，而对于w来说只要每次更新<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105214116522">即可。<br><br>ok，到这里，似乎所有的问题都解决了，让我们重新整理一下思绪，我们将问题转化了很多步：<br>网络权值偏置更新问题 ==&gt; f（x，w，b）的结果逼近t ==&gt; C（w，b）=（f（x，w，b）-t）^2取极小值问题 ==&gt; C（w，b）按梯度下降问题 ==&gt;取到极小值，网络达到最优<br><br>千万别忘了一点！！推导基于一个前提：我们已经提前知道了当前点的梯度。然而事实并非如此！！<br>这个问题困扰了NN研究者多年，1969年M.Minsky和S.Papert所著的《感知机》一书出版，它对单层神经网络进行了深入分析，并且从数学上证明了这种网络功能有限，甚至不能解决象"异或"这样的简单逻辑运算问题。同时，他们还发现有许多模式是不能用单层网络训练的，而对于多层网络则没有行之有效的低复杂度算法，最后他们甚至认为神经元网络无法处理非线性问题。然而于1974年，Paul Werbos首次给出了如何训练一般网络的学习算法—back propagation。这个算法可以高效的计算每一次迭代过程中的梯度，让以上我们的推导得以实现！！<br>不巧的是，在当时整个人工神经网络社群中无人知晓Paul所提出的学习算法。直到80年代中期，BP算法才重新被David Rumelhart、Geoffrey Hinton及Ronald Williams、David Parker和Yann LeCun独立发现，并获得了广泛的注意，引起了人工神经网络领域研究的第二次热潮。<br></p>
   <p></p>
   <h2><a></a><a target="_blank"></a>二、原理的引入</h2>上面已经提到，所谓反向传播，就是计算梯度的方法。对于反向传播，先不急着介绍它的原理，很多文章直接引入公式，反而使得我们很难去理解。这里先引入知乎上某位大神的回答。
   <br>
   <p></p>
   <p>来源：知乎<a href="https://www.zhihu.com/question/27239198?rf=24827633" rel="nofollow" target="_blank">https://www.zhihu.com/question/27239198?rf=24827633</a><br></p>
   <p><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105194353473"><br></p>
   <p>假设输入a=2，b=1，在这种情况下，我们很容易求出相邻节点之间的偏导关系</p>
   <p><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105194500114"><br></p>
   <p>利用链式法则：</p>
   <p><img alt="\frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial a}" src="https://zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+a%7D%3D%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+c%7D%5Ccdot+%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+a%7D"><span>以及</span><img alt="\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\cdot \frac{\partial d}{\partial b}" src="https://zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+b%7D%3D%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+c%7D%5Ccdot+%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+b%7D%2B%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+d%7D%5Ccdot+%5Cfrac%7B%5Cpartial+d%7D%7B%5Cpartial+b%7D"></p>
   <p><br></p>
   <p><img alt="\frac{\partial e}{\partial a}" src="http://zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+a%7D">的值等于从a到e的路径上的偏导值的乘积，而<img alt="\frac{\partial e}{\partial b}" src="http://zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e%7D%7B%5Cpartial+b%7D">的值等于从b到e的路径1(b-c-e)上的偏导值的乘积加上路径2(b-d-e)上的偏导值的乘积。也就是说，对于上层节点p和下层节点q，要求得<img alt="\frac{\partial p}{\partial q}" src="http://zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+q%7D">，需要找到从q节点到p节点的所有路径，并且对每条路径，求得该路径上的所有偏导数之乘积，然后将所有路径的 “乘积” 累加起来才能得到<img alt="\frac{\partial p}{\partial q}" src="http://zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial+q%7D">的值。</p>
   <p>这种情况下偏导很容易求得，因为我们已经知道网络的函数关系式，e=（a+b）*（b+1），这是一个没有权值干预，已知输入与输出之间关系的网络。实际当中我们只是知道e与输出之间的关系，就是上面说的C=（y-t）^2，而且会有成千上万的权值和偏置干预求导的过程。那么换个思路，能不能求输出对结果的偏导呢？</p>
   <p>再利用上图的关系。节点c对e偏导2并将结果堆放起来，节点d对e偏导3并将结果堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5。简要的概括，就是从最上层的节点e开始，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。</p>
   <h2><a></a><a target="_blank"></a>三、一个很好的例子</h2>
   <div>
    现在，我们再把权值考虑进去，以下是一个很好的例子，有助于我们去理解反向传播
   </div>
   <div>
    来源：Charlotte77的博客
    <a href="http://www.cnblogs.com/charlotte77/p/5629865.html" rel="nofollow" target="_blank">http://www.cnblogs.com/charlotte77/p/5629865.html</a>
   </div>
   <div>
    <p>假设，你有这样一个网络层：</p>
    <p><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106112715527"><br></p>
    <p>第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。</p>
    <p>　　现在对他们赋上初值，如下图：</p>
    <p><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106112636229"><br></p>
    <p>　　其中，输入数据 &nbsp;i1=0.05，i2=0.10;</p>
    <p>　　　　　输出数据 o1=0.01,o2=0.99;</p>
    <p>　　　　　初始权重 &nbsp;w1=0.15,w2=0.20,w3=0.25,w4=0.30;</p>
    <p>　　　　　　　　　 &nbsp;w5=0.40,w6=0.45,w7=0.50,w8=0.88</p>
    <p>&nbsp;</p>
    <p>　　目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。</p>
    <p>&nbsp;</p>
    <p>　　<span>Step 1 前向传播</span></p>
    <p>　　<span>1.输入层----&gt;隐含层：</span></p>
    <p>　　计算神经元h1的输入加权和：</p>
    <p><img width="366" height="82" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630142915359-294460310.png"></p>
    <p><br></p>
    <p>神经元h1的输出o1:(此处用到激活函数为sigmoid函数)：</p>
    <p><img width="369" height="55" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630150115390-1035378028.png"><br></p>
    <p><br></p>
    <p>同理，可计算出神经元h2的输出o2：</p>
    <p><img width="334" height="32" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630150244265-1128303244.png"></p>
    <p><span>2.隐含层----&gt;输出层：</span></p>
    <p>　　计算输出层神经元o1和o2的值：</p>
    <p>　　<img width="592" height="141" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630150517109-389457135.png"><img width="355" height="48" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630150638390-1210364296.png"></p>
    <p>&nbsp;</p>
    <p>这样前向传播的过程就结束了，我们得到输出值为[0.75136079 , 0.772928465]，与实际值[0.01 , 0.99]相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
    <p>&nbsp;Step 2 反向传播</p>
    <p><span>1.计算总误差</span></p>
    <p><span>总误差：(square error)</span></p>
    <p><img width="313" height="66" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630151201812-1014280864.png"></p>
    <p><span>但是有两个输出，所以分别计算o1和o2的误差，总误差为两者之和：</span><img width="606" height="58" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630151457593-1250510503.png"><img width="516" height="50" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630151508999-1967746600.png"><img width="549" height="51" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630151516093-1257166735.png"></p>
    <p>&nbsp;</p>
    <p><span>2.隐含层----&gt;输出层的权值更新：</span></p>
    <p>以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则）<img width="503" height="77" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630151916796-1001638091.png"></p>
    <p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
    <p></p>
    <div></div>
    <div></div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106112604072">
    <br>
    <p></p>
    <p>现在我们来分别计算每个式子的值：</p>
    <p>计算<img width="58" height="47" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152206781-7976168.png">：</p>
    <p><img width="523" height="140" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152258437-1960839452.png"></p>
    <p>计算<img width="64" height="53" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152417109-711077078.png">：</p>
    <p><img width="523" height="85" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152511937-1667481051.png"></p>
    <p>（这一步实际上就是对sigmoid函数求导，比较简单，可以自己推导一下）</p>
    <p>&nbsp;</p>
    <p>计算<img width="58" height="50" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152625593-2083321635.png">：</p>
    <p><img width="495" height="97" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152658109-214239362.png"></p>
    <p>最后三者相乘：</p>
    <p><img width="535" height="88" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630152811640-888140287.png"></p>
    <p>这样我们就计算出整体误差E(total)对w5的偏导值。</p>
    <p>回过头来再看看上面的公式，我们发现：</p>
    <p><img width="459" height="52" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153103187-515052589.png"></p>
    <p>为了表达方便，用<img width="19" height="25" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153202812-585186566.png">来表示输出层的误差：</p>
    <p><img width="459" height="99" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153251234-1144531293.png"></p>
    <p>因此，整体误差E(total)对w5的偏导公式可以写成：</p>
    <p><img width="461" height="60" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153405296-436656179.png"></p>
    <p>如果输出层误差计为负的话，也可以写成：</p>
    <p><img width="461" height="68" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153514734-1544628024.png"></p>
    <p>最后我们来更新w5的值：</p>
    <p><img width="667" height="54" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153614374-1624035276.png"></p>
    <p>（其中，<img width="13" height="26" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153700093-743859667.png">是学习速率，这里我们取0.5）</p>
    <p>同理，可更新w6,w7,w8:</p>
    <p><img width="592" height="169" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630153807624-1231975059.png"></p>
    <p>&nbsp;</p>
    <p><span>3.隐含层----&gt;隐含层的权值更新：</span></p>
    <p>　方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)----&gt;net(o1)----&gt;w5,但是在隐含层之间的权值更新时，是out(h1)----&gt;net(h1)----&gt;w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。</p>
    <p>&nbsp;<img width="485" height="400" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630154317562-311369571.png"></p>
    <p>&nbsp;</p>
    <p>计算<img width="53" height="39" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630154712202-1906007645.png">：</p>
    <p><img width="547" height="51" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630154758531-934861299.png"></p>
    <p>先计算<img width="62" height="51" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630154958296-1922097086.png">：</p>
    <p><img width="340" height="48" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155015546-1106216279.png"></p>
    <p><img width="530" height="47" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155036406-964647962.png"><img width="345" height="74" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155117656-1905928379.png"><img width="527" height="51" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155158468-157032005.png"></p>
    <p>同理，计算出：</p>
    <p><img width="201" height="42" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155310937-2103938446.png"></p>
    <p>两者相加得到总值：</p>
    <p><img width="608" height="62" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155435218-396769942.png"></p>
    <p>再计算<img width="62" height="43" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155555562-1422254830.png">：</p>
    <p><img width="601" height="100" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155628046-229505495.png"></p>
    <p>再计算<img width="57" height="47" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155731421-239852713.png">：</p>
    <p><img width="509" height="90" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155706437-964861747.png"></p>
    <p>最后，三者相乘：</p>
    <p><img width="632" height="110" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155827718-189457408.png"></p>
    <p>&nbsp;为了简化公式，用sigma(h1)表示隐含层单元h1的误差：</p>
    <p><img width="540" height="183" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630160345281-679307550.png"></p>
    <p>最后，更新w1的权值：</p>
    <p><img width="567" height="48" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630160523437-1906004593.png"></p>
    <p>同理，额可更新w2,w3,w4的权值：</p>
    <p><img width="517" height="141" alt="" src="http://images2015.cnblogs.com/blog/853467/201606/853467-20160630160603484-1471434475.png"></p>
    <p>这样误差反向传播法就完成了，最后我们再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为[0.015912196,0.984065734](原输入为[0.01,0.99]),证明效果还是不错的</p>
    <br>
   </div>
   <h2><a></a><a target="_blank"></a>四、最一般的情况</h2>
   <h2><img width="320" height="240" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20160401202509000"></h2>
   <div>
    <div>
     <span><br></span>
    </div>
    <div>
     <span><span><span></span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span>上图是一个三层人工神经网络，layer1至layer3分别是输入层、隐藏层和输出层。如图，先定义一些变量：</span></span>
    </div>
    <div>
     <span><span><span></span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106115101285"></span>表示第l-1层的第k个神经元连接到第l层的第j个神经元的权重；</span></span>
    </div>
    <div>
     <span><span><span></span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106115210841"></span>表示第l层的第j个神经元的偏置；</span></span>
    </div>
    <div>
     <span><span><span></span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106115642237">表示第l</span></span>
     <span>层的第</span>j
     <span>个神经元的输入，即</span>
     <span>：</span>
    </div>
    <div>
     &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
     <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106120152901">
    </div>
    <div>
     <span><span><span></span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106115859193">表示第l层的第j个神经元的输出，即：</span></span>
    </div>
    <div>
     &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
     <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106120310262">
    </div>
    <div>
     <span><span><span><span></span><span>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span>其中<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106115344889"></span><span>表示激活函数。</span></span></span>
    </div>
    <div>
     <span><span><span>L表示神经网络的最大层数，也可以理解为输出层。<br></span></span></span>
    </div>
    <div>
     <span></span>
     <div>
      <span><span>将第l</span><span>层第</span>j<span>个神经元中产生的错误（即实际值与预测值之间的误差）定义为：</span></span>
     </div>
     <div>
      &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
      <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161106120448077">
     </div>
     <div>
      <span>代价函数，依然用C来表示</span>
      <span></span>
     </div>
     <br>
    </div>
    <br>
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105204613371">
    <br>
   </div>
   <div>
    <br>
   </div>
   <p>以上4个方程中，第一个方程其实不难理解，就是求输出对估价函数C的偏导。<br></p>
   <p>唯一比较困难的，就是第二个方程，它给出了根据下一层的错误量δl+1计算δl的等式。为证明该等式，我们先依据δkl+1=∂C/∂zkl+1重新表达下等式δlj =∂C/∂zlj。这里可以应用链式法则：<span><br></span></p>
   <span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105211313625"></span>
   <p>在最后一行，我们互换了下表达式右侧的两项，并取代了 δkl+1的定义。为了对最后一行的第一项求值，注意：<span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105212806767"><br></span><span>作微分，我们得到<br></span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105212812627"><br></span><span>代回 (42) 我们得到<br></span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161105212818472"><br></span><span>&nbsp;这就是以分量形式呈现的 (BP2)。</span><span>后两式在完成了BP2证明之后就不太难了，留给读者来证明。</span></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
