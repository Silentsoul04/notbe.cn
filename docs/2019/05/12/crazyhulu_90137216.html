<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>数据源Parquet之使用,自动分区推断 « NotBeCN</title>
  <meta name="description" content="         数据源Parquet之使用编程方式加载数据   Parquet是面向分析型业务的列式存储格式   列式存储和行式存储相比有哪些优势呢？   1、可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。   2、压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/crazyhulu_90137216.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">数据源Parquet之使用,自动分区推断</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h3>数据源Parquet之使用编程方式加载数据</h3> 
  <p>Parquet是面向分析型业务的列式存储格式</p> 
  <p>列式存储和行式存储相比有哪些优势呢？</p> 
  <p>1、可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</p> 
  <p>2、压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</p> 
  <p>3、只读取需要的列，支持向量运算，能够获取更好的扫描性能。</p> 
  <p>案例：查询用户数据中的用户姓名</p> 
  <pre class="has">
<code class="language-java">package cn.spark.study.sql;

import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

/**
 * Parquet数据源之使用编程方式加载数据
 * @author Administrator
 *
 */
public class ParquetLoadData {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("ParquetLoadData");  
		JavaSparkContext sc = new JavaSparkContext(conf);
		SQLContext sqlContext = new SQLContext(sc);
		
		// 读取Parquet文件中的数据，创建一个DataFrame
		DataFrame usersDF = sqlContext.read().parquet(
				"hdfs://spark1:9000/spark-study/users.parquet");
		
		// 将DataFrame注册为临时表，然后使用SQL查询需要的数据
		usersDF.registerTempTable("users");  
		DataFrame userNamesDF = sqlContext.sql("select name from users");  
		
		// 对查询出来的DataFrame进行transformation操作，处理数据，然后打印出来
		List&lt;String&gt; userNames = userNamesDF.javaRDD().map(new Function&lt;Row, String&gt;() {

			private static final long serialVersionUID = 1L;

			@Override
			public String call(Row row) throws Exception {
				return "Name: " + row.getString(0);
			}
			
		}).collect();
		
		for(String userName : userNames) {
			System.out.println(userName);  
		}
	}
	
}</code></pre> 
  <p>自动分区推断（一）</p> 
  <p>表分区是一种常见的优化方式，比如Hive中就提供了表分区的特性。在一个分区表中，不同分区的数据通常存储在不同的目录中，分区列的值通常就包含在了分区目录的目录名中。Spark SQL中的Parquet数据源，支持自动根据目录名推断出分区信息。例如，如果将人口数据存储在分区表中，并且使用性别和国家作为分区列。那么目录结构可能如下所示：</p> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019051209341312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NyYXp5aHVsdQ==,size_16,color_FFFFFF,t_70"></p> 
  <p>自动分区推断（二）</p> 
  <p>如果将/tableName传入SQLContext.read.parquet()或者SQLContext.read.load()方法，那么Spark SQL就会自动根据目录结构，推断出分区信息，是gender和country。即使数据文件中只包含了两列值，name和age，但是Spark SQL返回的DataFrame，调用printSchema()方法时，会打印出四个列的值：name，age，country，gender。这就是自动分区推断的功能。</p> 
  <p>此外，分区列的数据类型，也是自动被推断出来的。目前，Spark SQL仅支持自动推断出数字类型和字符串类型。有时，用户也许不希望Spark SQL自动推断分区列的数据类型。此时只要设置一个配置即可， spark.sql.sources.partitionColumnTypeInference.enabled，默认为true，即自动推断分区列的类型，设置为false，即不会自动推断类型。禁止自动推断分区列的类型时，所有分区列的类型，就统一默认都是String。</p> 
  <p>案例：自动推断用户数据的性别和国家</p> 
  <pre class="has">
<code class="language-java">package cn.spark.study.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.SQLContext;

/**
 * Parquet数据源之自动推断分区
 * @author Administrator
 *
 */
public class ParquetPartitionDiscovery {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("ParquetPartitionDiscovery");  
		JavaSparkContext sc = new JavaSparkContext(conf);
		SQLContext sqlContext = new SQLContext(sc);
		
		DataFrame usersDF = sqlContext.read().parquet(
				"hdfs://spark1:9000/spark-study/users/gender=male/country=US/users.parquet");
		usersDF.printSchema();
		usersDF.show();
	}
	
}
</code></pre> 
  <h3>&nbsp;合并元数据</h3> 
  <p>如同ProtocolBuffer，Avro，Thrift一样，Parquet也是支持元数据合并的。用户可以在一开始就定义一个简单的元数据，然后随着业务需要，逐渐往元数据中添加更多的列。在这种情况下，用户可能会创建多个Parquet文件，有着多个不同的但是却互相兼容的元数据。Parquet数据源支持自动推断出这种情况，并且进行多个Parquet文件的元数据的合并。</p> 
  <p>因为元数据合并是一种相对耗时的操作，而且在大多数情况下不是一种必要的特性，从Spark 1.5.0版本开始，默认是关闭Parquet文件的自动合并元数据的特性的。可以通过以下两种方式开启Parquet数据源的自动合并元数据的特性：</p> 
  <p>1、读取Parquet文件时，将数据源的选项，mergeSchema，设置为true</p> 
  <p>2、使用SQLContext.setConf()方法，将spark.sql.parquet.mergeSchema参数设置为true</p> 
  <h3>JSON数据源</h3> 
  <p>Spark SQL可以自动推断JSON文件的元数据，并且加载其数据，创建一个DataFrame。可以使用SQLContext.read.json()方法，针对一个元素类型为String的RDD，或者是一个JSON文件。</p> 
  <p>但是要注意的是，这里使用的JSON文件与传统意义上的JSON文件是不一样的。每行都必须，也只能包含一个，单独的，自包含的，有效的JSON对象。不能让一个JSON对象分散在多行。否则会报错</p> 
  <p>综合性复杂案例：查询成绩为80分以上的学生的基本信息与成绩信息</p> 
  <pre class="has">
<code class="language-java">package cn.spark.study.sql;

import java.util.ArrayList;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import scala.Tuple2;

/**
 * JSON数据源
 * @author Administrator
 *
 */
public class JSONDataSource {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("JSONDataSource");  
		JavaSparkContext sc = new JavaSparkContext(conf);
		SQLContext sqlContext = new SQLContext(sc);
		
		// 针对json文件，创建DataFrame（针对json文件创建DataFrame）
		DataFrame studentScoresDF = sqlContext.read().json(
				"hdfs://spark1:9000/spark-study/students.json");  
		
		// 针对学生成绩信息的DataFrame，注册临时表，查询分数大于80分的学生的姓名
		// （注册临时表，针对临时表执行sql语句）
		studentScoresDF.registerTempTable("student_scores");
		DataFrame goodStudentScoresDF = sqlContext.sql(
				"select name,score from student_scores where score&gt;=80");
		
		// （将DataFrame转换为rdd，执行transformation操作）
		List&lt;String&gt; goodStudentNames = goodStudentScoresDF.javaRDD().map(
				
				new Function&lt;Row, String&gt;() {
					
					private static final long serialVersionUID = 1L;
		
					@Override
					public String call(Row row) throws Exception {
						return row.getString(0);
					}
					
				}).collect();
		
		// 然后针对JavaRDD&lt;String&gt;，创建DataFrame
		// （针对包含json串的JavaRDD，创建DataFrame）
		List&lt;String&gt; studentInfoJSONs = new ArrayList&lt;String&gt;();
		studentInfoJSONs.add("{\"name\":\"Leo\", \"age\":18}");  
		studentInfoJSONs.add("{\"name\":\"Marry\", \"age\":17}");  
		studentInfoJSONs.add("{\"name\":\"Jack\", \"age\":19}");
		JavaRDD&lt;String&gt; studentInfoJSONsRDD = sc.parallelize(studentInfoJSONs);
		DataFrame studentInfosDF = sqlContext.read().json(studentInfoJSONsRDD);
		
		// 针对学生基本信息DataFrame，注册临时表，然后查询分数大于80分的学生的基本信息
		studentInfosDF.registerTempTable("student_infos");  
		
		String sql = "select name,age from student_infos where name in (";
		for(int i = 0; i &lt; goodStudentNames.size(); i++) {
			sql += "'" + goodStudentNames.get(i) + "'";
			if(i &lt; goodStudentNames.size() - 1) {
				sql += ",";
			}
		}
		sql += ")";
		
		DataFrame goodStudentInfosDF = sqlContext.sql(sql);
		
		// 然后将两份数据的DataFrame，转换为JavaPairRDD，执行join transformation
		// （将DataFrame转换为JavaRDD，再map为JavaPairRDD，然后进行join）
		JavaPairRDD&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; goodStudentsRDD = 
				
				goodStudentScoresDF.javaRDD().mapToPair(new PairFunction&lt;Row, String, Integer&gt;() {

					private static final long serialVersionUID = 1L;
		
					@Override
					public Tuple2&lt;String, Integer&gt; call(Row row) throws Exception {
						return new Tuple2&lt;String, Integer&gt;(row.getString(0), 
								Integer.valueOf(String.valueOf(row.getLong(1))));  
					}
					
				}).join(goodStudentInfosDF.javaRDD().mapToPair(new PairFunction&lt;Row, String, Integer&gt;() {
		
					private static final long serialVersionUID = 1L;
		
					@Override
					public Tuple2&lt;String, Integer&gt; call(Row row) throws Exception {
						return new Tuple2&lt;String, Integer&gt;(row.getString(0),
								Integer.valueOf(String.valueOf(row.getLong(1))));   
					}
					
				}));
		
		// 然后将封装在RDD中的好学生的全部信息，转换为一个JavaRDD&lt;Row&gt;的格式
		// （将JavaRDD，转换为DataFrame）
		JavaRDD&lt;Row&gt; goodStudentRowsRDD = goodStudentsRDD.map(
				
				new Function&lt;Tuple2&lt;String,Tuple2&lt;Integer,Integer&gt;&gt;, Row&gt;() {

					private static final long serialVersionUID = 1L;

					@Override
					public Row call(
							Tuple2&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; tuple)
							throws Exception {
						return RowFactory.create(tuple._1, tuple._2._1, tuple._2._2);
					}
					
				});
		
		// 创建一份元数据，将JavaRDD&lt;Row&gt;转换为DataFrame
		List&lt;StructField&gt; structFields = new ArrayList&lt;StructField&gt;();
		structFields.add(DataTypes.createStructField("name", DataTypes.StringType, true)); 
		structFields.add(DataTypes.createStructField("score", DataTypes.IntegerType, true));  
		structFields.add(DataTypes.createStructField("age", DataTypes.IntegerType, true));  
		StructType structType = DataTypes.createStructType(structFields);
		
		DataFrame goodStudentsDF = sqlContext.createDataFrame(goodStudentRowsRDD, structType);
		
		// 将好学生的全部信息保存到一个json文件中去
		// （将DataFrame中的数据保存到外部的json文件中去）
		goodStudentsDF.write().format("json").save("hdfs://spark1:9000/spark-study/good-students");  
	}
	
}</code></pre> 
  <h3>Hive数据源实战</h3> 
  <p>Spark SQL支持对Hive中存储的数据进行读写。操作Hive中的数据时，必须创建HiveContext，而不是SQLContext。HiveContext继承自SQLContext，但是增加了在Hive元数据库中查找表，以及用HiveQL语法编写SQL的功能。除了sql()方法，HiveContext还提供了hql()方法，从而用Hive语法来编译sql。</p> 
  <p>使用HiveContext，可以执行Hive的大部分功能，包括创建表、往表里导入数据以及用SQL语句查询表中的数据。查询出来的数据是一个Row数组。</p> 
  <p>将hive-site.xml拷贝到spark/conf目录下，将mysql connector拷贝到spark/lib目录下</p> 
  <pre class="has">
<code>HiveContext sqlContext = new HiveContext(sc);
sqlContext.sql("CREATE TABLE IF NOT EXISTS students (name STRING, age INT)");
sqlContext.sql("LOAD DATA LOCAL INPATH '/usr/local/spark-study/resources/students.txt' INTO TABLE students");
Row[] teenagers = sqlContext.sql("SELECT name, age FROM students WHERE age&lt;=18").collect();</code></pre> 
  <p>将数据保存到表中</p> 
  <p>Spark SQL还允许将数据保存到Hive表中。调用DataFrame的saveAsTable命令，即可将DataFrame中的数据保存到Hive表中。与registerTempTable不同，saveAsTable是会将DataFrame中的数据物化到Hive表中的，而且还会在Hive元数据库中创建表的元数据。</p> 
  <p>默认情况下，saveAsTable会创建一张Hive Managed Table，也就是说，数据的位置都是由元数据库中的信息控制的。当Managed Table被删除时，表中的数据也会一并被物理删除。</p> 
  <p>registerTempTable只是注册一个临时的表，只要Spark Application重启或者停止了，那么表就没了。而saveAsTable创建的是物化的表，无论Spark Application重启或者停止，表都会一直存在。</p> 
  <p>调用HiveContext.table()方法，还可以直接针对Hive中的表，创建一个DataFrame。</p> 
  <p>案例：查询分数大于80分的学生的完整信息</p> 
  <pre class="has">
<code class="language-java">package cn.spark.study.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.hive.HiveContext;

/**
 * Hive数据源
 * @author Administrator
 *
 */
public class HiveDataSource {

	@SuppressWarnings("deprecation")
	public static void main(String[] args) {
		// 首先还是创建SparkConf
		SparkConf conf = new SparkConf()
				.setAppName("HiveDataSource");
		// 创建JavaSparkContext
		JavaSparkContext sc = new JavaSparkContext(conf);
		// 创建HiveContext，注意，这里，它接收的是SparkContext作为参数，不是JavaSparkContext
		HiveContext hiveContext = new HiveContext(sc.sc());
		
		// 第一个功能，使用HiveContext的sql()方法，可以执行Hive中能够执行的HiveQL语句
		
		// 判断是否存在student_infos表，如果存在则删除
		hiveContext.sql("DROP TABLE IF EXISTS student_infos");
		// 判断student_infos表是否不存在，如果不存在，则创建该表
		hiveContext.sql("CREATE TABLE IF NOT EXISTS student_infos (name STRING, age INT)");
		// 将学生基本信息数据导入student_infos表
		hiveContext.sql("LOAD DATA "
				+ "LOCAL INPATH '/usr/local/spark-study/resources/student_infos.txt' "
				+ "INTO TABLE student_infos");
		
		// 用同样的方式给student_scores导入数据
		hiveContext.sql("DROP TABLE IF EXISTS student_scores"); 
		hiveContext.sql("CREATE TABLE IF NOT EXISTS student_scores (name STRING, score INT)");  
		hiveContext.sql("LOAD DATA "
				+ "LOCAL INPATH '/usr/local/spark-study/resources/student_scores.txt' "
				+ "INTO TABLE student_scores");
		
		// 第二个功能，执行sql还可以返回DataFrame，用于查询
		
		// 执行sql查询，关联两张表，查询成绩大于80分的学生
		DataFrame goodStudentsDF = hiveContext.sql("SELECT si.name, si.age, ss.score "
				+ "FROM student_infos si "
				+ "JOIN student_scores ss ON si.name=ss.name "
				+ "WHERE ss.score&gt;=80");
		
		// 第三个功能，可以将DataFrame中的数据，理论上来说，DataFrame对应的RDD的元素，是Row即可
		// 将DataFrame中的数据保存到hive表中
		
		// 接着将DataFrame中的数据保存到good_student_infos表中
		hiveContext.sql("DROP TABLE IF EXISTS good_student_infos");  
		goodStudentsDF.saveAsTable("good_student_infos");  
		
		// 第四个功能，可以用table()方法，针对hive表，直接创建DataFrame
		
		// 然后针对good_student_infos表，直接创建DataFrame
		Row[] goodStudentRows = hiveContext.table("good_student_infos").collect();  
		for(Row goodStudentRow : goodStudentRows) {
			System.out.println(goodStudentRow);  
		}
		
		sc.close();
	}
	
}</code></pre> 
  <h3>JDBC数据源实战</h3> 
  <p>Spark SQL支持使用JDBC从关系型数据库（比如MySQL）中读取数据。读取的数据，依然由DataFrame表示，可以很方便地使用Spark Core提供的各种算子进行处理。</p> 
  <p>实际上用Spark SQL处理JDBC中的数据是非常有用的。比如说，你的MySQL业务数据库中，有大量的数据，比如1000万，然后，你现在需要编写一个程序，对线上的脏数据某种复杂业务逻辑的处理，甚至复杂到可能涉及到要用Spark SQL反复查询Hive中的数据，来进行关联处理。</p> 
  <p>那么此时，用Spark SQL来通过JDBC数据源，加载MySQL中的数据，然后通过各种算子进行处理，是最好的选择。因为Spark是分布式的计算框架，对于1000万数据，肯定是分布式处理的。而如果你自己手工编写一个Java程序，那么不好意思，你只能分批次处理了，先处理2万条，再处理2万条，可能运行完你的Java程序，已经是几天以后的事情了。</p> 
  <p>案例：查询分数大于80分的学生信息</p> 
  <pre class="has">
<code class="language-java">package cn.spark.study.sql;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import scala.Tuple2;

/**
 * JDBC数据源
 * @author Administrator
 *
 */
public class JDBCDataSource {

	public static void main(String[] args) {
		SparkConf conf = new SparkConf()
				.setAppName("JDBCDataSource");  
		JavaSparkContext sc = new JavaSparkContext(conf);
		SQLContext sqlContext = new SQLContext(sc);
		
		// 总结一下
		// jdbc数据源
		// 首先，是通过SQLContext的read系列方法，将mysql中的数据加载为DataFrame
		// 然后可以将DataFrame转换为RDD，使用Spark Core提供的各种算子进行操作
		// 最后可以将得到的数据结果，通过foreach()算子，写入mysql、hbase、redis等等db / cache中
		
		// 分别将mysql中两张表的数据加载为DataFrame
		Map&lt;String, String&gt; options = new HashMap&lt;String, String&gt;();
		options.put("url", "jdbc:mysql://spark1:3306/testdb");
		options.put("dbtable", "student_infos");
		DataFrame studentInfosDF = sqlContext.read().format("jdbc")
				.options(options).load();
	
		options.put("dbtable", "student_scores");
		DataFrame studentScoresDF = sqlContext.read().format("jdbc")
				.options(options).load();
		
		// 将两个DataFrame转换为JavaPairRDD，执行join操作
		JavaPairRDD&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; studentsRDD = 
				
				studentInfosDF.javaRDD().mapToPair(
				
						new PairFunction&lt;Row, String, Integer&gt;() {
		
							private static final long serialVersionUID = 1L;
				
							@Override
							public Tuple2&lt;String, Integer&gt; call(Row row) throws Exception {
								return new Tuple2&lt;String, Integer&gt;(row.getString(0), 
										Integer.valueOf(String.valueOf(row.get(1))));  
							}
							
						})
				.join(studentScoresDF.javaRDD().mapToPair(
							
						new PairFunction&lt;Row, String, Integer&gt;() {
		
							private static final long serialVersionUID = 1L;
		
							@Override
							public Tuple2&lt;String, Integer&gt; call(Row row) throws Exception {
								return new Tuple2&lt;String, Integer&gt;(String.valueOf(row.get(0)),
										Integer.valueOf(String.valueOf(row.get(1))));  
							}
							
						}));
		
		// 将JavaPairRDD转换为JavaRDD&lt;Row&gt;
		JavaRDD&lt;Row&gt; studentRowsRDD = studentsRDD.map(
				
				new Function&lt;Tuple2&lt;String,Tuple2&lt;Integer,Integer&gt;&gt;, Row&gt;() {

					private static final long serialVersionUID = 1L;

					@Override
					public Row call(
							Tuple2&lt;String, Tuple2&lt;Integer, Integer&gt;&gt; tuple)
							throws Exception {
						return RowFactory.create(tuple._1, tuple._2._1, tuple._2._2);
					}
					
				});
		
		// 过滤出分数大于80分的数据
		JavaRDD&lt;Row&gt; filteredStudentRowsRDD = studentRowsRDD.filter(
				
				new Function&lt;Row, Boolean&gt;() {

					private static final long serialVersionUID = 1L;

					@Override
					public Boolean call(Row row) throws Exception {
						if(row.getInt(2) &gt; 80) {
							return true;
						} 
						return false;
					}
					
				});
		
		// 转换为DataFrame
		List&lt;StructField&gt; structFields = new ArrayList&lt;StructField&gt;();
		structFields.add(DataTypes.createStructField("name", DataTypes.StringType, true));  
		structFields.add(DataTypes.createStructField("age", DataTypes.IntegerType, true)); 
		structFields.add(DataTypes.createStructField("score", DataTypes.IntegerType, true)); 
		StructType structType = DataTypes.createStructType(structFields);
		
		DataFrame studentsDF = sqlContext.createDataFrame(filteredStudentRowsRDD, structType);
		
		Row[] rows = studentsDF.collect();
		for(Row row : rows) {
			System.out.println(row);  
		}
		
		// 将DataFrame中的数据保存到mysql表中
		// 这种方式是在企业里很常用的，有可能是插入mysql、有可能是插入hbase，还有可能是插入redis缓存
		studentsDF.javaRDD().foreach(new VoidFunction&lt;Row&gt;() {
			
			private static final long serialVersionUID = 1L;

			@Override
			public void call(Row row) throws Exception {
				String sql = "insert into good_student_infos values(" 
						+ "'" + String.valueOf(row.getString(0)) + "',"
						+ Integer.valueOf(String.valueOf(row.get(1))) + ","
						+ Integer.valueOf(String.valueOf(row.get(2))) + ")";   
				
				Class.forName("com.mysql.jdbc.Driver");  
				
				Connection conn = null;
				Statement stmt = null;
				try {
					conn = DriverManager.getConnection(
							"jdbc:mysql://spark1:3306/testdb", "", "");
					stmt = conn.createStatement();
					stmt.executeUpdate(sql);
				} catch (Exception e) {
					e.printStackTrace();
				} finally {
					if(stmt != null) {
						stmt.close();
					} 
					if(conn != null) {
						conn.close();
					}
				}
			}
			
		}); 
		
		sc.close();
	}
	
}</code></pre> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
