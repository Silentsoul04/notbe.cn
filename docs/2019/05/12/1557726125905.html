<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>python下载百度文库非付费文档 « NotBeCN</title>
  <meta name="description" content="                  百度文库有很多有价值的资料，虽然网页可以浏览，但只有下载客户端才能下载。于是就想用python下载下来。   首先打开网页：  可以看到word文档是网页中的元素拼接而成的，我们先看看数据来源。先用谷歌浏览器抓一下包，发现请求很多，而文档的数据并不在返回的网页中，应该是ajax...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/1557726125905.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">python下载百度文库非付费文档</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p>百度文库有很多有价值的资料，虽然网页可以浏览，但只有下载客户端才能下载。于是就想用python下载下来。</p> 
  <p>首先打开网页：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190512134031535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1F3ZXJ0eXVpb3AyMDE2,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 可以看到word文档是网页中的元素拼接而成的，我们先看看数据来源。先用谷歌浏览器抓一下包，发现请求很多，而文档的数据并不在返回的网页中，应该是ajax加载的。一个一个查看请求太慢了，打开Charles全局搜索文档中的文字，未找到，不知道是什么原因。只能一个一个查找请求了,发现是这样一条请求包含数据：<a href="https://wkbjcloudbos.bdimg.com/v1/docconvert4795/wk/4b2e147d83111fcd611f6a697ea427e8/0.json?responseContentType=application%2Fjavascript&amp;responseCacheControl=max-age%3D3888000&amp;responseExpires=Wed%2C%2026%20Jun%202019%2013%3A38%3A46%20%2B0800&amp;authorization=bce-auth-v1%2Ffa1126e91489401fa7cc85045ce7179e%2F2019-05-12T05%3A38%3A46Z%2F3600%2Fhost%2F952f2f4107ce424d09f176604db5ed2de0a25266bb390cf335f000c0b25aa00f&amp;x-bce-range=0-13342&amp;token=eyJ0eXAiOiJKSVQiLCJ2ZXIiOiIxLjAiLCJhbGciOiJIUzI1NiIsImV4cCI6MTU1NzY0MzEyNiwidXJpIjp0cnVlLCJwYXJhbXMiOlsicmVzcG9uc2VDb250ZW50VHlwZSIsInJlc3BvbnNlQ2FjaGVDb250cm9sIiwicmVzcG9uc2VFeHBpcmVzIiwieC1iY2UtcmFuZ2UiXX0%3D.EjChG02byzRwcrz4uSyfIFP0824aAQA0fgJvFMossNE%3D.1557643126" rel="nofollow">https://wkbjcloudbos.bdimg.com/v1/docconvert4795/wk/4b2e147d83111fcd611f6a697ea427e8/0.json?responseContentType=application%2Fjavascript&amp;responseCacheControl=max-age%3D3888000&amp;responseExpires=Wed%2C 26 Jun 2019 13%3A38%3A46 %2B0800&amp;authorization=bce-auth-v1%2Ffa1126e91489401fa7cc85045ce7179e%2F2019-05-12T05%3A38%3A46Z%2F3600%2Fhost%2F952f2f4107ce424d09f176604db5ed2de0a25266bb390cf335f000c0b25aa00f&amp;x-bce-range=0-13342&amp;token=eyJ0eXAiOiJKSVQiLCJ2ZXIiOiIxLjAiLCJhbGciOiJIUzI1NiIsImV4cCI6MTU1NzY0MzEyNiwidXJpIjp0cnVlLCJwYXJhbXMiOlsicmVzcG9uc2VDb250ZW50VHlwZSIsInJlc3BvbnNlQ2FjaGVDb250cm9sIiwicmVzcG9uc2VFeHBpcmVzIiwieC1iY2UtcmFuZ2UiXX0%3D.EjChG02byzRwcrz4uSyfIFP0824aAQA0fgJvFMossNE%3D.1557643126</a><br> 数据的格式很复杂，其中主要有内容的字体位置和正文，还有css的style。这样的数据即使拿到也是无法复原成word格式的，只能通过截图的方式获取图片，在转换为PDF格式的，如果一定要word格式的，只能百度搜索PDF转Word。但我试了，效果不是很理想，只能说过得去。</p> 
  <p>这个请求链接包含了很多的参数，这样的链接不可能是浏览器自己拼接的，肯定是服务器返回给浏览器的，于是在Charles中搜索关键字，发现在请求链接的返回网页中的其中一个js变量(当然这一步并没有什么意义，只是说明一下怎么得到链接)。到了这一步，猜测Charles是不是不支持中文搜索，百度并没有得到验证，所以这仅仅是猜想。</p> 
  <p>接着我试了其他的格式，PPT格式是图片数据，TXT格式是文本数据，PDF格式有些是图片有些是文本拼接。所以我们可以将PPT和TXT格式的区分开来，如果是PPT则直接下载图片，如果是TXT则直接下载文本，其他格式则使用selenium截图。</p> 
  <p>下载PPT和TXT格式的很成功，没有遇到什么问题，不过值得一提的是获取资源的所有链接都是HTTP2.0的请求，而requests是只支持HTTP1.1的，所以我们需要使用另一个库–hyper，使用很简单，官网有详细的案例。很多网站都已经有HTTP2.0的请求了，这可能是一个趋势吧。</p> 
  <p>selenium截图的话，如果驱动的是Chrome或者Firefox，截图是只能截可见区域的，而PhantomJS则是可以截整个网页，那么我们只要某个页面元素的截图怎么办呢，最简单的方法就是先截整个网页，在将某个元素的图扣下来。</p> 
  <p>我们可以先使用Chrome测试，然后在使用phantomjs。但当我再Chrome测试完成，就差截图的时候，发现phantomjs截取的图里含有大量空白。<br> 可能猜想：</p> 
  <ol> 
   <li>未加头信息</li> 
   <li>截图太快，未成功加载</li> 
  </ol> 
  <p>验证了一下两个都不是，我是使用WebDriverWait等到指定元素加载完成才截的图。开始的3页是可以成功显示的，但点击继续加载剩余页之后的都显示为空白。难道phantomjs不支持ajax加载的信息？百度未果。另外，使用Chrome是可以成功加载的，只是无法截图。</p> 
  <p>如果有知道的大佬还请指教。<br> 代码如下：</p> 
  <pre><code># -*- coding: utf-8 -*-
import time
import os
from PIL import Image

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
#from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

#dcap = dict(DesiredCapabilities.PHANTOMJS)
##从USER_AGENTS列表中随机选一个浏览器头，伪装浏览器
#dcap["phantomjs.page.settings.userAgent"] = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.61 Safari/537.36'
#driver = webdriver.PhantomJS(desired_capabilities=dcap)                

driver = webdriver.PhantomJS()
#driver = webdriver.Chrome()
driver.maximize_window()

url = 'https://wenku.baidu.com/view/71301e497ed5360cba1aa8114431b90d6c85899b.html?rec_flag=default&amp;sxts=1557566831620'
wait = WebDriverWait(driver, 10)
driver.get(url)
location = []


for i in range(1, 100):
    try:
        page = wait.until(EC.presence_of_element_located((By.ID, 'pageNo-' + str(i))))
    except TimeoutException:
        n = i
        break
    driver.execute_script("arguments[0].scrollIntoView(false);", page)
    wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, f'#pageNo-{i} .reader-txt-layer')))
    left = page.location['x']
    top = page.location['y']
    right = left + page.size['width']
    bottom = top + page.size['height']
    location.append((left, top, right, bottom))
    #print(i)
    if i == 3:
        more = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'moreBtn')))
        cur_y = more.location['y']
        driver.execute_script(f"window.scrollTo({cur_y-700}, {cur_y-300})")
        more.click()
        time.sleep(2.5)


if not os.path.exists('image'): 
    os.mkdir('image')
os.chdir('image')
driver.save_screenshot('screenshot.png')
driver.quit()    

for i in range(1, n):
    image_full = Image.open('screenshot.png')
    image_obj = image_full.crop(location[i-1])
    image_obj.save(str(i) + '.png')

</code></pre> 
  <p>下载PPT和TXT的代码(未完善，仅供参考)：</p> 
  <pre><code># -*- coding: utf-8 -*-
import time
import re
import json
import os
import requests
from hyper.contrib import HTTP20Adapter
#from settings import *


def get_info(session, doc_url, t):
    doc_id = re.search(r'/(\w+)\.html', doc_url).group(1)
    url = 'https://wenku.baidu.com/api/doc/getdocinfo'
    params = {
        'callback': 'cb',
        'doc_id': doc_id,
        't': int(time.time()*1000),
        '_': t
    }
    headers = {
        'Host': 'wenku.baidu.com',
        'Connection': 'keep-alive',
        'Accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
        'X-Requested-With': 'XMLHttpRequest',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
        'Referer': 'https://wenku.baidu.com/view/177d0765c77da26924c5b090.html?from=search',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9'
    }
    resp = session.get(url, params=params, headers=headers, verify=False)
    if resp.status_code == 200:
        doc_info_str = re.search(r'\{.*\}', resp.text).group()
        doc_info = json.loads(doc_info_str)
        return doc_info
    else:
        print(f'获取文档信息失败，状态码为:{resp.status_code}')
        return None

def txter(session, doc_info):
    doc_id = doc_info.get('doc_id')
    temp = doc_info.get('md5sum').strip('&amp;')
    md5sum, sign = re.search(r'md5sum=(\w+)&amp;sign=(\w+)', temp).group(1, 2)
    rsign = doc_info.get('rsign')
    rn = doc_info.get('docInfo').get('totalPageNum')
    title = doc_info.get('seoTitle')
    
    base_url = 'https://wkretype.bdimg.com/retype/text/'
    url = base_url + doc_id
    params = {
        'md5sum': md5sum,
        'sign': sign,
        'callback': 'cb',
        'pn': '1',
        'rn': rn,
        'type': 'txt',
        'rsign': rsign,
        '_': int(time.time()*1000)
    }
    _path = '&amp;'.join((f'{x[0]}={x[1]}' for x in params.items()))
    headers = {
        ':method': 'GET',
        ':authority': 'wkretype.bdimg.com',
        ':scheme': 'https',
        ':path': '/retype/text/' + doc_id + '?' + _path,
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
        'accept': '*/*',
        'referer': f'https://wenku.baidu.com/view/{doc_id}.html?from=search',
        'accept-encoding': 'gzip, deflate, br',
        'accept-language': 'zh-CN,zh;q=0.9'
    }
    resp = session.get(url, params=params, headers=headers, verify=False)
    if resp.status_code == 200:
        texts_str = resp.text.strip('cb(').strip(')')
        texts = json.loads(texts_str)
        with open(title + '.txt', 'w', encoding='utf-8') as f:
            for text in texts:
                txt = ''.join((x.get('c') for x in text.get('parags')))
                f.write(txt)
    else:
        print(f'获取txt数据失败，状态码为:{resp.status_code}')
        return None
    
def ppter(session, doc_info):
    doc_id = doc_info.get('doc_id')
    temp = doc_info.get('md5sum').strip('&amp;')
    md5sum, sign = re.search(r'md5sum=(\w+)&amp;sign=(\w+)', temp).group(1, 2)
    #rsign = doc_info.get('rsign')
    title = doc_info.get('seoTitle')
    if not os.path.exists(title):
        os.mkdir(title)
    os.chdir(title)
    
    bcses = doc_info.get('bcsParam')
    for bcs in bcses:
        zoom = bcs.get('zoom')
        png, jpg = re.search(r'&amp;png=(\d*-\d*)&amp;jpg=(\d*-\d*)', zoom).group(1, 2)
        params = {
            'pn': bcs.get('page'),
            'o': 'jpg_6',
            'md5sum': md5sum,
            'sign': sign,
            'png': png,
            'jpg': jpg
        }
        downloader(session, doc_id, params)
    
def png2pdf():
    pass

def Other(url):
    pass

def downloader(session, doc_id, params):
    page = params.get('pn')
    url = 'https://wkretype.bdimg.com/retype/zoom/' + doc_id
    _path = '&amp;'.join((f'{x[0]}={x[1]}' for x in params.items()))
    headers = {
        ':authority': 'wkretype.bdimg.com',
        ':method': 'GET',
        ':path': '/retype/zoom/' + doc_id + '?' + _path,
        ':scheme': 'https',
        'accept': 'image/webp,image/*,*/*;q=0.8',
        'accept-encoding': 'gzip, deflate, sdch, br',
        'accept-language': 'zh-CN,zh;q=0.8',
        'referer': f'https://wenku.baidu.com/view/{doc_id}.html?from=search',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.61 Safari/537.36'
    }
    resp = session.get(url, params=params, headers=headers, verify=False)
    if resp.status_code == 200:
        with open(str(page) + '.png', 'wb') as f:
            f.write(resp.content)
    else:
        print(f'下载图片失败，状态码为:{resp.status_code}')
        return None

if __name__ == '__main__':
    session = requests.Session()
    doc_url = 'https://wenku.baidu.com/view/cb0b663630126edb6f1aff00bed5b9f3f90f7256.html?from=search'
    #doc_url = 'https://wenku.baidu.com/view/177d0765c77da26924c5b090.html?from=search'
    t = int(time.time() * 1000)
    session.mount('https://wkretype.bdimg.com/', HTTP20Adapter())  
    doc_info = get_info(session, doc_url, t)
    doc_type = doc_info.get('docInfo').get('docType')
    if doc_type == '8':
        txter(session, doc_info)
    elif doc_type == '1':
        pass
    else:
        ppter(session, doc_info)

    
</code></pre> 
  <p>如果找到解决的原因再将代码完善，这里就先记录一下。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
