<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>神经网络中的神经元和激活函数详解 « NotBeCN</title>
  <meta name="description" content="                     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_45031002_90138500.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">神经网络中的神经元和激活函数详解</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>在上一节，我们通过两个浅显易懂的例子表明，人工智能的根本目标就是在不同的数据集中找到他们的边界，依靠这条边界线，当有新的数据点到来时，只要判断这个点与边界线的相互位置就可以判断新数据点的归属。</p>
    <p><img title="" alt="这里写图片描述" src="http://upload-images.jianshu.io/upload_images/2849961-92bf182dd096b854?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
    <p>上一节我们举得例子中，数据集可以使用一条直线区分开。但对很多问题而言，单一直线是无法把数据点区分开的，例如亦或运算， 当两数的值不同时，亦或结果为1，相同时亦或运算结果为0，我们用 oxr 标记亦或运算，那么输入是0和1时，有以下几种情况：</p>
    <pre class="prettyprint"><code class="hljs erlang has-numbering"><span class="hljs-number">1</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">0</span> = <span class="hljs-number">1</span><span class="hljs-number">0</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">1</span> = <span class="hljs-number">1</span><span class="hljs-number">1</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">1</span> = <span class="hljs-number">0</span><span class="hljs-number">0</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">0</span> = <span class="hljs-number">0</span></code>
     <div class="hljs-button signin"></div>
     <ul class="pre-numbering"></ul>
     <ul class="pre-numbering">
      <li>1</li>
     </ul></pre>
    <p>我们把输入的四种情况绘制到坐标轴上看看：</p>
    <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209161732426?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
    <p>我们看到，两个绿色点属于同一集合，因为绿色点做亦或运算后结果都是1，红色点属于统一集合，因为他们做运算后结果都是0，然而面对这种情形，你根本无法用一根直线把两种集合点区分开来，你必须如上图所示，用两根直线才能区分，如果点在两根直线之间，那么他们属于同一集合，如果点处于两跟直线之外，那么他们属于另外一个集合。</p>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p>所谓神经网络就是由很多个神经元组成的集合，我们可以认为一个神经元用于绘制一段分割线，复杂的数据分布需要很多条形状各异的线条才能组成合理的分界线，数据分布的情况越复杂，我们就需要越多个神经元来进行运算。</p>
   <p>深度学习的神经网络借助了生物学对脑神经系统的研究成果。一个生物大脑神经元如下所示：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209162644343?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>最左边的部分’dendrite‘叫突触，它用来接收外界输入的电信号，中间部分axon叫轴突，它把突触接收的信号进行整合处理，右边部分terminals叫终端输出，它把轴突整合后的信号切分成多部分，分别传送给其他神经元。下图是脑神经学家从鸽子脑子获取的神经元图像：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209162957635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>人的大脑大概有一千亿个神经元组成一个庞大的计算网络。苍蝇大脑只有十万个神经元，尽管如此，苍蝇就能够控制飞行，寻找食物，识别和躲避危险，这些很看似简单的动作操控就连现在最强大的计算机都无法实现。生物大脑其运算能力远逊于计算机，为何生物能轻而易举做到的事情计算机却做不到呢？大脑的运行机制目前人类还没有完全搞懂，但有一点可以肯定的是，生物大脑的运算运行存在“模糊性”，而电子计算机不行。</p>
   <p>我们看一个神经元是如何工作的。神经元接收的是电信号，然后输出另一种电信号。如果输入电信号的强度不够大，那么神经元就不会做出任何反应，如果电信号的强度大于某个界限，那么神经元就会做出反应，向其他神经元传递电信号：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209163641726?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>想象你把手指深入水中，如果水的温度不高，你不会感到疼痛，如果水的温度不断升高，当温度超过某个度数时，你会神经反射般的把手指抽出来，然后才感觉到疼痛，这就是输入神经元的电信号强度超过预定阈值后，神经元做出反应的结果。为了模拟神经元这种根据输入信号强弱做出反应的行为，在深度学习算法中，运用了多种函数来模拟这种特性，最常用的分布是步调函数和sigmoid函数，我们先看看步调函数的特性，我们通过以下代码来绘制步调函数：</p>
   <pre class="prettyprint"><code class="hljs avrasm has-numbering">import matplotlib<span class="hljs-preprocessor">.pyplot</span> as plt<span class="hljs-built_in">x</span> = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<span class="hljs-built_in">y</span> = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]plt<span class="hljs-preprocessor">.step</span>(<span class="hljs-built_in">x</span>, <span class="hljs-built_in">y</span>)plt<span class="hljs-preprocessor">.show</span>()</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>上面代码运行后结果如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209164248398?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>我们看到，这个函数的特点是，当输入的x小于1时，函数的输出一直都是零。当输入的x大于等于1时，输出一下子从零跃迁到1，当x输入处于1到2之间时，输出一直是1，当x增大到2以上时，输出一下子跃迁到2，以此类推。</p>
   <p>第二种常用的模拟函数就是sigmoid,它的形状就像字母S,输入下面代码绘制sigmoid函数：</p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pylab<span class="hljs-keyword">import</span> pylab <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> (<span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x)))mySamples = []mySigmoid = []x = plt.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)y = plt.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)plt.plot(x, sigmoid(x), <span class="hljs-string">'r'</span>, label = <span class="hljs-string">'linspace(-10, 10, 10)'</span>)plt.plot(y, sigmoid(y), <span class="hljs-string">'r'</span>, label=<span class="hljs-string">'linspace(-10, 10, 1000)'</span>)plt.grid()plt.title(<span class="hljs-string">'Sigmoid function'</span>)plt.suptitle(<span class="hljs-string">'Sigmoid'</span>)plt.legend(loc=<span class="hljs-string">'lower right'</span>)plt.text(<span class="hljs-number">4</span>, <span class="hljs-number">0.8</span>, <span class="hljs-string">r'$\sigma(x)=\frac{1}{1+e^(-x)}$'</span>, fontsize=<span class="hljs-number">15</span>)plt.gca().xaxis.set_major_locator(plt.MultipleLocator(<span class="hljs-number">1</span>))plt.gca().yaxis.set_major_locator(plt.MultipleLocator(<span class="hljs-number">0.1</span>))plt.xlabel(<span class="hljs-string">'X Axis'</span>)plt.ylabel(<span class="hljs-string">'Y Axis'</span>)plt.show()</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>上面代码执行后结果如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209164713667?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>从函数图我们看到，当输入小于0时，函数的输出增长很缓慢，当输入大于0时，输出便极具增长，等到x大到一定程度后，输出保持在固定水准。sigmoid函数的代数式子如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209164904517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>其中的字母e表示欧拉常数，它的值约为2.71828。以后面对更复杂的问题时，我们还得使用更复杂的模拟函数，所有这些模拟神经元对电信号进行反应的函数统称为<strong><em>激活函数</em></strong>。</p>
   <p>一个神经元会同时接收多个电信号，把这些电信号统一起来，用激活函数处理后再输出新的电信号，如下图：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209165320424?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>神经网络算法中设计的神经元会同时接收多个输入参数，它把这些参数加总求和，然后代入用激活函数，产生的结果就是神经元输出的电信号。如果输入参数加总的值不大，那么输出的信号值就会很小，如果输入信号中，有某一个值很大其他的都很小，那么加总后值很大，输出的信号值就会变大，如果每个输入参数都不算太大，但加总后结果很大，于是输出的信号值就会很大，这种情况就使得运算具备一定的<strong><em>模糊性</em></strong>,这样就跟生物大脑的神经元运转方式很相像。</p>
   <p>神经元不是各自为战，而是连成一个网络，并对电信号的处理形成一种链式反应：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209165844533?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>前一个神经元接收输入信号，处理后会把输出信号分别传送给下一层的多个神经元。在神经网络算法上也会模拟这种特性，在算法设计中，我们会构造如下的数据结构：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209170033172?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>上面有三层节点，每层有三个节点，第一层的节点接收输入，进行运算后，把输出结果分别提交给下一层的三个节点，如此类推直到最后一层。现在问题是，这种结构如何像上一节我们举得例子那样，根据误差进行学习调整呢？事实上，在上一层的节点把处理后的电信号传达到下一层的节点时，输出信号会分成若干部分分别传给下一层的不同节点，每一部分都对应一个权值，如下图：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209170352549?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>我们看到，第一层的节点1把输出信号传给第二层的节点1时，传递路径上有一个权值W(1,1)，也就是节点1输出的电信号值乘以这个权值W(1,1)后，所得的结果才会提交给第二层的节点1，同理第一层节点1输出的信号要乘以W(1,2)这个权值后，所得结果才会传递给第二层节点2.</p>
   <p>这些参数就对应于我们上一节例子中用于调整的参数，整个网络对输入进行运算后，在最外层产生输出，输出会跟结果进行比对，获取误差，然后网络再根据误差反过来调整这些层与层之间的传递参数，只不过参数调整的算法比我们前一节的例子要复杂不少。接下来我们看看一个具体的两层网络信号传递运算过程。</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209174442510?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>上图是一个两层网络，每个网络有两个节点，假设从第一次开始输入两个信号，分别是1，0.5：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209174539898?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>第一层神经元直接把输入加总后分发到第二层，第二层神经元使用的激活函数是sigmoid, 神经元之间的信号权值如下：</p>
   <pre class="prettyprint"><code class="hljs fix has-numbering"><span class="hljs-attribute">W(1,1) </span>=<span class="hljs-string"> 0.9; W(1,2) = 0.2W(2,1) = 0.3; W(2,2) = 0.8</span></code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>就如上一节例子描述的，一开始每层节点间的权值是随机获取的。于是第一层神经元处理信号后把输出传递给第二层：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209174933916?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>主要的运算发生在第二层的神经元节点。第二层的神经元要把第一层传来的信号值加总然后在传给sigmoid激活函数</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209175316154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>从第一层第一个节点传给第二层第一个节点的信号值是 1.0 * 0.9 = 0.9; 第一层第二个节点传给第二层第一个节点的信号值是 0.5 * 0.3 = 0.15。第二层第一个节点把这些信号值加总后得 X = 0.9 + 0.15 = 1.05; 再把这个值传给sigmoid函数: 1 / (1 + exp(-x))；也就是y = 1 / (1 + exp(-1.05)) = 0.7408;</p>
   <p>第一层第一个节点传递给第二层第二个节点的信号值是 1.0 * 0.2 = 0.2; 第一层第二个节点传给第二层第二个节点的信号值是 0.5 * 0.8 = 0.4, 第二层第二个节点把这些信号值加总得 X = 0.2 + 0.4 = 0.6, 再将其传入激活函数得 y = 1 / (1 + exp(-0.6)) = 0.6457，最后我们得到神经网络的输出结果为： (0.7408, 0.6457)。</p>
   <p>下一节我们将深入研究如何使用张量运算加快神经网络的运算，以及探讨如何通过误差调整网络中节点间的权值。</p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
