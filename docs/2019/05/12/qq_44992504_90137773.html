<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>机器学习--Logistic回归计算过程的推导 « NotBeCN</title>
  <meta name="description" content="                     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;       ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/12/qq_44992504_90137773.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">机器学习--Logistic回归计算过程的推导</h1>
    <p class="post-meta">May 12, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>(很多讲逻辑回归的文章都没有给出详细的推导，只是列出最后的计算公式，今天在网上看到一篇解释得非常详细的文章，赶紧转载一下：</p>
    <p><a href="http://blog.csdn.net/dongtingzhizi/article/details/15962797" rel="nofollow" target="_blank">【机器学习笔记1】Logistic回归总结</a>(http://blog.csdn.net/dongtingzhizi/article/details/15962797)</p>
    <p>&nbsp;作者说"未经允许，不得转载"，我这里先冒犯了，如果觉得不合适，请告知。</p>
    <p>)</p>
    <p><br></p>
    <p></p>
    <h1><a></a><a target="_blank"></a><strong>&nbsp;Logistic回归总结</strong></h1>
    <p></p>
    <a target="_blank"></a>
    <a target="_blank"></a>1.引言
    <span>首先说一下我的感受，《机器学习实战》一书在介绍原理的同时将全部的<a title="算法与数据结构知识库" class="replace_word" href="http://lib.csdn.net/base/datastructure" rel="nofollow" target="_blank">算法</a>用源代码实现，非常具有操作性，可以加深对算法的理解，但是美中不足的是在原理上介绍的比较粗略，很多细节没有具体介绍。所以，对于没有基础的朋友（包括我）某些地方可能看的一头雾水，需要查阅相关资料进行了解。所以说，该书还是比较适合有基础的朋友。</span>
    <p><span>本文主要介绍以下三个方面的内容：</span></p>
    <p><span>（1）Logistic Regression的基本原理，分布在第二章中；</span></p>
    <p><span>（2）Logistic Regression的具体过程，包括：选取预测函数，求解Cost函数和<strong><em>J(θ)</em></strong>，梯度下降法求<strong><em>J(θ)</em></strong>的最小值，以及递归下降过程的向量化（vectorization），分布在第三章中；</span></p>
    <p><span>（3）对《机器学习实战》中给出的实现代码进行了分析，对阅读该书LogisticRegression部分遇到的疑惑进行了解释。没有基础的朋友在阅读该书的Logistic Regression部分时可能会觉得一头雾水，书中给出的代码很简单，但是怎么也跟书中介绍的理论联系不起来。也会有很多的疑问，比如：一般都是用梯度下降法求损失函数的最小值，为何这里用梯度上升法呢？书中说用梯度上升发，为何代码实现时没见到求梯度的代码呢？这些问题在第三章和第四章中都会得到解答。</span></p>
    <p><span>文中参考或引用内容的出处列在最后的“参考文献”中。文中所阐述的内容仅仅是我个人的理解，如有错误或疏漏，欢迎大家批评指正。下面进入正题。</span></p>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <h1><a></a><a target="_blank"></a><a target="_blank"></a>2. 基本原理</h1>
   <p><span>Logistic Regression和Linear Regression的原理是相似的，按照我自己的理解，可以简单的描述为这样的过程：</span></p>
   <p><span>（1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为<strong><em>h</em></strong>函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。</span></p>
   <p><span>（2）构造一个Cost函数（损失函数），该函数表示预测的输出（<strong><em>h</em></strong>）与训练数据类别（<strong><em>y</em></strong>）之间的偏差，可以是二者之间的差（<strong><em>h-y</em></strong>）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为<strong><em>J(θ)</em></strong>函数，表示所有训练数据预测值与实际类别的偏差。</span></p>
   <p><span>（3）显然，<strong><em>J(θ)</em></strong>函数的值越小表示预测函数越准确（即<strong><em>h</em></strong>函数越准确），所以这一步需要做的是找到<span>J(θ)</span>函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。</span></p>
   <p><span><br></span></p>
   <h1><a></a><a target="_blank"></a><a target="_blank"></a>3. 具体过程</h1>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.1&nbsp; 构造预测函数</h2>
   <p><span>Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据第二章中的步骤，需要先找到一个预测函数（<strong><em>h</em></strong>），显然，该函数的输出必须是两个值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203307078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span></p>
   <p><span>对应的函数图像是一个取值在0和1之间的S型曲线（图1）。</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202319171?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图1</span></p>
   <p><span>接下来需要确定数据划分的边界类型，对于图2和图3中的两种数据分布，显然图2需要一个线性的边界，而图3需要一个非线性的边界。接下来我们只讨论线性边界的情况。</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202358203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图2</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202421671?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图3</span></p>
   <p><span>对于线性边界的情况，边界形式如下：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203346500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>构造预测函数为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203423234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span><strong><em>hθ(x)</em></strong>函数的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203441312?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p align="right"><br></p>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.2&nbsp; 构造Cost函数</h2>
   <p><span>Andrew Ng在课程中直接给出了Cost函数及<strong><em>J(θ)</em></strong>函数如式（5）和（6），但是并没有给出具体的解释，只是说明了这个函数来衡量<strong><em>h</em></strong>函数预测的好坏是合理的。</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203457937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203517875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>实际上这里的Cost函数和<strong><em>J(θ)</em></strong>函数是基于<a href="http://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" rel="nofollow" target="_blank">最大似然估计</a>推导得到的。下面详细说明推导的过程。（4）式综合起来可以写成：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203546390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>取似然函数为：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203558968?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>对数似然函数为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203626296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>最大似然估计就是要求得使<strong><em>l(θ</em></strong>)取最大值时的<em><strong>θ</strong></em>，其实这里可以使用梯度上升法求解，求得的<em><strong>θ</strong></em>就是要求的最佳参数。但是，在Andrew Ng的课程中将<em><strong>J(θ)</strong></em>取为（6）式，即：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203644500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>因为乘了一个负的系数<strong><em>-1/m</em></strong>，所以<em><strong>J(θ)</strong></em>取最小值时的<em><strong>θ</strong></em>为要求的最佳参数。</span></p>
   <p><span><br></span></p>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.3&nbsp; 梯度下降法求<em>J(θ)</em>的最小值</h2>
   <p><span>求<em><strong>J(θ)</strong></em>的最小值可以使用梯度下降法，根据梯度下降法可得<em><strong>θ</strong></em>的更新过程：</span></p>
   <p align="right"><span>&nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203704250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>式中为<strong><em>α</em></strong>学习步长，下面来求偏导：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203723187?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>上式求解过程中用到如下的公式：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203741453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>因此，（11）式的更新过程可以写成：</span></p>
   <p align="right"><span>&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203807453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>因为式中<em><strong>α</strong></em>本来为一常量，所以<strong><em>1/m</em></strong>一般将省略，所以最终的<em><strong>θ</strong></em>更新过程为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113205240203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>另外，补充一下，3.2节中提到求得<strong><em>l(θ</em>)</strong>取最大值时的<em><strong>θ</strong></em>也是一样的，用梯度上升法求（9）式的最大值，可得：</span></p>
   <p align="right"><span>&nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203849390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>观察上式发现跟（14）是一样的，所以，采用梯度上升发和梯度下降法是完全一样的，这也是《机器学习实战》中采用梯度上升法的原因。</span></p>
   <p><span><br></span></p>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.4&nbsp; 梯度下降过程向量化</h2>
   <p><span>关于<em><strong>θ</strong></em>更新过程的vectorization，Andrew Ng的课程中只是一带而过，没有具体的讲解。</span></p>
   <p><span>《机器学习实战》连Cost函数及求梯度等都没有说明，所以更不可能说明vectorization了。但是，其中给出的实现代码确是实现了vectorization的，图4所示代码的32行中weights（也就是<em><strong>θ</strong></em>）的更新只用了一行代码，直接通过矩阵或者向量计算更新，没有用for循环，说明确实实现了vectorization，具体代码下一章分析。</span></p>
   <p><span>文献[3]中也提到了vectorization，但是也是比较粗略，很简单的给出vectorization的结果为：</span></p>
   <p align="right"><span>&nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203911984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>且不论该更新公式正确与否，这里的<strong>Σ(...)</strong>是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization，不像《机器学习实战》的代码中一条语句就可以完成<em><strong>θ</strong></em>的更新。</span></p>
   <p><span>下面说明一下我理解《机器学习实战》中代码实现的vectorization过程。</span></p>
   <p><span>约定训练数据的矩阵形式如下，<strong><em>x</em></strong>的每一行为一条训练样本，而每一列为不同的特称取值：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203934875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>约定待求的参数<em><strong>θ</strong></em>的矩阵形式为：</span></p>
   <p align="right"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203954453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>先求<strong><em>x.θ</em></strong>并记为<strong><em>A</em></strong>：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204012546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>求<strong><em>hθ(x)-y</em></strong>并记为<em><strong>E</strong></em>：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204103593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">&nbsp;</span></p>
   <p><span><strong><em>g(A)</em></strong>的参数<strong><em>A</em></strong>为一列向量，所以实现<strong><em>g</em></strong>函数时要支持列向量作为参数，并返回列向量。由上式可知<strong><em>hθ(x)-y</em></strong>可以由<em><strong>g(A)-y</strong></em>一次计算求得。</span></p>
   <p><span>再来看一下（15）式的<em><strong>θ</strong></em>更新过程，当<strong><em>j=0</em></strong>时：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204122000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>同样的可以写出<em><strong>θj</strong></em>，</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204138093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>综合起来就是：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204152062?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>综上所述，vectorization后<em><strong>θ</strong></em>更新的步骤如下：</span></p>
   <p><span>（1）求<strong><em>A=x.θ</em></strong>；</span></p>
   <p><span>（2）求<strong><em>E=g(A)-y</em></strong>；</span></p>
   <p><span>（3）求<strong>θ:=θ-α.x'.E,</strong>x'表示矩阵x的转置。</span></p>
   <p><span>也可以综合起来写成：</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131213085438093"><br></span></p>
   <p><span>前面已经提到过：1/m是可以省略的。</span></p>
   <h1><a></a><a target="_blank"></a><a target="_blank"></a>4. 代码分析</h1>
   <p><span>图4中是《机器学习实战》中给出的部分实现代码。</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202512453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图4</span></p>
   <p><span>sigmoid函数就是前文中的<strong><em>g(z)</em></strong>函数，参数<strong>inX</strong>可以是向量，因为程序中使用了<a title="Python知识库" class="replace_word" href="http://lib.csdn.net/base/python" rel="nofollow" target="_blank">Python</a>的numpy。</span></p>
   <p><span>gradAscent函数是梯度上升的实现函数，参数dataMatin和classLabels为训练数据，23和24行对训练数据做了处理，转换成numpy的矩阵类型，同时将横向量的classlabels转换成列向量labelMat，此时的dataMatrix和labelMat就是（18）式中的<strong>x</strong>和<strong>y</strong>。alpha为学习步长，maxCycles为迭代次数。weights为n维（等于<strong><em>x</em></strong>的列数）列向量，就是（19）式中的<em><strong>θ</strong></em>。</span></p>
   <p><span>29行的for循环将更新<em><strong>θ</strong></em>的过程迭代maxCycles次，每循环一次更新一次。对比3.4节最后总结的向量化的<em><strong>θ</strong></em>更新步骤，30行相当于求了</span><span><strong><em>A=x.θ</em></strong>和</span><span><strong><em>g(A)</em></strong></span><span>，31行相当于求了<em><strong>E=g(A)-y</strong></em>，32行相当于求<em><strong>θ:=θ-α.x'.E</strong></em>。所以这三行代码实际上与向量化的<em><strong>θ</strong></em>更新步骤是完全一致的。</span></p>
   <p><span>总结一下，从上面代码分析可以看出，虽然只有十多行的代码，但是里面却隐含了太多的细节，如果没有相关基础确实是非常难以理解的。相信完整的阅读了本文，就应该没有问题了！^_^。</span></p>
   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
