<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>cs231n assignment(一) Softmax分类和两层神经网络以及反向传播的代码推导 « NotBeCN</title>
  <meta name="description" content="         目录   序   Softmax分类器   反向传播   数据构建以及网络训练   交叉验证参数优化&nbsp;     序   &nbsp; &nbsp; &nbsp; &nbsp;原来都是用的c++学习的传统图像分割算法。主要学习聚类分割、水平集、图割，欢迎一起讨论学习。   &nbsp; ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://notbe.cn/2019/05/10/1557729123335.html">
  <link rel="alternate" type="application/rss+xml" title="NotBeCN" href="https://notbe.cn/feed.xml" />
</head>


  <body>

    <div class="header-placeholder"></div>
<header class="header">
  <div class="wrapper">
    <div id="sidebar-toggle">TOC</div>
    <a class="site-title" href="/">NotBeCN</a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/" target="_blank">关于</a>
      
        
        <a class="page-link" href="https://uzshare.com" target="_blank">社区</a>
      
        
        <a class="page-link" href="/donate/" target="_blank">Donate</a>
      
        
        <a class="page-link" href="/games/shejiyazi/" target="_blank">射个鸭子</a>
      
    </nav>
  </div>
</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">cs231n assignment(一) Softmax分类和两层神经网络以及反向传播的代码推导</h1>
    <p class="post-meta">May 10, 2019</p>
  </header>

  <article class="post-content">
    <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="%E5%BA%8F-toc" style="margin-left:0px;"><a href="#%E5%BA%8F" rel="nofollow">序</a></p> 
  <p id="Softmax%E5%88%86%E7%B1%BB%E5%99%A8-toc" style="margin-left:0px;"><a href="#Softmax%E5%88%86%E7%B1%BB%E5%99%A8" rel="nofollow">Softmax分类器</a></p> 
  <p id="%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:0px;"><a href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">反向传播</a></p> 
  <p id="%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%8F%8A%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83-toc" style="margin-left:0px;"><a href="#%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%8F%8A%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83" rel="nofollow">数据构建以及网络训练</a></p> 
  <p id="%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%C2%A0-toc" style="margin-left:0px;"><a href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%C2%A0" rel="nofollow">交叉验证参数优化&nbsp;</a></p> 
  <hr id="hr-toc">
  <h1>序</h1> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;原来都是用的c++学习的传统图像分割算法。主要学习聚类分割、水平集、图割，欢迎一起讨论学习。</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;刚刚开始学习cs231n的课程，正好学习python，也做些实战加深对模型的理解。</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;<a href="https://study.163.com/course/courseLearn.htm?courseId=1003223001#/learn/text?lessonId=1050981708&amp;courseId=1003223001" rel="nofollow">课程链接</a></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;1、这是自己的学习笔记，会参考别人的内容，如有侵权请联系删除。</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;2、代码参考<a href="https://zhuanlan.zhihu.com/p/28204173" rel="nofollow">WILL</a>&nbsp;、<a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" rel="nofollow">杜克</a>，但是有了很多自己的学习注释</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;3、有些原理性的内容不会讲解，但是会放上我觉得讲的不错的博客链接</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;4、由于之前没怎么用过numpy，也对python不熟，所以也是一个python和numpy模块的学习笔记</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;本章前言：softmax本来应该单独成为一节，但是由于内容和前面的章节高度重复，神经网络中又使用了softmax输出，考虑之后将softmax的内容和neural network放到一起。阅读本章内容需要有神经网络的基本知识。本章重点：<strong>反向传播</strong></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;在jupyter中写的代码，要import需要下载成为.py文件，import之后如果.py文件中的内容有了修改需要重新打开jupyter，很麻烦，现在在import之后加上以下代码，更改.py文件后就不需要重新打开jupyter了。</p> 
  <pre class="has">
<code class="language-python">#自动加载外部模块
%reload_ext autoreload
%autoreload 2</code></pre> 
  <hr>
  <h1 id="Softmax%E5%88%86%E7%B1%BB%E5%99%A8">Softmax分类器</h1> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;softmax分类器可以看作是在svm得分的基础上，利用公式将得分转化为相对应的概率。</p> 
  <p style="text-align:center;"><img alt="" class="has" height="105" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190510170001310.png" width="300"></p> 
  <p style="text-align:center;"><img alt="" class="has" height="206" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190510170301831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxODI4MzUx,size_16,color_FFFFFF,t_70" width="500"></p> 
  <p>给出softmax的代价函数的代码实现，同样分为朴素版和向量版&nbsp;</p> 
  <pre class="has">
<code class="language-python">import numpy as np
from random import shuffle

def softmax_loss_naive(W, X, y, reg):
    loss = 0.0
    num_classes = W.shape[1]
    num_train = X.shape[0]
    for i in range(num_train):
        f_i = X[i].dot(W)
        f_i = f_i - np.max(f_i)  #减小数值，增强稳定性
        sum_j = np.sum(np.exp(f_i))
        p = lambda k:np.exp(f_i[k])/sum_j
        loss += -np.log(p(y[i]))       #只要计算正确的那一类的损失
        
        #计算梯度
        for k in range(num_classes):
            p_k = p(k)
            dW[:,k]+=(p_k-(k==y[i]))*X[i]
    loss / = num_train
    loss+= 0.5 * reg * np.sum(W*W)
    dW /= num_train
    dW += reg*W
    
    return loss,dW

def softmax_loss_vectorized(W,X,y,reg):
    num_train = X.shape[0]
    f = X.dot(W)
    f -= np.max(f,axis =1,keepdims = True)
    sum_f = np.sum(np.exp(f),axis=1,keepdims=True)
    p = np.exp(f)/sum_f
    
    loss = np.sum(-np.log(p[np.arange(num_train),y]))  #查找每一行正确分类的元素
    
    ind = np.zeros_like(p)
    ind[np.arange(num_train),y]=1       #正确分类需要用概率减一，创建0矩阵，正确分类部分为1
    dW = X.T.dot(p-ind)
    
    loss / = num_train
    loss+= 0.5 * reg * np.sum(W*W)
    dW /= num_train
    dW += reg*W
    
    return loss,dW</code></pre> 
  <hr>
  <h1 id="%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">反向传播</h1> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;上面代码中计算权重矩阵梯度时用到了反向传播中的<strong>链式求导法</strong>则的思想。关于反向传播的理解，这里给出两个链接帮助理解：<a href="https://blog.csdn.net/coder_Gray/article/details/79808999" rel="nofollow">深度学习——对于反向传播的理解(举例验证)</a>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;<a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" rel="nofollow">CS231n课程笔记翻译：反向传播笔记</a></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;读完这两篇对反向传播中的链式求导有了基础了解，下面就是一个二层神经网络的反向传播的推导过程（字丑），其中softmax部分的推导<a href="https://study.163.com/course/courseLearn.htm?courseId=1003223001#/learn/text?lessonId=1050984188&amp;courseId=1003223001" rel="nofollow">参考</a>课程。</p> 
  <p style="text-align:center;"><img alt="" class="has" height="525" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190510181350308.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxODI4MzUx,size_16,color_FFFFFF,t_70" width="700"></p> 
  <p>&nbsp;</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;参考上述推导过程，下面给出两层神经网络的实现方式：</p> 
  <pre class="has">
<code class="language-python">import numpy as np

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,std=1e-4):
        self.params = {}
        #初始化权重矩阵
        self.params['W1'] = std * np.random.randn(input_size,hidden_size)   #W1.shape = (输入层特征数，隐藏层特征数)
        self.params['b1'] = np.zeros(hidden_size)                           #b1.shape = (1, 隐藏层特征数)
        self.params['W2'] = std * np.random.randn(hidden_size,output_size)  #W2.shape = (输入层特征数，隐藏层特征数)
        self.params['b2'] = np.zeros(output_size)
        
    def loss(self,X,y = None,reg = 0.0):
        W1, b1 = self.params['W1'],self.params['b1']
        W2, b2 = self.params['W2'],self.params['b2']
        N,D = X.shape     #N样本数num，D特征维度dimension
        
        scores = None     #得分
        
        h_output = np.maximum(0,X.dot(W1) + b1)   #隐藏层输出（N,H），Relu激活， X是以行来存储  np.maximum比较两组数据的最大值
        scores = h_output.dot(W2) + b2            #输出层（未激活）（N，C）
        
        #如果没有输入y，则直接输出得分
        if y is None:
            return scores
        
        loss = None
        
        #softmax中包含exp，值会比较大，为减小数值采取归一化操作
        shift_scores = scores - np.max(scores,axis = 1).reshape((-1,1))   #归一化，计算每一行的最大值，然后把所有的score减去该max，这是和图像归一化中每个特征减去特诊的max不一样的
        softmax_output = np.exp(shift_scores)/np.sum(np.exp(shift_scores),axis = 1).reshape(-1,1)   #softmax公式
        loss = -np.sum(np.log(softmax_output[range(N), list(y)]))         #计算代价（前面的负号是公式）
        loss/= N                                                          #计算均值
        loss += 0.5*reg*(np.sum(W1*W1)+np.sum(W2*W2))                     #正则，需要将每一层的权值加起来
        
        #输出层计算梯度，参考softmax梯度计算
        grads= {}
        dscores= softmax_output
        dscores[range(N), list(y)] -= 1
        dscores /= N
        grads['W2'] = h_output.T.dot(dscores) + reg*W2
        grads['b2'] = np.sum(dscores,axis = 0)
        
        #隐藏层梯度计算
        dh = dscores.dot(W2.T)
        dh_ReLu = (h_output&gt;0)*dh
        grads['W1'] = X.T.dot(dh_ReLu) + reg*W1
        grads['b1'] = np.sum(dh_ReLu,axis=0)
        return loss,grads
    
    def train(self,X,y,X_val,y_val,learning_rate = 1e-3,learning_rate_decay=0.95,reg=1e-5,num_iters = 1000,batch_size=200,verbose=False):
        num_train = X.shape[0]
        iterations_per_epoch = max(num_train/batch_size,1)  #每一轮的迭代数目,1个epoch等于使用训练集中的全部样本训练一次
        loss_history =[]
        train_acc_history = []
        val_acc_history = []
        
        for it in range(num_iters):
            X_bacth = None
            y_batch = None
            
            idx = np.random.choice(num_train,batch_size,replace = True)
            X_bacth = X[idx]
            y_batch = y[idx]
            loss,grads = self.loss(X_bacth,y=y_batch,reg = reg)
            loss_history.append(loss)
            
            #参数更新
            self.params['W2'] += -learning_rate*grads['W2']
            self.params['b2'] += -learning_rate*grads['b2']
            self.params['W1'] += -learning_rate*grads['W1']
            self.params['b1'] += -learning_rate*grads['b1']
            
            if verbose and it %100 ==0:
                print('iteration %d / %d : loss %f '(it,num_iters,loss))
            
            #每次epoch说明遍历1次训练集，记录精度，并且更改学习率
            if it % iterations_per_epoch == 0:
                train_acc = (self.predict(X_bacth)==y_batch).mean()
                val_acc = np.mean(self.predict(X_val)==y_val)
                train_acc_history.append(train_acc)
                val_acc_history.append(val_acc)
                
                learning_rate*= learning_rate_decay
            
        return {
            'loss_history':loss_history,
            'train_acc_history':train_acc_history,
            'val_acc_history':val_acc_history
        }
    
    def predict(self,X):
        y_pred = None
        h = np.maximum(0,X.dot(self.params['W1'])+self.params['b1'])
        scores = h.dot(self.params['W2'])+self.params['b2']
        y_pred = np.argmax(scores,axis=1)    #取得概率最大的位置
        return y_pred</code></pre> 
  <hr>
  <h1 id="%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E4%BB%A5%E5%8F%8A%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83">数据构建以及网络训练</h1> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp;将之前的数据构建整合成一个方法，注意这边的归一化操作是将每个特征减去对应的特征点最大值，这和softmax中一组得分减取改组得分的最大值不一样。</p> 
  <pre class="has">
<code class="language-python">def get_cifar_data(num_training =49000,num_validation=1000,num_test=1000):
    X_train,y_train,X_test,y_test = load_cifar10('cifar-10-batches-py')
    #验证集
    mask = range(num_training, num_training+num_validation)     #先取验证集，大小为最后的1000条数据
    x_val = X_train[mask]
    y_val = y_train[mask]
    #训练集
    mask = range(num_training)
    x_train = X_train[0:num_training,:,:,:]
    y_train = y_train[mask]
    #测试集
    mask = range(num_test)
    x_test = X_test[mask]
    y_test = y_test[mask]
    
    mean_image = np.mean(x_train, axis=0)
    #归一化操作，将所有的特征都减去对应的均值
    x_train -= mean_image
    x_val -= mean_image
    x_test -= mean_image

    x_train = np.reshape(x_train,(x_train.shape[0],-1))
    x_val = np.reshape(x_val,(x_val.shape[0],-1))
    x_test = np.reshape(x_test,(x_test.shape[0],-1))
    return x_train,y_train,x_val,y_val,x_test,y_test</code></pre> 
  <hr>
  <p>数据构建测试&nbsp;</p> 
  <pre class="has">
<code class="language-python">X_train,y_train,X_val,y_val,X_test,y_test = get_cifar_data()
print('Train data shape: ', X_train.shape)
print('Train labels shape: ', y_train.shape)
print("validation data shape: ", X_val.shape)
print('validation labels shape: ', y_val.shape)
print('test data shape: ', X_test.shape)
print('test labels shape: ',y_test.shape)

"""
X_train,y_train,X_val,y_val,X_test,y_test = get_cifar_data()
print('Train data shape: ', X_train.shape)
print('Train labels shape: ', y_train.shape)
print("validation data shape: ", X_val.shape)
print('validation labels shape: ', y_val.shape)
print('test data shape: ', X_test.shape)
print('test labels shape: ',y_test.shape)
"""</code></pre> 
  <p>&nbsp;训练</p> 
  <pre class="has">
<code class="language-python">input_size = 32*32*3
hidden_size = 50
num_class = 10
net = TwoLayerNet(input_size,hidden_size,num_class)
stats = net.train(X_train,y_train,X_val,y_val,learning_rate = 1e-3,learning_rate_decay=0.95,reg=1e-5,num_iters = 1000,batch_size=200,verbose=True)
val_acc = (net.predict(X_val)==y_val).mean()
print('validation accuracy:',val_acc)

"""
iteration 0 / 1000 : loss 2.302589 
iteration 100 / 1000 : loss 1.907440 
iteration 200 / 1000 : loss 1.798388 
iteration 300 / 1000 : loss 1.721173 
iteration 400 / 1000 : loss 1.639906 
iteration 500 / 1000 : loss 1.691722 
iteration 600 / 1000 : loss 1.623998 
iteration 700 / 1000 : loss 1.493039 
iteration 800 / 1000 : loss 1.543409 
iteration 900 / 1000 : loss 1.500300 
validation accuracy: 0.48
"""</code></pre> 
  <hr>
  <pre class="has">
<code class="language-python">#loss和accuracy的可视化
plt.subplot(211)
plt.plot(stats['loss_history'])
plt.title('loss history')
plt.xlabel('iteration')
plt.ylabel('loss')

plt.subplot(212)
plt.plot(stats['train_acc_history'],label='train')
plt.plot(stats['val_acc_history'],label = 'val')
plt.title('classfication accuracy history')
plt.xlabel('epoch')
plt.ylabel('classfication accuracy')
plt.show()</code></pre> 
  <p style="text-align:center;"><img alt="" class="has" height="253" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190510182550286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxODI4MzUx,size_16,color_FFFFFF,t_70" width="400"></p> 
  <hr>
  <p>权重矩阵的可视化&nbsp;</p> 
  <pre class="has">
<code class="language-python">from visualize_grid import *
def show_net_weights(net):
    W1 = net.params['W1']
    W1 = W1.reshape(32,32,3,-1).transpose(3,0,1,2)
    plt.imshow(visualize_grid(W1,padding=3).astype('uint8'))
    plt.gca().axis('off')
    plt.show()
show_net_weights(net)</code></pre> 
  <p style="text-align:center;"><img alt="" class="has" height="234" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190510182737348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxODI4MzUx,size_16,color_FFFFFF,t_70" width="250"></p> 
  <hr>
  <h1 id="%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%C2%A0">交叉验证参数优化&nbsp;</h1> 
  <pre class="has">
<code class="language-python">#交叉验证
input_size=32*32*3
num_class = 10
hidden_size=[75,100,125]
results={}
best_val_acc= 0
best_net=None
learning_rates = np.array([0.7,0.8,0.9])*1e-3
regulation_strengths=[0.75,1.0,1.25]
print('run')
for hs in hidden_size:
    for lr in learning_rates:
        for reg in regulation_strengths:
            net = TwoLayerNet(input_size,hs,num_class)
            
            stats=net.train(X_train,y_train,X_val,y_val,learning_rate = lr,learning_rate_decay=0.95,reg=reg,num_iters = 1000,batch_size=200,verbose=False)
            val_acc=np.mean(net.predict(X_val)==y_val)
            if val_acc &gt; best_val_acc:
                best_val_acc = val_acc
                best_net = net
            results[(hs,lr,reg)]=val_acc
print('finish')

for hs,lr,reg in sorted(results):
    val_acc = results
    print('hs %d lr %e reg %e val accuract: %f' % (hs,lr,reg,val_acc))
print('best validation accuracy achived during cross_validation:%f' %best_val_acc)</code></pre> 
  <p>&nbsp;</p> 
 </div> 
</div>
  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="https://uzstatic-360cdn.belost.xyz/theme/default/images/logo.png" alt="柚子社区">
  <div class="col-box-title name">NotBeCN</div>
  <!-- <p>最新资讯</p> -->
  <p class="contact">
    
    <a href="mailto:fandyvon@163.com" target="_blank">邮箱</a>
    
    <a href="https://uzshare.com" target="_blank">柚子社区</a>
    
    <a href="https://uzzz.org" target="_blank">找组织</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">最新</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2019/05/14/zxh1220_90138586.html">[原创软件] [软件发布] 定时备份文件发送邮箱，不再怕数据丢失了</a></li>
    
      <li><a class="post-link" href="/2019/05/14/weixin_45037290_90140056.html">Get智能写作满月记 ——产品篇</a></li>
    
      <li><a class="post-link" href="/2019/05/14/nulio__90138386.html">《深度探索C++对象模型》..............</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_41248707_90140031.html">mysql 多表联查之连接查询</a></li>
    
      <li><a class="post-link" href="/2019/05/13/qq_21122683_90125902.html">golang基础(二)</a></li>
    
      <li><a class="post-link" href="/2019/05/13/1557726108256.html">今日份的PTA刷题</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90137366.html">Android之折线图</a></li>
    
      <li><a class="post-link" href="/2019/05/12/zzzfffei_90136638.html">Android之实现选中时改变样式</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">目录</div>
</div>

<div class="col-box">
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- right_sidebar -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-8889449066804352"
       data-ad-slot="2081363239"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2019 
</div>
</footer>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script src="/js/easybook.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123344652-5');
</script>


<script data-ad-client="ca-pub-8889449066804352" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.async = true;
  hm.src = "https://hm.baidu.com/hm.js?9b378145d7399199b371d067f4c8be96";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>




  </body>

</html>
